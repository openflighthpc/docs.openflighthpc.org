{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"docs/","title":"Welcome to the OpenFlight Knowledgebase","text":""},{"location":"docs/#what-does-this-cover","title":"What does this cover?","text":"<p>This documentation provides Open Knowledge on HPC. It is designed to cover a range of topics including, but not limited to:</p> <ul> <li>HPC Concepts</li> <li>Getting Started with Linux in a HPC Environment</li> <li>Creating Research Environments </li> <li>Performing HPC Workflows </li> <li>Testing a HPC Environment</li> </ul>"},{"location":"docs/#what-is-going-on","title":"What is going on?","text":"<p>OpenFlight is currently refactoring it's documentation to provide a better user experience, expand the content and improve access to the knowledge within.</p>"},{"location":"docs/#when-will-it-be-done","title":"When will it be done?","text":"<p>This will be ready when: </p> <ul> <li> Existing docs migrated<ul> <li> Documentation Overview</li> <li> General Environment Usage</li> <li> Flight Environment Usage</li> <li> HPC Environment Usage</li> <li> Cluster Build Methods</li> <li> Workflow Examples</li> <li> Functionality Testing</li> </ul> </li> <li> Additional content added<ul> <li> HPC Concepts &amp; Guidance (both on-site &amp; cloud)</li> <li> Flight Service documentation</li> <li> Expansion on Kubernetes Usage</li> <li> Flight Solo Restructure &amp; Expansion (Getting Started, How It Works, Cluster Build Refactor)</li> <li> Benchmarking Workflows (migrate from here)</li> <li> Tensorflow Fashion Workflow Example</li> <li> Basic HPC Workflow</li> <li> Expansion &amp; Consistency of Tool Documentation (Installing, Configuring, Using)</li> <li> Glossary</li> </ul> </li> </ul>"},{"location":"docs/flight-environment/","title":"What is the Flight Environment?","text":"<p>The Flight Environment generally encompasses the tools in the OpenFlight project.</p> <p>The tools developed by OpenFlight are designed to work independently but are more powerful when used in conjunction with one another as they each aim to address small areas of HPC environment usage. </p> <p>The Flight Environment broadly consists of:</p> <ul> <li>Flight User Suite - A collection of CLI tools aimed to improve general HPC environment access by streamlining common workflow tasks. Things like launching desktop sessions, adding software ecosystems and managing object storage.</li> <li>Flight Web Suite - A web front-end aimed at HPC end-users to get them accessing their HPC environment as easily as possible. This is done by providing a web experience that gives them access to terminals, desktops and files on the system. </li> <li>Flight Admin Tools - A collection of CLI tools aimed at admins to aid with common HPC environment configuration, such as, gathering system information, building an inventory of systems and applying configuration to the cluster. </li> </ul>"},{"location":"docs/flight-environment/#who-is-it-for","title":"Who is it for?","text":"<p>The Flight User Suite is designed for use by end-users - that's the scientists, researchers, engineers and software developers who actually run compute workloads and process data. This documentation is designed to help these people to get the best out this environment, without needing assistance from teams of IT professionals. Flight provides tools which enable users to service themselves - it's very configurable, and can be expanded by individual users to deliver a scalable platform for computational workloads.</p>"},{"location":"docs/flight-environment/#flight-user-suite","title":"Flight User Suite","text":"<p>The Flight User Suite sits unobtrusively on the research environment by default, the tools will not be accessible until the environment is activated. </p> <p>The Flight User Suite consists of: </p> <ul> <li>Runway - A self-contained Ruby environment and an entrypoint for accessing the other flight tools</li> <li>Starter - Profile scripts for integrating the user suite into the shell environment, giving easy access to all Flight Environment tools</li> <li>Desktop - An intuitive tool for launching VNC-ready virtual desktops of many different desktop environments (gnome, xterm, kde, etc)</li> <li>Env - Access to, and management of, various software managers to ensure access to a wide variety of HPC applications</li> <li>Silo - Management of object-based storage for managing files and software, providing a simple interface to user data</li> <li>Job - Create and manage customized job scripts from predefined templates, launch jobs and monitor their activity</li> </ul>"},{"location":"docs/flight-environment/#flight-web-suite","title":"Flight Web Suite","text":"<p>The Flight Web Suite consists of: </p> <ul> <li>WWW - A self-contained web-server for accessing Web Suite applications</li> <li>Login - User access management, integrated with system authentication to allow users to securely access the Web Suite</li> <li>Console - A web-based terminal giving CLI access to the system running the Web Suite</li> <li>Desktop - Create and manage remote desktop sessions on the HPC environment</li> <li>File Manager - Access to user files, providing a simple way to upload and download data directly into the user environment</li> <li>Job - Create and manage customized job scripts and their execution</li> </ul>"},{"location":"docs/flight-environment/#flight-admin-tools","title":"Flight Admin Tools","text":"<p>The Flight Admin Tools are:</p> <ul> <li>Gather - Collect and store system information</li> <li>Hunter - Send system information and manage a host inventory over the network</li> <li>Profile - Apply configuration identities to hosts in a HPC environment </li> <li>PDSH - OpenFlight's build of PDSH</li> </ul>"},{"location":"docs/flight-environment/get-flight/","title":"Obtaining the Flight Environment","text":"<p>There are a few options for obtaining the Flight Environment, some of the methods will be covered within this documentation:</p> From the OpenFlight Repository Prebuilt RPMs and DEB packages are built and maintained by the project, this is the recommended way of installing Flight on an existing system. Through the Flight Solo image For those utilising public and/or private cloud, the Flight Solo image provides a preconfigured OS loaded with the OpenFlight tools and useful extra hooks to get you started with HPC deployment quickly. Downloading the source code Every project is available in the OpenFlight GitHub organisation, the very latest changes are available here however it is not recommended to build it from source for general usage. The source code is subject to frequent changes and won't be vigorously tested until just before packaging and releasing in the repository."},{"location":"docs/flight-environment/get-flight/configure/","title":"Configuring the Flight Environment","text":""},{"location":"docs/flight-environment/get-flight/configure/#general-flight-environment","title":"General Flight Environment","text":""},{"location":"docs/flight-environment/get-flight/configure/#config-commands","title":"Config Commands","text":"<p>Some basic aspects of the flight environment can be set from the CLI. </p>"},{"location":"docs/flight-environment/get-flight/configure/#global-configuration","title":"Global Configuration","text":"<p>Global environment configuration can be set through the <code>flight config</code> command. This command provides the ability to get, set and list the global environment configuration. </p> <p>The command can be run as follows:  <pre><code>flight config set NAME VALUE\n</code></pre></p> <p>Some common global configuration options are:</p> <ul> <li><code>cluster.name</code> - The name of the cluster for the HPC environment, this will be visible in the command prompt</li> <li><code>pdsh.priority</code> - Set the priority of pdsh/nodeattr commands</li> </ul>"},{"location":"docs/flight-environment/get-flight/configure/#user-configuration","title":"User Configuration","text":"<p>The <code>flight set</code> command is used to modify the flight environment within your user scope to work as you'd like it to by enabling/disabling different features. </p> <p>The command can be run as follows:  <pre><code>flight set OPTION [on|off]\n</code></pre></p> <p>Some common options are:</p> <ul> <li><code>hints</code>-  Show or hide command hints on login</li> <li><code>welcome</code> - Show or hide the welcome splash screen on login</li> <li><code>secondary</code> - Toggle whether the flight environment should be loaded in subshells</li> <li><code>always</code> - Toggle whether the flight environment is activated </li> </ul> <p>Further information an options can be found with the command <code>flight info</code>.</p> <p>Tip</p> <p>If you have root permissions then the option <code>--global</code> can be appended to the <code>flight set</code> command to modify the default settings for all users</p>"},{"location":"docs/flight-environment/get-flight/configure/#filesystem-structure","title":"Filesystem Structure","text":"<p>When installed from the packages, the Flight Environment stores everything under <code>/opt/flight</code> (referenced as the <code>flight_ROOT</code>). This directory mirrors the Linux Filesystem Hierarchy Standard. </p> <p>Briefly, the <code>flight_ROOT</code> consists of: </p> <ul> <li><code>bin/</code> - Flight environment entrypoint commands </li> <li><code>etc/</code> - Flight environment configuration files</li> <li><code>lib/</code> - Libraries and script</li> <li><code>libexec/</code> - Hooks and scripts for the flight tools</li> <li><code>opt/</code> - Installation location for flight tools </li> <li><code>usr/</code> - User content and resources</li> <li><code>var/</code> - Additional libraries and log files</li> </ul> <p>Generally speaking, configuration files for the flight environment can be found under <code>etc/</code> in the <code>flight_ROOT</code> or through the configuration directories of specific tools in <code>opt/TOOL_NAME/etc/</code> within <code>flight_ROOT</code>. </p> <p>Tip</p> <p>Every flight tool provides a breakdown of the available configuration options and how to set them either through their README or in a <code>etc/config.yml.ex</code> file in the source repository. </p> <p>For example, available configuration options for version 1.11.3 of flight desktop can be found in the Flight Desktop GitHub repo</p>"},{"location":"docs/flight-environment/get-flight/install/","title":"Installing the Flight Environment","text":"<p>The OpenFlight project packages tools as both RPMs and debs that are hosted in package repositories which can be quickly installed with a couple of commands.</p>"},{"location":"docs/flight-environment/get-flight/install/#setting-up-the-repository","title":"Setting Up The Repository","text":"EL7 EL8 EL9 Ubuntu 22.04 <ul> <li>Install the OpenFlight release RPM:     <pre><code>sudo yum install https://repo.openflighthpc.org/pub/centos/7/openflighthpc-release-latest.noarch.rpm\n</code></pre></li> <li>Rebuild the yum cache:     <pre><code>sudo yum makecache\n</code></pre></li> <li>Most tools require packages available in the EPEL repository, installing it is recommended:     <pre><code>sudo yum install epel-release\n</code></pre></li> </ul> <ul> <li>Install the OpenFlight release RPM:     <pre><code>sudo dnf install https://repo.openflighthpc.org/openflight/centos/8/x86_64/openflighthpc-release-3-1.noarch.rpm\n</code></pre></li> <li>Add the Power Tools repository:     <pre><code>sudo dnf config-manager --set-enabled powertools\n</code></pre></li> <li>Most tools require packages available in the EPEL repository, installing it is recommended:     <pre><code>sudo dnf install epel-release\n</code></pre></li> <li>Finally, rebuild the dnf cache:     <pre><code>sudo dnf makecache\n</code></pre></li> </ul> <ul> <li>Install the OpenFlight release RPM:     <pre><code>sudo dnf install https://repo.openflighthpc.org/openflight/centos/9/x86_64/openflighthpc-release-3-1.noarch.rpm\n</code></pre></li> <li>Add the Code Ready Builder repository:     <pre><code>sudo dnf config-manager --set-enabled crb\n</code></pre></li> <li>Most tools require packages available in the EPEL repository, installing it is recommended:     <pre><code>sudo dnf install epel-release\n</code></pre></li> <li>Finally, rebuild the dnf cache:     <pre><code>sudo dnf makecache\n</code></pre></li> </ul> <ul> <li>Import the public signature for OpenFlight:     <pre><code>sudo apt-key adv --fetch-keys https://repo.openflighthpc.org/openflighthpc-archive-key.asc\n</code></pre></li> <li>Install the OpenFlight repository:     <pre><code>sudo apt-add-repository \"deb https://repo.openflighthpc.org/openflight/ubuntu stable main\"\n</code></pre></li> <li>Update the apt cache:     <pre><code>sudo apt-get update\n</code></pre></li> </ul> <p>Info</p> <p>There are 3 repositories available - production (enabled by default), dev (providing development releases of tools) and vault (access to old, unsupported versions and retired tools).</p>"},{"location":"docs/flight-environment/get-flight/install/#installing-the-tools","title":"Installing the Tools","text":""},{"location":"docs/flight-environment/get-flight/install/#flight-user-suite","title":"Flight User Suite","text":"<p>The quickest and simplest way to get up and running with the user suite is to install the group package for the tools. This will ensure that compatible versions of all the tools are installed.</p>  EL7 EL8/EL9 Ubuntu 22.04 <ul> <li>Install the user suite group package     <pre><code>sudo yum install flight-user-suite\n</code></pre></li> </ul> <ul> <li>Install the user suite group package     <pre><code>sudo dnf install flight-user-suite\n</code></pre></li> </ul> <ul> <li>Install the user suite group package     <pre><code>sudo apt-get install flight-user-suite\n</code></pre></li> </ul> <p>Note</p> <p>After installation, logout and back in again to expose the <code>flight</code> command to the shell</p>"},{"location":"docs/flight-environment/get-flight/install/#flight-web-suite","title":"Flight Web Suite","text":"<p>Flight Web Suite consists of multiple packages so the simplest way to get going with it is to install the group package. Some elements of the Web Suite are dependent on tools in the User Suite and will pull those packages in if the User Suite is not already installed. </p>  EL7 EL8/EL9 Ubuntu 22.04 <ul> <li>Install the web suite group package     <pre><code>sudo yum install flight-web-suite\n</code></pre></li> <li>Install extra optional packages      <pre><code>sudo yum install python-websockify xorg-x11-apps netpbm-progs\n</code></pre></li> </ul> <ul> <li>Install the web suite group package     <pre><code>sudo dnf install flight-web-suite\n</code></pre></li> <li>Install extra optional packages      <pre><code>sudo dnf install python-websockify xorg-x11-apps netpbm-progs\n</code></pre></li> </ul> <ul> <li>Install the web suite group package     <pre><code>sudo apt-get install flight-web-suite\n</code></pre></li> <li>Install extra optional packages      <pre><code>sudo apt-get install netpbm x11-apps websockify\n</code></pre></li> </ul>"},{"location":"docs/flight-environment/get-flight/install/#flight-admin-tools","title":"Flight Admin Tools","text":"<p>The Flight Admin Tools exist separately and can be installed by name from the repositories. The packages are: </p> <ul> <li><code>flight-gather</code></li> <li><code>flight-hunter</code></li> <li><code>flight-profile</code></li> <li><code>flight-pdsh</code></li> </ul>  EL7 EL8/EL9 Ubuntu 22.04 <ul> <li>Install the desired tool     <pre><code>sudo yum install PACKAGE\n</code></pre></li> </ul> <ul> <li>Install the admin tools     <pre><code>sudo dnf install PACKAGE\n</code></pre></li> </ul> <ul> <li>Install the admin tools     <pre><code>sudo apt-get install PACKAGE\n</code></pre></li> </ul>"},{"location":"docs/flight-environment/use-flight/","title":"Using the Flight Environment","text":"<p>There are many moving parts to the Flight Environment so the sections within here cover the basics as well as detailed usage of all tools within the Flight User Suite, Web Suite &amp; Admin Tools. </p>"},{"location":"docs/flight-environment/use-flight/environment-basics/","title":"Flight Environment Basics","text":""},{"location":"docs/flight-environment/use-flight/environment-basics/#activate-the-flight-environment","title":"Activate the Flight Environment","text":"<p>The Flight User Suite sits unobtrusively on the research environment with the <code>flight</code> command serving as an entrypoint to the various system commands.</p> <p>This provides some quick tips to activating the system and finding out more about the flight system (<code>flight info</code>).</p> <p>To load the system, simply run <code>flight start</code> (which, when first run, will generate some login keys for allowing passwordless login to compute nodes)</p> <p></p> <p>Tip</p> <p>The flight system can be set to automatically start on login for the user by running <code>flight set always on</code></p>"},{"location":"docs/flight-environment/use-flight/environment-basics/#flight-howto","title":"Flight HowTo","text":"<p>The Flight User Suite comes with built-in guides to assist with usage of the research environment from setting up the flight environment to using the queue system.</p>"},{"location":"docs/flight-environment/use-flight/environment-basics/#showing-available-guides","title":"Showing Available Guides","text":"<p>To show the available guides, list them with: <pre><code>[flight@chead1 (mycluster1) ~]$ flight howto ls\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Index \u2502 Name                             \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 1     \u2502 Get Started                      \u2502\n\u2502 2     \u2502 Work With The Flight Environment \u2502\n\u2502 3     \u2502 Use Flight User Suite            \u2502\n\u2502 4     \u2502 Run Jobs                         \u2502\n\u2502 5     \u2502 Use A Scheduler                  \u2502\n\u2502 6     \u2502 Flight Desktop                   \u2502\n\u2502 7     \u2502 Flight Env                       \u2502\n\u2502 8     \u2502 Flight Job                       \u2502\n\u2502 9     \u2502 Flight Login Api                 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre></p>"},{"location":"docs/flight-environment/use-flight/environment-basics/#viewing-a-guide","title":"Viewing a Guide","text":"<p>A guide can be viewed by requesting either the index or name of the desired guide. When viewing a guide, the content will open inside a <code>less</code> session.</p> <p>To view the get started guide by name:</p> <pre><code>flight howto show 'Get Started'\n</code></pre> <p>To view the get started guide by index:</p> <pre><code>flight howto show 1\n</code></pre> <p>When finished with the guide, simply escape the <code>less</code> session with <code>q</code>. Alternatively, the optional argument <code>--no-pager</code> will prevent any sort of view manager, instead the contents of the guide will be output straight to the terminal.</p>"},{"location":"docs/flight-environment/use-flight/flight-admin-tools/","title":"Flight Admin Tools","text":"<p>The Flight Admin Tools are a collection of CLI tools aimed at admins to aid with common HPC environment configuration, such as, gathering system information, building an inventory of systems and applying configuration to the cluster. </p>"},{"location":"docs/flight-environment/use-flight/flight-admin-tools/gather/","title":"Flight Gather","text":"<p>Flight Gather collects system information into a processable YAML format. </p> <p>Data collected by Flight Gather: </p> <ul> <li>BIOS Version</li> <li>Serial Number</li> <li>Total RAM</li> <li>CPU info (sockets, cores, IDs, models) </li> <li>Network info (interfaces, MAC addresses, speed) </li> <li>Disk info (name, size)</li> <li>GPU info (name, slot) </li> <li>Platform</li> </ul> <p>Further to the above, the user can provide groups (in a similar vein to genders) to add further categorisation to the data. This is useful when collating the data from many systems. </p>"},{"location":"docs/flight-environment/use-flight/flight-admin-tools/gather/#using-flight-gather","title":"Using Flight Gather","text":"<p>There are a few basic usages to flight gather:</p> <ul> <li>Collect system information      <pre><code>flight gather collect\n</code></pre></li> <li>View the system information     <pre><code>flight gather show \n</code></pre></li> <li>Modify groups assigned to the data     <pre><code>flight gather modify --primary MyPrimaryGroupName\n</code></pre></li> </ul> <p>Tip</p> <p>To find further information on the tool and command options see the help page for the tool <code>flight gather --help</code>, the same can be done for sub-commands (e.g. <code>flight gather collect --help</code>) </p>"},{"location":"docs/flight-environment/use-flight/flight-admin-tools/hunter/","title":"Flight Hunter","text":"<p>Flight hunter is a host inventory tool used find and connect to nodes in a compute cluster. It is predominantly used while setting up a cluster.</p> <p>Flight hunter's namesake feature is the ability to discover nodes on the local network by hunting. It can also send information between nodes, and can send on startup to a designated server node.</p>"},{"location":"docs/flight-environment/use-flight/flight-admin-tools/hunter/#hunt","title":"Hunt","text":"<p>Hunting can be started with the command <code>flight hunter hunt</code>. e.g. <pre><code>[flight@chead1 (mycluster1) [login1] ~]$ flight hunter hunt\nHunter running on port 8888 - Ctrl+C to stop\n</code></pre></p> <p>There are several options that change how <code>hunt</code> works</p>"},{"location":"docs/flight-environment/use-flight/flight-admin-tools/hunter/#hunt-options","title":"Hunt Options","text":"<ul> <li><code>--allow-existing</code> - Duplicate entries will replace existing ones rather than being rejected.</li> <li><code>--port &lt;port&gt;</code> - Overrides the default port to accept on.</li> <li><code>--include-self</code> - Sends itself to hunter.</li> <li><code>--auth &lt;auth&gt;</code> - Defines a password to use as an authentication key, only incoming senders with the correct auth will be accepted.</li> <li><code>--auto-parse &lt;regex&gt;</code> - Automatically parses nodes matching this regex when they are added.</li> </ul> <p>Hunter also has the ability to automatically apply a profile identity to nodes as soon as they are parsed. This is called <code>auto-apply</code> - it is not a command line option, it must be configured through hunter config files. </p> <p>Tip</p> <p>All the above options can be set in the Flight Hunter config file located within <code>etc/config.yml</code> in the installation directory (if installed through the package this will be at <code>/opt/flight/opt/hunter/etc/config.yml</code>). For more information on configuring Flight Tools see the configuration document</p>"},{"location":"docs/flight-environment/use-flight/flight-admin-tools/hunter/#send","title":"Send","text":"<p>Sending inventory information to a hunter server can be done with the command <code>flight hunter send</code>. And a message will be displayed on a successful transmission. <pre><code>[flight@chead1 ~]$ flight hunter send --server 10.10.0.1\nSuccessful transmission\n</code></pre></p>"},{"location":"docs/flight-environment/use-flight/flight-admin-tools/hunter/#send-options","title":"Send Options","text":"<ul> <li><code>--port &lt;port number&gt;</code> - Overrides the default port to send to.</li> <li><code>--server &lt;ip address&gt;</code> - Overrides the default server name, useful if the default is not set.</li> <li><code>--auth &lt;auth&gt;</code> - Defines a password to use as an authentication key.</li> <li><code>--broadcast</code> - Makes send broadcast to all addresses on the default subnet.</li> <li><code>--broadcast-address &lt;subnet&gt;</code> - Overrides the default subnet to send to, use if the default subnet was not configured.</li> <li><code>--groups &lt;group1,group2,group3...&gt;</code> - Specifies the groups that this nodes should be in.</li> <li><code>--label &lt;label&gt;</code> - Defines a label to use when parsing this node.</li> <li><code>--prefix &lt;prefix&gt;</code> - Defines a prefix, which will be numbered, and used as a label.</li> <li><code>--command &lt;command&gt;</code> - Runs the given console command and sends the output as the payload of the send.</li> <li><code>--retry-interval &lt;interval&gt;</code> - The number of seconds to wait before attempting to send again </li> </ul> <p>Tip</p> <p>All the above options can be set in the Flight Hunter config file located within <code>etc/config.yml</code> in the installation directory (if installed through the package this will be at <code>/opt/flight/opt/hunter/etc/config.yml</code>). For more information on configuring Flight Tools see the configuration document</p>"},{"location":"docs/flight-environment/use-flight/flight-admin-tools/hunter/#parse","title":"Parse","text":"<p>One of hunter's main features is the ability to parse discovered nodes to prepare them for use with other flight tools. A benefit of the parsing feature is applying \"labels\" to discovered hosts, in ephemeral systems the hostnames may not be ideally formed or of a memorable length so labels make it easier to identify hosts. </p> <p>Note</p> <p>There must be some discovered nodes in the hunter buffer, otherwise there is nothing to parse.</p> <p>The <code>parse</code> command will generate a list, for example: <pre><code>[flight@login-node.novalocal ~]$ flight hunter parse\nSelect nodes: (Scroll for more nodes)\n\u2023 \u2b21 login-node.novalocal - 127.0.0.1\n  \u2b21 compute-node-1.novalocal - 10.151.15.194\n  \u2b21 compute-node-2.novalocal - 10.151.15.238\n</code></pre> To parse, select a node from the list with Space, and you will be taken to the label editor.</p> <p><pre><code>Choose label: login-node.novalocal\n</code></pre> Here, you can edit the label like plain text. <pre><code>Choose label: login1\n</code></pre></p> <p>Tip</p> <p>You can clear the current node name by pressing Down in the label editor.</p> <p>When done editing, press Enter to save. The modified node label will appear next to the ip address and original node label. <pre><code>Select nodes: login-node.novalocal - 127.0.0.1 (login1) (Scroll for more nodes)\n\u2023 \u2b22 login-node.novalocal - 127.0.0.1 (login1)\n  \u2b21 compute-node-1.novalocal - 10.151.15.194\n  \u2b21 compute-node-2.novalocal - 10.151.15.238\n</code></pre></p> <p>From this point, you can either hit the Enter key to finish parsing and process the selected nodes, or continue changing node labels. Either way, you can return to this list by running <code>flight hunter parse</code> again.</p> <p>Question</p> <p>Do you have many hosts in the buffer to pass at once?</p> <p>If the hostnames of these are acceptable or labels/prefixes have been set then the <code>--auto</code> option will allow for bulk parsing of the buffer. Further information on configuring auto-parsing is available in the Flight Hunter repository</p>"},{"location":"docs/flight-environment/use-flight/flight-admin-tools/hunter/#parse-options","title":"Parse options","text":"<p>There are some additional options that can be used with parse.</p> <ul> <li><code>--prefix &lt;prefix&gt;</code> - Defines a prefix, which will be numbered, and used as a label.</li> <li><code>--start &lt;number&gt;</code> - Defines the start value for the numbering of prefixes.</li> <li><code>--auto</code> - Automatically parses everything in the buffer list.</li> <li><code>--allow-existing</code> - Duplicate entries will replace existing ones rather than being rejected.</li> <li><code>--skip-used-index</code> - If there is already a node with particular label, then entering the a different node with the same label will increase the index to the next available one instead of throwing an error.</li> <li><code>--dry-run</code> - Print the generated node labels without actually parsing any nodes</li> <li><code>--default-label</code> - Set what the default label for a node should be if no label/prefix has been provided (either in the node presets or on the parse CLI). This can be one of <code>short</code> (use the short hostname of the node), <code>long</code> (use the FQDN of the node) and <code>blank</code> (do not generate any label) </li> </ul> <p>Tip</p> <p>All the above options can be set in the Flight Hunter config file located within <code>etc/config.yml</code> in the installation directory (if installed through the package this will be at <code>/opt/flight/opt/hunter/etc/config.yml</code>). For more information on configuring Flight Tools see the configuration document</p>"},{"location":"docs/flight-environment/use-flight/flight-admin-tools/hunter/#other-hunter-commands","title":"Other hunter commands","text":"<p>Further commands and features of hunter are covered below</p>"},{"location":"docs/flight-environment/use-flight/flight-admin-tools/hunter/#autorun","title":"<code>autorun</code>","text":"<p>This command automatically runs either <code>hunt</code> or <code>send</code> based on its configuration.</p> <p>Note</p> <p>This is used by the hunter service to launch hunter with the correct settings and isn't intended for use by end-users</p>"},{"location":"docs/flight-environment/use-flight/flight-admin-tools/hunter/#dump-buffer","title":"<code>dump-buffer</code>","text":"<p>Drops all nodes in the buffer list.</p>"},{"location":"docs/flight-environment/use-flight/flight-admin-tools/hunter/#list","title":"<code>list</code>","text":"<p>Shows all nodes in the parsed list of nodes, and has several additional options:</p> <ul> <li><code>--plain</code> - Displays in a plain format</li> <li><code>--by-group</code> - Displays by group.</li> <li><code>--buffer</code> - Shows the buffer list instead.</li> </ul>"},{"location":"docs/flight-environment/use-flight/flight-admin-tools/hunter/#modify-groups-node","title":"<code>modify-groups &lt;node&gt;</code>","text":"<p>Allows for adding or removing of groups to a node.</p> <ul> <li><code>--add &lt;groups&gt;</code> - Adds a group or comma separated list of groups.</li> <li><code>--remove &lt;groups&gt;</code> - Removes a group or comma separated list of groups.</li> <li><code>--buffer</code> - Modifies groups in the buffer list instead.</li> <li><code>--regex</code> - The nodename is parsed as regex instead, and group changes are made to all nodes that match.</li> </ul> <p>Tip</p> <p>You can select multiple nodes at once by writing a comma separated list, or with square bracket expansion (like genders syntax). For example, <code>remove node[01-02] compute</code> would remove <code>node01</code> and <code>node02</code></p>"},{"location":"docs/flight-environment/use-flight/flight-admin-tools/hunter/#modify-label-old-new","title":"<code>modify-label &lt;old&gt; &lt;new&gt;</code>","text":"<p>Changes the label of a node from its current one to a new one.</p>"},{"location":"docs/flight-environment/use-flight/flight-admin-tools/hunter/#remove-node-label","title":"<code>remove-node &lt;label&gt;</code>","text":"<p>Remove a node from the parsed list.</p> <ul> <li><code>--buffer</code> - Remove from the buffer list, and used the node id instead.</li> <li><code>--name</code> - Specify the node by regex matching the hostname instead.</li> </ul> <p>Tip</p> <p>You can select multiple nodes at once by writing a comma separated list, or with square bracket expansion (like genders syntax). For example, <code>remove node[01-02] compute</code> would remove <code>node01</code> and <code>node02</code></p>"},{"location":"docs/flight-environment/use-flight/flight-admin-tools/hunter/#rename-group-group-new-name","title":"<code>rename-group &lt;group&gt; &lt;new name&gt;</code>","text":"<p>Renames a group, keeping all the nodes it contains.</p> <ul> <li><code>--buffer</code> - Uses the buffer list instead.</li> </ul>"},{"location":"docs/flight-environment/use-flight/flight-admin-tools/hunter/#show-label","title":"<code>show &lt;label&gt;</code>","text":"<p>Shows the details of a node, identifying it by label</p> <ul> <li><code>--buffer</code> - View a node in the buffer list instead, identifying it by id.</li> <li><code>--plain</code> - Print in a plain format, more easily machine readable.</li> </ul>"},{"location":"docs/flight-environment/use-flight/flight-admin-tools/pdsh/","title":"Flight PDSH","text":"<p>The Flight PDSH command provides a build of PDSH that sits within the flight filesystem and defaults to using a genders file located at <code>/opt/flight/etc/genders</code>.</p> <p>If a system pdsh package is installed then the priority of the conflicting pdsh commands can be set with the <code>flight config</code> command. To use the flight pdsh command by default run: <pre><code>flight config set pdsh.priority embedded\n</code></pre></p> <p>To use the system one, set the <code>pdsh.priority</code> to <code>system</code>. </p> <p>Further information on using genders and pdsh can be found in the HPC Environment Basics section.</p>"},{"location":"docs/flight-environment/use-flight/flight-admin-tools/profile/","title":"Flight Profile","text":"<p>Flight profile is a tool that manages the profiles of cluster nodes. In this context, a profile is a cluster type (e.g. SLURM, Kubernetes, Jupyter Lab), and the different types of nodes are identities. For example, a profile might be <code>slurm multinode</code>, and the identities would be <code>login</code> and <code>compute</code>. This page covers the sub-commands of <code>flight profile</code>, which are run in the format <code>flight profile &lt;sub-command&gt; --&lt;option&gt;</code>.</p>"},{"location":"docs/flight-environment/use-flight/flight-admin-tools/profile/#using-flight-profile","title":"Using Flight Profile","text":"<p>Generally speaking, the process of utilising Flight Profile is to:</p> <ul> <li> <p>Prepare the type you wish to use</p> <p>Info</p> <p>The type you wish to use will need to be prepared on all nodes that are to be included in the cluster, this is because the preparation process installs the required dependencies for the cluster type for the current system only</p> </li> <li> <p>Run through the configure prompt to customise the type</p> </li> <li>Apply identities from within the profile to hosts</li> </ul> <p>The various commands of Flight profile are described in further detail below.</p>"},{"location":"docs/flight-environment/use-flight/flight-admin-tools/profile/#configure","title":"<code>configure</code>","text":"<p>When <code>flight profile configure</code> is run, the user will be guided through a series of questions that need to be answered to properly configure the cluster. There are too many questions to cover them all here, but the cluster build methods section has more information about specific clusters.</p> <ul> <li><code>--reset-type</code> - After a cluster type has been chosen, it will be locked in when <code>configure</code> is run in the future. This argument allows the user to reset and select a different cluster type.</li> <li><code>--show</code> - Shows the answers for the current configuration.</li> <li><code>--accept-defaults</code> - When using <code>--answers</code>, take the default values for any answers not given.</li> <li> <p><code>--answers</code> - Pass json text as the answers instead of using the menu. This could be done either by pasting text or passing the contents of a file (e.g. <code>flight profile configure --answers \"$(cat /path/file.json)\"</code> ). Below are some examples of what the json could look like be:</p> Slurm StandaloneSlurm MultinodeKubernetes MultinodeJupyter <pre><code>{\n  \"cluster_type\": \"openflight-slurm-standalone\",\n  \"cluster_name\": \"my-cluster\",\n  \"default_username\": \"flight\",\n  \"default_password\": \"0penfl1ght\",\n  \"access_host\": \"51.104.217.61\"\n}\n</code></pre> <pre><code>{\n  \"cluster_type\": \"openflight-slurm-multinode\",\n  \"cluster_name\": \"my-cluster\",\n  \"ipa_use\": \"false\",\n  \"default_username\": \"flight\",\n  \"default_password\": \"0penfl1ght\",\n  \"access_host\": \"51.104.217.61\",\n  \"compute_ip_range\": \"10.10.0.0/16\"\n}\n</code></pre> <pre><code>{\n  \"cluster_type\": \"openflight-kubernetes-multinode\",\n  \"cluster_name\": \"my-cluster\",\n  \"default_username\": \"flight\",\n  \"default_password\": \"0penfl1ght\",\n  \"access_host\": \"51.104.217.61\",\n  \"compute_ip_range\": \"10.10.0.0/16\",\n  \"pod_ip_range\": \"192.168.0.0/16\"\n}\n</code></pre> <pre><code>{\n  \"cluster_type\": \"openflight-jupyter-standalone\",\n  \"cluster_name\": \"my-cluster\",\n  \"default_username\": \"flight\",\n  \"default_password\": \"0penfl1ght\",\n  \"access_host\": \"51.104.217.61\"\n}\n</code></pre> </li> </ul> <p>An example of using <code>answers</code> to configure a slurm multinode cluster would be: <pre><code>flight profile configure --answers '{  \"cluster_type\": \"openflight-slurm-standalone\",  \"cluster_name\": \"my-cluster\",  \"default_username\": \"flight\",  \"default_password\": \"0penfl1ght\",  \"access_host\": \"51.104.217.61\"}'\n</code></pre></p> <p>Tip</p> <p>The OpenFlight Cluster Types provided along with Flight Profile attempt to determine sensible defaults which will aid in providing the correct information for configuring the cluster</p>"},{"location":"docs/flight-environment/use-flight/flight-admin-tools/profile/#apply-nodenode2-identity","title":"<code>apply &lt;node,node2...&gt; &lt;identity&gt;</code>","text":"<p>Applies an identity to one or more nodes. e.g. <code>flight profile apply node01,node02</code> or <code>flight profile apply node01</code>. If an identity's dependencies are not met then the application of the identity to the node will become queued until requirements are satisfied.</p> <ul> <li><code>--force</code> - Overwrite the identity of a node that has already been applied to.</li> <li><code>--remove-on-shutdown</code> - Adds a systemd hook to the node which will trigger removal from the cluster on shutdown if the node's identity supports removal.</li> <li><code>--wait</code> - Don't background the process of removal </li> <li><code>--groups</code> - Select nodes to apply to based on hunter groups</li> <li><code>--detect-identity</code> - Attempt to automatically determine the identity to apply based on the hunter groups of the node. If any group matches that of the available identities for the current cluster type, it will be applied. </li> <li><code>--dry-run</code> - Show which identities would be applied to which nodes without actually performing apply process</li> </ul> <p>Tip</p> <p>You can select multiple nodes at once by writing a comma separated list, or with square bracket expansion (like genders syntax). For example, <code>apply node[01-02] compute</code> would apply <code>compute</code> to <code>node01</code> and <code>node02</code></p>"},{"location":"docs/flight-environment/use-flight/flight-admin-tools/profile/#avail","title":"<code>avail</code>","text":"<p>Lists the available cluster types.</p>"},{"location":"docs/flight-environment/use-flight/flight-admin-tools/profile/#clean-node","title":"<code>clean &lt;node&gt;</code>","text":"<p>Removes the data for one or more nodes that failed application or removal from appearing in the list of profile accessible nodes. This means the node will show as <code>available</code>.</p>"},{"location":"docs/flight-environment/use-flight/flight-admin-tools/profile/#dequeue-node","title":"<code>dequeue &lt;node&gt;</code>","text":"<p>Removes a node from the apply queue.</p>"},{"location":"docs/flight-environment/use-flight/flight-admin-tools/profile/#identities-type","title":"<code>identities &lt;type&gt;</code>","text":"<p>Shows a list of all available identities for a cluster type. If there is no given cluster type, identities will be shown for the currently configured cluster type.</p>"},{"location":"docs/flight-environment/use-flight/flight-admin-tools/profile/#list","title":"<code>list</code>","text":"<p>Displays the identity and status of every node available to profile. e.g. <pre><code>[flight@chead1 (mycluster1) [login1] ~]$ flight profile list\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Node   \u2502 Identity \u2502 Status    \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 login1 \u2502 login    \u2502 complete  \u2502\n\u2502 node02 \u2502 compute  \u2502 failed    \u2502\n\u2502 node01 \u2502          \u2502 available \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre></p>"},{"location":"docs/flight-environment/use-flight/flight-admin-tools/profile/#prepare-type","title":"<code>prepare &lt;type&gt;</code>","text":"<p>Prepares a cluster type, completing dependencies. If no cluster type is specified then the currently configured one is used.</p> <p>Info</p> <p>The type you wish to use will need to be prepared on all nodes that are to be included in the cluster, this is because the preparation process installs the required dependencies for the cluster type for the current system only</p>"},{"location":"docs/flight-environment/use-flight/flight-admin-tools/profile/#remove-nodenode","title":"<code>remove &lt;node,node...&gt;</code>","text":"<p>Removes the identity of a node, so that it is no longer works as part of the cluster.</p> <ul> <li><code>--remove-hunter-entry</code> - Also remove it from the hunter list.</li> <li><code>--force</code> - Bypass restrictions on using <code>remove</code> on a node.</li> <li><code>--wait</code> - Don't background the process of removal </li> </ul> <p>Note</p> <p><code>remove</code> is limited to only some identities, so not all identities can be removed.</p> <p>Tip</p> <p>You can select multiple nodes at once by writing a comma separated list, or with square bracket expansion (like genders syntax). For example, <code>remove node[01-02] compute</code> would remove <code>node01</code> and <code>node02</code></p>"},{"location":"docs/flight-environment/use-flight/flight-admin-tools/profile/#view-node","title":"<code>view &lt;node&gt;</code>","text":"<p>Shows the setup/removal progress of a node and its current status - <code>--raw</code> - Shows the entire ansible log output. - <code>--watch</code> - View the process \"live\", regularly updates the output on the screen to show current status</p>"},{"location":"docs/flight-environment/use-flight/flight-admin-tools/profile/#auto-apply","title":"Auto-apply","text":"<p>Profile can automatically apply an identity to a node with the auto-apply configuration. This is done by configuring <code>flight hunter hunt</code> to automatically apply identities whenever a node is parsed with <code>auto-parse</code>.</p> <p>Warning</p> <p>You must have already configured flight profile with the <code>configure</code> command or else auto-applying will not work.</p>"},{"location":"docs/flight-environment/use-flight/flight-admin-tools/profile/#setup-auto-apply","title":"Setup Auto-apply","text":"<ol> <li> <p>Open the file <code>/opt/flight/opt/hunter/etc/config.yml</code></p> <p>Note</p> <p>You will need to have root user permissions to edit this config file.</p> </li> <li> <p>Add these lines:      <pre><code>auto_apply:\n  &lt;regex&gt;: &lt;identity&gt;\n</code></pre></p> <ul> <li>You can add extra lines of <code>&lt;regex&gt;: &lt;identity&gt;</code> to catch more identities. e.g.     <pre><code>auto_apply:\n  cnode: compute\n  chead: login\n</code></pre></li> </ul> </li> <li>Restart the hunter service with <code>flight service restart hunter</code>.<ul> <li>Alternatively, you can stop the hunter service with <code>flight service stop hunter</code> and then run <code>flight hunter hunt</code>.</li> </ul> </li> </ol>"},{"location":"docs/flight-environment/use-flight/flight-admin-tools/profile/#auto-remove","title":"Auto-remove","text":"<p>Profile can automatically remove nodes when they are shutdown, additionally it can remove them from the hunter inventory when they are successfully removed. This setup, coupled with auto-apply, can create a cluster that dynamically grows and shrinks with an auto-scaling group on various cloud platforms. </p>"},{"location":"docs/flight-environment/use-flight/flight-admin-tools/profile/#setup-auto-remove","title":"Setup Auto-remove","text":"<ol> <li> <p>Open the file <code>/opt/flight/opt/hunter/etc/config.yml</code></p> <p>Note</p> <p>You will need to have root user permissions to edit this config file.</p> </li> <li> <p>Add the following lines:     <pre><code>remove_on_shutdown: true\nremove_hunter_entry: true\n</code></pre></p> </li> </ol> <p>Now when a node has an identity applied to it then the service to automatically trigger removal on shutdown will be added to it. Further to this, when the node is successfully removed then the corresponding hunter entry will also be removed. </p>"},{"location":"docs/flight-environment/use-flight/flight-user-suite/","title":"Flight User Suite","text":"<p>The Flight User Suite is a collection of CLI tools aimed to improve general HPC environment access by streamlining common workflow tasks. Things like launching desktop sessions, adding software ecosystems and managing object storage.</p>"},{"location":"docs/flight-environment/use-flight/flight-user-suite/flight-desktop/","title":"Flight Desktop","text":"<p>Your Flight User Suite login node can run graphical desktop sessions to support users who want to run interactive applications across the research environment. The system can support a number of different sessions simultaneously, and allow multiple remote participants to connect to the same session to support training and collaborative activities. This section of the documentation describes how to setup and use this tool.</p> <p>Tip</p> <p>The Flight Web Suite provides an alternative method for managing desktop sessions in the HPC environment. </p>"},{"location":"docs/flight-environment/use-flight/flight-user-suite/flight-desktop/launch/","title":"Launch a Desktop Session","text":"<p>Users can launch a new session by using the <code>flight desktop start gnome</code> command. After launching the desktop, a message will be printed with connection details to access the new session:</p> <pre><code>Starting a 'gnome' desktop session:\n\n   &gt; \u2705 Starting session\n\nA 'gnome' desktop session has been started.\n\n== Session details ==\n      Name:\n  Identity: 4549eae1-6f8b-4983-8057-99b378afcdd3\n      Type: gnome\n   Host IP: 51.104.217.61\n  Hostname: chead1\n      Port: 5901\n   Display: :1\n  Password: mkO3Zxjl\n  Geometry: 1024x768\n\nThis desktop session is not directly accessible from outside of your\ncluster as it is running on a machine that only provides internal\ncluster access.  In order to access your desktop session you will need\nto perform port forwarding using 'ssh'.\n\nRefer to 'flight desktop show 4549eae1' for more details.\n\nIf prompted, you should supply the following password: mkO3Zxjl\n</code></pre> <p>Users need a VNC client to connect to the graphical desktop session - for a list of tested clients, see prerequisites.</p> <p>Users with Mac clients can use the URL provided in the command output to connect to the session; e.g. from the above example, simply enter <code>vnc://flight:mkO3Zxjl@51.104.217.61:5901</code> into the Safari address bar.</p> <p>Linux and Windows users should enter the IP address and port number shown into their VNC client in the format <code>IP:port</code>. For example - for the output above, Linux and Windows client users would enter <code>51.104.217.61:5901</code> into their VNC client:</p> <p></p> <p>A one-time randomized password is automatically generated by the flight system when a new session is started. Linux and Windows users may be prompted to enter this password when they connect to the desktop session.</p> <p>Once connected to the graphical desktop, users can use the session as they would a local Linux machine:</p> <p></p>"},{"location":"docs/flight-environment/use-flight/flight-user-suite/flight-desktop/port-forward/","title":"Port Forwarding a Desktop Session","text":"<p>Depending on your connection to the HPC you may see this message when starting a desktop session:</p> <pre><code>This desktop session is not directly accessible from outside of your\ncluster as it is running on a machine that only provides internal\ncluster access.  In order to access your desktop session you will need\nto perform port forwarding using 'ssh'.\n</code></pre> <p>For example in the following:</p> <pre><code>[flight@chead1 (mycluster1) ~]$ flight desktop start gnome\nStarting a 'gnome' desktop session:\n\n   &gt; \u2705 Starting session\n\nA 'gnome' desktop session has been started.\n\n== Session details ==\n      Name:\n  Identity: dd8acf76-1494-4c88-adb1-a8bbd405d965\n      Type: gnome\n   Host IP: 20.68.202.163\n  Hostname: chead1\n      Port: 5903\n   Display: :3\n  Password: WB3gUQMW\n  Geometry: 1024x768\n\nThis desktop session is not directly accessible from outside of your\ncluster as it is running on a machine that only provides internal\ncluster access.  In order to access your desktop session you will need\nto perform port forwarding using 'ssh'.\n\nRefer to 'flight desktop show dd8acf76' for more details.\n\nIf prompted, you should supply the following password: WB3gUQMW\n</code></pre> <p>By running the command <code>flight desktop show &lt;name&gt;</code> we can see more information</p> <pre><code>[flight@chead1 (mycluster1) ~]$ flight desktop show dd8acf76\n\n== Session details ==\n      Name:\n  Identity: dd8acf76-1494-4c88-adb1-a8bbd405d965\n      Type: gnome\n   Host IP: 20.68.202.163\n  Hostname: chead1\n      Port: 5903\n   Display: :3\n  Password: WB3gUQMW\n  Geometry: 1024x768\n\nThis desktop session is not directly accessible from outside of your\ncluster as it is running on a machine that only provides internal\ncluster access.  In order to access your desktop session you will need\nto perform port forwarding using 'ssh':\n\n  ssh -L 5903:20.68.202.163:5903 flight@\n\nOnce the ssh connection has been established, depending on your\nclient, you can connect to the session using one of:\n\n  vnc://flight:WB3gUQMW@localhost:5903\n  localhost:5903\n  localhost:3\n\nIf, when connecting, you receive a warning as follows, try again with\na different port number, e.g. 5904, 5905 etc.:\n\n  channel_setup_fwd_listener_tcpip: cannot listen to port: 5903\n\nIf prompted, you should supply the following password: WB3gUQMW\n</code></pre>  Windows Linux /  Mac <p>On Windows, the desktop environment can still be connected like so:</p> <ol> <li> <p>Begin your environment as demonstrated on the previous page.</p> </li> <li> <p>Note down the IP address, port number and password.</p> </li> <li> <p>Open PuTTy, or get it from the prerequisites page and then open it.</p> </li> <li> <p>Load one of your saved sessions or save a new session with your log in details.</p> <p></p> </li> <li> <p>Find the \"Category:\" section on the left of the window</p> <ol> <li>Scroll down to \"Connection\" and expand it.</li> <li>Scroll down to \"SSH\" and expand it.</li> <li> <p>Click on the \"Tunnels page.</p> <p></p> </li> </ol> </li> <li> <p>Input the source port (a local port) and destination which is <code>IP address:port number</code> of the desktop environment that was started.</p> <p></p> </li> <li> <p>After inputting the information, click \"Add\" and the details will move to a box above.</p> <p></p> </li> <li> <p>Scroll back to the top of the \"Category:\" section, and click on \"Session\"</p> <p></p> </li> <li> <p>Save the session and then click open to run the command line interface and log in.</p> </li> <li> <p>Open your VNC client, and type <code>localhost:XXXXX</code> where <code>XXXX</code> is the source port number you entered earlier.</p> <p></p> </li> <li> <p>Click \"Connect\", and if prompted enter the password you noted down.</p> </li> </ol> <p>The steps for connecting with Linux/Mac are outlined in the output of the <code>flight desktop show</code> command above.</p>"},{"location":"docs/flight-environment/use-flight/flight-user-suite/flight-desktop/prepare/","title":"Preparing a Desktop Type","text":""},{"location":"docs/flight-environment/use-flight/flight-user-suite/flight-desktop/prepare/#view-available-types","title":"View Available Types","text":"<p>Your research environment supports many types of graphical session designed to provide interactive applications directly to users. To view the available types of session, use the command <code>flight desktop avail</code>:</p> <pre><code>[flight@chead1 (mycluster1) ~]$ flight desktop avail\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Name  \u2502 Summary                                          \u2502 State      \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 gnome \u2502 GNOME v3, a free and open-source desktop         \u2502 Unverified \u2502\n\u2502       \u2502 environment for Unix-like operating systems.     \u2502            \u2502\n\u2502       \u2502                                                  \u2502            \u2502\n\u2502       \u2502  &gt; https://www.gnome.org/                        \u2502            \u2502\n\u2502       \u2502                                                  \u2502            \u2502\n\u2502 kde   \u2502 KDE Plasma Desktop (KDE 4). Plasma is KDE's      \u2502 Unverified \u2502\n\u2502       \u2502 desktop environment. Simple by default, powerful \u2502            \u2502\n\u2502       \u2502 when needed.                                     \u2502            \u2502\n\u2502       \u2502                                                  \u2502            \u2502\n\u2502       \u2502  &gt; https://kde.org/                              \u2502            \u2502\n\u2502       \u2502                                                  \u2502            \u2502\n\u2502 xfce  \u2502 Xfce is a lightweight desktop environment        \u2502 Unverified \u2502\n\u2502       \u2502 for UNIX-like operating systems. It aims to be   \u2502            \u2502\n\u2502       \u2502 fast and low on system resources, while still    \u2502            \u2502\n\u2502       \u2502 being visually appealing and user friendly.      \u2502            \u2502\n\u2502       \u2502                                                  \u2502            \u2502\n\u2502       \u2502  &gt; https://xfce.org/                             \u2502            \u2502\n\u2502       \u2502                                                  \u2502            \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n[flight@chead1 (mycluster1) ~]$\n</code></pre> <p>Application types that are <code>unverified</code> need to be prepared before they can be started.</p>"},{"location":"docs/flight-environment/use-flight/flight-user-suite/flight-desktop/prepare/#preparing-a-type","title":"Preparing a Type","text":"<p>To prepare a new session type, use the command <code>flight desktop prepare &lt;type&gt;</code> (preparing will automatically install any required application and support files, if these dependencies have been installed manually then a desktop session can be checked for verification with <code>flight desktop verify &lt;type&gt;</code>). Once enabled, users can start a new session using the command <code>flight desktop start &lt;type&gt;</code>.</p> <p>Note</p> <p>The <code>prepare</code> command is only available to the <code>root</code> user as it requires installation of packages</p> <p>Note</p> <p>Preparing a new session type only enables it for the machine that you run the command from, any other nodes will need to have the type enabled too.</p>"},{"location":"docs/flight-environment/use-flight/flight-user-suite/flight-desktop/prepare/#using-sudo-to-prepare","title":"Using sudo to prepare","text":"<p>Since only the root user can use <code>prepare</code>, you also cannot use <code>sudo</code> to run <code>prepare</code>.</p> <p>Instead the user must become the root user and enable the Flight system then run <code>prepare</code>.</p>"},{"location":"docs/flight-environment/use-flight/flight-user-suite/flight-desktop/resize/","title":"Resizing a Desktop Session","text":"<p>When launching a graphical desktop session using the <code>flight desktop</code> utility, a session resolution can be specified using the <code>--geometry &lt;size&gt;</code> option. For example, to launch a <code>gnome</code> desktop session with a resolution of 1920x1080 pixels, use the command:</p> <pre><code>flight desktop start --geometry 1920x1080 gnome\n</code></pre> <p>By default, your graphical desktop session will launch with a compatibility resolution of 1024x768. For example, to change the default resolution to 1920x1080 pixels, use the command: <pre><code>flight desktop set geometry 1920x1080\n</code></pre></p> <p>Users can resize the desktop to fit their screens using the command <code>flight desktop resize &lt;session id&gt; &lt;resolution&gt;</code>. For example: <pre><code>flight desktop resize 4549eae1 1920x1080\n</code></pre></p> <p>Your graphical desktop session will automatically resize to the new resolution requested. Use your local VNC client application to adjust the compression ratio, colour depth and frame-rate sessions in order to achieve the best user-experience for the desktop session.</p>"},{"location":"docs/flight-environment/use-flight/flight-user-suite/flight-desktop/secure/","title":"Securing your Desktop Session","text":"<p>As the VNC protocol does not natively provide support for security protocols such as SSL, you may wish to take steps to secure access to your VNC sessions.</p> <p>Several third party tools exist to help you secure your VNC connections.  One option is ssvnc, available here.</p> <p>Alternatively, you could use an SSH tunnel to access your session. Refer to online guides for setup instructions.</p>"},{"location":"docs/flight-environment/use-flight/flight-user-suite/flight-desktop/viewing-terminating/","title":"Viewing and Terminating Sessions","text":"<p>Users can view a list of the currently running sessions by using the command <code>flight desktop list</code>.</p> <pre><code>[flight@chead1 (mycluster1) ~]$ flight desktop list\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Name \u2502 Identity \u2502 Type  \u2502 Host name \u2502 IP address    \u2502 Display (Port) \u2502 Password \u2502 State  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502      \u2502 4549eae1 \u2502 gnome \u2502 chead1    \u2502 20.68.202.163 \u2502 :1 (5901)      \u2502 mkO3Zxjl \u2502 Active \u2502\n\u2502      \u2502 52e44bdd \u2502 gnome \u2502 chead1    \u2502 20.68.202.163 \u2502 :3 (5903)      \u2502 5eAlaST0 \u2502 Active \u2502\n\u2502      \u2502 abbbe30b \u2502 gnome \u2502 chead1    \u2502 20.68.202.163 \u2502 :2 (5902)      \u2502 XLH7bV30 \u2502 Active \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>To display connection information for an existing session, use the command <code>flight desktop show &lt;session-ID&gt;</code>. This command allows users to review the IP-address, port number and one-time password settings for an existing session.</p> <pre><code>[flight@chead1 (mycluster1) ~]$ flight desktop show 4549eae1\n\n== Session details ==\n      Name:\n  Identity: 4549eae1-6f8b-4983-8057-99b378afcdd3\n      Type: gnome\n   Host IP: 20.68.202.163\n  Hostname: chead1\n      Port: 5901\n   Display: :1\n  Password: mkO3Zxjl\n  Geometry: 1024x768\n\nThis desktop session is not directly accessible from outside of your\ncluster as it is running on a machine that only provides internal\ncluster access.  In order to access your desktop session you will need\nto perform port forwarding using 'ssh':\n\n  ssh -L 5901:20.68.202.163:5901 flight@\n\nOnce the ssh connection has been established, depending on your\nclient, you can connect to the session using one of:\n\n  vnc://flight:mkO3Zxjl@localhost:5901\n  localhost:5901\n  localhost:1\n\nIf, when connecting, you receive a warning as follows, try again with\na different port number, e.g. 5902, 5903 etc.:\n\n  channel_setup_fwd_listener_tcpip: cannot listen to port: 5901\n\nIf prompted, you should supply the following password: mkO3Zxjl\n</code></pre> <p>Users can terminate a running session by ending their graphical application (e.g. by logging out of a Gnome session, or exiting a terminal session), or by using the <code>flight desktop kill &lt;session-ID&gt;</code> command. A terminated session will be immediately stopped, disconnecting any users.</p> <pre><code>flight desktop kill 4549eae1\n</code></pre>"},{"location":"docs/flight-environment/use-flight/flight-user-suite/flight-env/","title":"Flight Env","text":"<p>The breadth of application of HPC environments can be intimidating. There are many different users and workflows for every application.</p> <p>This part of the documentation aims to provide some suitable examples to demonstrate the ease of establishing consistent workflows and the flexibility provided by the flight tools in assisting with workflow creation.</p> <p>Traditionally, administrators handle a lot of workflow creation. From manually compiling applications all the way through to assisting users launching their workflows. With this in mind, these documents should be able to assist both admins and users to establish their own customised solutions with minimal disruption,</p>"},{"location":"docs/flight-environment/use-flight/flight-user-suite/flight-env/usage/","title":"Flight Env Command Usage","text":""},{"location":"docs/flight-environment/use-flight/flight-user-suite/flight-env/usage/#viewing-available-ecosystems","title":"Viewing Available Ecosystems","text":"<p>Various package-ecosystems are available for managing software on your research environment. These can be viewed by using the <code>env</code> subcommand:</p> <pre><code>flight env avail\n</code></pre>"},{"location":"docs/flight-environment/use-flight/flight-user-suite/flight-env/usage/#creating-a-local-ecosystem","title":"Creating a Local Ecosystem","text":"<p>A local ecosystem is only available to the user that creates it. All of the packages and libraries are installed to the users home directory.</p> <p>To install a package ecosystem, use the create command as follows (replacing easybuild with your desired package ecosystem):</p> <pre><code>flight env create easybuild\n</code></pre> <p>Once a package ecosystem has been installed, it needs to be activated for the session to be able to manage software with it:</p> <pre><code>[flight@chead1 (mycluster1) ~]$ flight env activate easybuild\n&lt;easybuild&gt; [flight@chead1 (mycluster1) ~]$\n</code></pre> <p>Tip</p> <p>Your preferred software ecosystem can be set to automatically activate for your user within the flight system by running <code>flight env set-default easybuild</code>, replacing easybuild with your chosen software ecosystem</p>"},{"location":"docs/flight-environment/use-flight/flight-user-suite/flight-env/usage/#creating-a-global-ecosystem","title":"Creating a Global Ecosystem","text":"<p>A global ecosystem is available to all users on the system. All of the packages and libraries are installed to a shared storage directory. The global directories can be configured in <code>/opt/flight/opt/env/etc/config.yml</code> with the <code>global_depot_path:</code> and <code>global_build_cache_path</code> keys.</p> <p>Note</p> <p>The user requires suitable write permissions to the configured global depot paths in order to be able to create a global ecosystem</p> <p>To install a global package ecosystem, use the create command with the global option flag:</p> <pre><code>flight env create -g easybuild\n</code></pre> <p>Once the global ecosystem has been installed, it needs to be activated for the session to be able to monitor software with it:</p> <pre><code>[root@chead1 (mycluster1) ~]$ flight env activate easybuild@global\n&lt;easybuild@global&gt; [flight@chead1 (mycluster1) ~]$\n</code></pre>"},{"location":"docs/flight-environment/use-flight/flight-user-suite/flight-env/usage/#custom-ecosystem-names","title":"Custom Ecosystem Names","text":"<p>When installing an ecosystem, a custom alias can be added by appending <code>@mycustomname</code> to the end of creation command. For example, to create a local gridware installation with the alias <code>test</code>:</p> <pre><code>flight env create easybuild@test\n</code></pre> <p>To activate this environment, the alias will need to be specified in the activation command:</p> <pre><code>[flight@chead1 (mycluster1) ~]$ flight env activate easybuild@test\n&lt;easybuild@test&gt; [flight@chead1 (mycluster1) ~]$\n</code></pre>"},{"location":"docs/flight-environment/use-flight/flight-user-suite/flight-env/ecosystems/","title":"Package Ecosystems","text":"<p>The Flight Env command provides streamlined installation of many popular software management ecosystems. These can be seen below along with the features they provide.</p> Conda Easybuild Modules Singularity Spack Dependency Management Compilation Preconfigured Binaries Multiple Versions <p>More information on the features:</p> <ul> <li>Dependency Management - The ecosystem automatically resolves and installs dependencies</li> <li>Compilation - The ecosystem builds software from source for optimised functionality on the system (Note: these sorts of installations will take a long time)</li> <li>Preconfigured Binaries - The ecosystem provides binaries of the available software for quicker installation to get applications installed quickly</li> <li>Multiple Versions - The ecosystem allows for installation of multiple versions of software which can be toggled between</li> </ul> <p> is for partial support. Technically these features are supported in certain use cases and with additional work. This is not considered full support as multiple versions may require undocumented or uncommon use-cases of the tool</p>"},{"location":"docs/flight-environment/use-flight/flight-user-suite/flight-env/ecosystems/conda/","title":"Conda Usage Example","text":"<p>Conda is an open source package management system and environment management system that runs on Windows, macOS and Linux. Conda quickly installs, runs and updates packages and their dependencies. Conda easily creates, saves, loads and switches between environments on your local computer. It was created for Python programs, but it can package and distribute software for any language.</p>"},{"location":"docs/flight-environment/use-flight/flight-user-suite/flight-env/ecosystems/conda/#creating-and-using-ecosystem","title":"Creating and Using Ecosystem","text":"<p>Flight Env provides quick setup methods to create a conda software ecosystem.</p> <p>To install and use conda:</p> <ul> <li>Activate the flight system.</li> <li>Create the conda installation for the user:</li> </ul> <pre><code>[flight@chead1 ~]$ flight env create conda\nCreating environment conda@default\n   &gt; \u2705 Verifying prerequisites\n   &gt; \u2705 Fetching prerequisite (miniconda)\n   &gt; \u2705 Creating environment (conda@default)\nEnvironment conda@default has been created\n</code></pre> <ul> <li>Activate the conda ecosystem:</li> </ul> <pre><code>[flight@chead1 ~]$ flight env activate conda\n(base) &lt;conda&gt; [flight@chead1 ~]$\n</code></pre> <ul> <li>Check that conda can be run: <pre><code>(base) &lt;conda&gt; [flight@chead1 ~]$ conda --version\nconda 4.7.10\n</code></pre></li> </ul>"},{"location":"docs/flight-environment/use-flight/flight-user-suite/flight-env/ecosystems/conda/#installing-and-running-perl","title":"Installing and Running Perl","text":"<p>An example workflow using perl is demonstrated below.</p> <ul> <li>View available versions:</li> </ul> <pre><code>(base) &lt;conda&gt; [flight@chead1 ~]$ conda search perl\nLoading channels: done\n# Name                       Version           Build  Channel\nperl                          5.26.0      hae598fd_0  pkgs/main\nperl                          5.26.2      h14c3975_0  pkgs/main\n</code></pre> <ul> <li> <p>Install specific version: <pre><code>(base) &lt;conda&gt; [flight@chead1 ~]$ conda install perl=5.26.2\nCollecting package metadata (current_repodata.json): done\nSolving environment: done\n\n## Package Plan ##\n\n  environment location: /home/flight/.local/share/flight/env/conda+default\n\n  added / updated specs:\n    - perl=5.26.2\n\n\nThe following packages will be downloaded:\n\n    package                    |            build\n    ---------------------------|-----------------\n    ca-certificates-2019.5.15  |                1         134 KB\n    certifi-2019.6.16          |           py37_1         156 KB\n    conda-4.7.11               |           py37_0         3.0 MB\n    perl-5.26.2                |       h14c3975_0        10.5 MB\n    ------------------------------------------------------------\n                                           Total:        13.7 MB\n\nThe following NEW packages will be INSTALLED:\n\n  perl               pkgs/main/linux-64::perl-5.26.2-h14c3975_0\n\nThe following packages will be UPDATED:\n\n  ca-certificates                               2019.5.15-0 --&gt; 2019.5.15-1\n  certifi                                  2019.6.16-py37_0 --&gt; 2019.6.16-py37_1\n  conda                                       4.7.10-py37_0 --&gt; 4.7.11-py37_0\n\n\nProceed ([y]/n)? y\n\n\nDownloading and Extracting Packages\nconda-4.7.11         | 3.0 MB    | ############################################################################################ | 100%\ncertifi-2019.6.16    | 156 KB    | ############################################################################################ | 100%\nperl-5.26.2          | 10.5 MB   | ############################################################################################ | 100%\nca-certificates-2019 | 134 KB    | ############################################################################################ | 100%\nPreparing transaction: done\nVerifying transaction: done\n    Executing transaction: done\n</code></pre></p> </li> <li> <p>Check installation location: <pre><code>(base) &lt;conda&gt; [flight@chead1 ~]$ which perl\n~/.local/share/flight/env/conda+default/bin/perl\n</code></pre></p> </li> <li> <p>Install perl library (this may prompt for initial <code>cpan</code> configuration, once configuration is complete then the library will be installed):</p> </li> </ul> <pre><code>(base) &lt;conda&gt; [flight@chead1 ~]$ cpan File::Slurp\nLoading internal null logger. Install Log::Log4perl for logging messages\nReading '/home/flight/.cpan/Metadata'\n  Database was generated on Mon, 09 Sep 2019 15:17:03 GMT\nRunning install for module 'File::Slurp'\n&lt;-- snip --&gt;\n</code></pre> <ul> <li>Check installation worked: <pre><code>(base) &lt;conda&gt; [flight@chead1 ~]$ cpan File::Slurp\nLoading internal null logger. Install Log::Log4perl for logging messages\nReading '/home/flight/.cpan/Metadata'\n  Database was generated on Mon, 09 Sep 2019 15:17:03 GMT\nFile::Slurp is up to date (9999.27).\n</code></pre></li> </ul>"},{"location":"docs/flight-environment/use-flight/flight-user-suite/flight-env/ecosystems/easybuild/","title":"Easybuild Usage Example","text":"<p>EasyBuild is a software build and installation framework that allows you to manage (scientific) software on High Performance Computing (HPC) systems in an efficient way.</p>"},{"location":"docs/flight-environment/use-flight/flight-user-suite/flight-env/ecosystems/easybuild/#creating-and-using-ecosystem","title":"Creating and Using Ecosystem","text":"<p>Flight Env provides quick setup methods to create an easybuild software ecosystem.</p> <p>To install and use easybuild:</p> <ul> <li>Activate the flight system.</li> <li>Create the easybuild installation for the user:</li> </ul> <pre><code>[flight@chead1 ~]$ flight env create easybuild\nCreating environment easybuild@default\n   &gt; \u2705 Verifying prerequisites\n   &gt; \u2705 Fetching prerequisite (lua)\n   &gt; \u2705 Extracting prerequisite (lua)\n   &gt; \u2705 Building prerequisite (lua)\n   &gt; \u2705 Installing prerequisite (lua)\n   &gt; \u2705 Fetching prerequisite (tcl)\n   &gt; \u2705 Extracting prerequisite (tcl)\n   &gt; \u2705 Building prerequisite (tcl)\n   &gt; \u2705 Installing prerequisite (tcl)\n   &gt; \u2705 Fetching prerequisite (lmod)\n   &gt; \u2705 Extracting prerequisite (lmod)\n   &gt; \u2705 Configuring prerequisite (lmod)\n   &gt; \u2705 Installing prerequisite (lmod)\n   &gt; \u2705 Fetching prerequisite (easybuild)\n   &gt; \u2705 Bootstrapping EasyBuild environment (easybuild@default)\nEnvironment easybuild@default has been created\n</code></pre> <ul> <li>Activate the easybuild ecosystem: <pre><code>[flight@chead1 ~]$ flight env activate easybuild\n&lt;easybuild&gt; [flight@chead1 ~]$\n</code></pre></li> <li>Check that easybuild can be run: <pre><code>&lt;easybuild&gt; [flight@chead1 ~]$ module load EasyBuild\n&lt;easybuild&gt; [flight@chead1 ~]$ eb --version\nThis is EasyBuild 3.9.4 (framework: 3.9.4, easyblocks: 3.9.4) on host chead1.pri.basic.cluster.local.\n</code></pre></li> </ul>"},{"location":"docs/flight-environment/use-flight/flight-user-suite/flight-env/ecosystems/easybuild/#installing-and-running-perl","title":"Installing and Running Perl","text":"<p>An example workflow using perl is demonstrated below.</p> <ul> <li>View available versions: <pre><code>&lt;easybuild&gt; [flight@chead1 ~]$ eb -S perl\nCFGS1=/home/flight/.local/share/flight/env/easybuild+default/software/EasyBuild/3.9.4/lib/python2.7/site-packages/easybuild_easyconfigs-3.9.4-py2.7.egg/easybuild/easyconfigs\n * $CFGS1/a/annovar/annovar-2016Feb01-foss-2016a-Perl-5.22.1.eb\n * $CFGS1/b/Bio-DB-HTS/Bio-DB-HTS-2.11-foss-2017b-Perl-5.26.0.eb\n * $CFGS1/b/Bio-DB-HTS/Bio-DB-HTS-2.11-foss-2018b-Perl-5.28.0.eb\n * $CFGS1/b/Bio-DB-HTS/Bio-DB-HTS-2.11-intel-2017b-Perl-5.26.0.eb\n&lt;-- snip --&gt;\n * $CFGS1/p/Perl/Perl-5.26.1-GCCcore-6.4.0.eb\n * $CFGS1/p/Perl/Perl-5.26.1-foss-2018a-bare.eb\n * $CFGS1/p/Perl/Perl-5.26.1-foss-2018a.eb\n * $CFGS1/p/Perl/Perl-5.28.0-GCCcore-7.3.0.eb\n * $CFGS1/p/Perl/Perl-5.28.1-GCCcore-8.2.0.eb\n&lt;-- snip --&gt;\n * $CFGS1/y/YAML-Syck/YAML-Syck-1.27-goolf-1.4.10-Perl-5.16.3.eb\n * $CFGS1/y/YAML-Syck/YAML-Syck-1.27-ictce-5.3.0-Perl-5.16.3.eb\n\nNote: 9 matching archived easyconfig(s) found, use --consider-archived-easyconfigs to see them\n</code></pre></li> <li> <p>Install specific version: <pre><code>&lt;easybuild&gt; [flight@chead1 ~]$ eb Perl-5.28.1-GCCcore-8.2.0.eb --robot\n== temporary log file in case of crash /tmp/eb-MdohD2/easybuild-J6tjhZ.log\n== resolving dependencies ...\n== processing EasyBuild easyconfig /home/flight/.local/share/flight/env/easybuild+default/software/EasyBuild/3.9.4/lib/python2.7/site-packages/easybuild_easyconfigs-3.9.4-py2.7.egg/easybuild/easyconfigs/m/M4/M4-1.4.18.eb\n== building and installing M4/1.4.18...\n== fetching files...\n== creating build dir, resetting environment...\n== unpacking...\n== patching...\n== preparing...\n== configuring...\n== building...\n&lt;-- snip --&gt;\n</code></pre></p> </li> <li> <p>Check installation location: <pre><code>&lt;easybuild&gt; [flight@chead1 ~]$ which perl\n</code></pre></p> </li> <li>Install perl library (this may prompt for initial <code>cpan</code> configuration, once configuration is complete then the library will be installed): <pre><code>&lt;easybuild&gt; [flight@chead1 ~]$ cpan File::Slurp\nLoading internal null logger. Install Log::Log4perl for logging messages\nReading '/home/flight/.cpan/Metadata'\n  Database was generated on Mon, 09 Sep 2019 15:47:03 GMT\nRunning install for module 'File::Slurp'\n&lt;-- snip --&gt;\n</code></pre></li> <li>Check installation worked: <pre><code>&lt;easybuild&gt; [flight@chead1 ~]$ cpan File::Slurp\nLoading internal null logger. Install Log::Log4perl for logging messages\nReading '/home/flight/.cpan/Metadata'\n  Database was generated on Mon, 09 Sep 2019 15:47:03 GMT\nFile::Slurp is up to date (9999.27).\n</code></pre></li> </ul>"},{"location":"docs/flight-environment/use-flight/flight-user-suite/flight-env/ecosystems/modules/","title":"Modules Usage Example","text":"<p>Modules provides simple environment management. Tools and software to be used with modules will be compiled or installed outside of the modules environment.</p>"},{"location":"docs/flight-environment/use-flight/flight-user-suite/flight-env/ecosystems/modules/#creating-and-using-ecosystem","title":"Creating and Using Ecosystem","text":"<p>Flight Env provides quick setup methods to create a modules software ecosystem.</p> <p>To install and use modules:</p> <ul> <li>Activate the flight system.</li> <li>Create the modules installation for the user: <pre><code>[flight@chead1 ~]$ flight env create modules\nCreating environment modules@default\n   &gt; \u2705 Verifying prerequisites\n   &gt; \u2705 Fetching prerequisite (modules)\n   &gt; \u2705 Extracting prerequisite (modules)\n   &gt; \u2705 Building prerequisite (modules)\n   &gt; \u2705 Installing prerequisite (modules)\n   &gt; \u2705 Creating environment (modules@default)\nEnvironment modules@default has been created\n</code></pre></li> <li>Activate the modules ecosystem: <pre><code>[flight@chead1 ~]$ flight env activate modules\n&lt;modules&gt; [flight@chead1 ~]$\n</code></pre></li> <li>Check that modules can be run: <pre><code>&lt;modules&gt; [flight@chead1 ~]$ module --version\nModules Release 4.3.0 (2019-07-26)\n</code></pre></li> </ul>"},{"location":"docs/flight-environment/use-flight/flight-user-suite/flight-env/ecosystems/modules/#installing-software-general-overview","title":"Installing Software - General Overview","text":"<p>Unlike the other package ecosystems, Modules provides the ecosystem management tool but not any package management tools. Therefore, with the Modules ecosystem, you are free to compile and install software in a Module compatible manner.</p> <p>Module files can be installed to <code>~/.local/share/flight/env/modules+default/modulefiles</code> (for local Modules ecosystems) or to <code>/opt/apps/flight/env/modules+global/modulefiles</code> (for global modules ecosystems).</p> <p>!!!     If the Modules ecosystem has been installed with a custom ecosystem name then the path will not be <code>modules+default</code>/<code>modules+global</code> but instead <code>modules+mycustomname</code></p> <p>For more information on building software for Modules, see the modulefile reference and build documentation for the chosen software.</p>"},{"location":"docs/flight-environment/use-flight/flight-user-suite/flight-env/ecosystems/modules/#installing-and-running-perl","title":"Installing and Running Perl","text":""},{"location":"docs/flight-environment/use-flight/flight-user-suite/flight-env/ecosystems/modules/#compile-perl-from-source","title":"Compile Perl from Source","text":"<ul> <li>Download perl 5.30.1 source: <pre><code>&lt;modules&gt; [flight@chead1 ~]$ wget https://www.cpan.org/src/5.0/perl-5.30.1.tar.gz\n</code></pre></li> <li>Decompress the source files: <pre><code>&lt;modules&gt; [flight@chead1 ~]$ tar -xzf perl-5.30.1.tar.gz\n</code></pre></li> <li>Configure the software to install to a localperl directory: <pre><code>&lt;modules&gt; [flight@chead1 ~]$ cd perl-5.30.1\n&lt;modules&gt; [flight@chead1 ~]$ ./Configure -des -Dprefix=$HOME/localperl\n</code></pre></li> <li>Compile and install perl: <pre><code>&lt;modules&gt; [flight@chead1 ~]$ make\n&lt;modules&gt; [flight@chead1 ~]$ make install\n</code></pre></li> </ul>"},{"location":"docs/flight-environment/use-flight/flight-user-suite/flight-env/ecosystems/modules/#create-modulefile-and-test","title":"Create Modulefile and Test","text":"<ul> <li>Create the perl modulefile: <pre><code>&lt;modules&gt; [flight@chead1 ~]$ cat &lt;&lt; EOF &gt; ~/.local/share/flight/env/modules+default/modulefiles/perl-5.30.1\n#%Module1.0\nproc ModulesHelp { } {\nglobal dotversion\n\nputs stderr \"\\tPerl 5.30.1\"\n}\n\nmodule-whatis \"Perl 5.30.1\"\nconflict perl\nprepend-path PATH ~/localperl/bin\nprepend-path LD_LIBRARY_PATH ~/localperl/lib\nprepend-path LIBRARY_PATH ~/localperl/lib\nprepend-path MANPATH ~/localperl/man\nEOF\n</code></pre></li> <li>Check install location: <pre><code>&lt;modules&gt; [flight@chead1 ~]$ module load perl-5.30.1\n&lt;modules&gt; [flight@chead1 ~]$ which perl\n~/localperl/bin/perl\n</code></pre></li> <li>Install perl library (this may prompt for initial <code>cpan</code> configuration, once configuration is complete then the library will be installed): <pre><code>&lt;modules&gt; [flight@chead1 ~]$ cpan File::Slurp\nLoading internal logger. Log::Log4perl recommended for better logging\nReading '/home/flight/.cpan/Metadata'\n  Database was generated on Wed, 11 Mar 2020 15:29:03 GMT\n&lt;-- snip --&gt;\nAppending installation info to /home/flight/localperl/lib/5.30.1/x86_64-linux/perllocal.pod\n  CAPOEIRAB/File-Slurp-9999.30.tar.gz\n  /usr/bin/make install  -- OK\n</code></pre></li> <li>Check installation worked: <pre><code>&lt;modules&gt; [flight@chead1 ~]$ cpan File::Slurp\nLoading internal logger. Log::Log4perl recommended for better logging\nReading '/home/flight/.cpan/Metadata'\n  Database was generated on Wed, 11 Mar 2020 15:29:03 GMT\nFile::Slurp is up to date (9999.30).\n</code></pre></li> </ul>"},{"location":"docs/flight-environment/use-flight/flight-user-suite/flight-env/ecosystems/singularity/","title":"Singularity Usage Example","text":"<p>Singularity is high-performance container technology specifically designed to enhance Enterprise Performance Computing by building containers that support HPC, analytics, artificial intelligence, machine learning, and deep learning to provide \"intelligence anywhere\".</p>"},{"location":"docs/flight-environment/use-flight/flight-user-suite/flight-env/ecosystems/singularity/#creating-and-using-ecosystem","title":"Creating and Using Ecosystem","text":"<p>Flight Env provides quick setup methods to create a singularity software ecosystem.</p> <p>To install and use singularity:</p> <p>Warning</p> <p>If installing singularity for a user then there are a number of restrictions and additional steps to consider in configuring the environment. See the <code>Personal Environment</code> section of <code>flight env info singularity</code>.</p> <ul> <li>Activate the flight system.</li> <li>Create the singularity installation for the user: <pre><code>[flight@chead1 ~]$ flight env create singularity\nCreating environment singularity@default\n   &gt; \u2705 Verifying prerequisites\n   &gt; \u2705 Fetching prerequisite (squashfs)\n   &gt; \u2705 Extracting prerequisite (squashfs)\n   &gt; \u2705 Building prerequisite (squashfs)\n   &gt; \u2705 Installing prerequisite (squashfs)\n   &gt; \u2705 Fetching prerequisite (go)\n   &gt; \u2705 Extracting prerequisite (go)\n   &gt; \u2705 Fetching prerequisite (singularity)\n   &gt; \u2705 Extracting prerequisite (singularity)\n   &gt; \u2705 Building prerequisite (singularity)\n   &gt; \u2705 Installing prerequisite (singularity)\n   &gt; \u2705 Creating environment (singularity@default)\nEnvironment singularity@default has been created\n</code></pre></li> <li>Activate the singularity ecosystem: <pre><code>[flight@chead1 ~]$ flight env activate singularity\n&lt;singularity&gt; [flight@chead1 ~]$\n</code></pre></li> <li>Check that singularity can be run: <pre><code>&lt;singularity&gt; [flight@chead1 ~]$ singularity --version\nsingularity version 3.2.1\n</code></pre></li> </ul>"},{"location":"docs/flight-environment/use-flight/flight-user-suite/flight-env/ecosystems/singularity/#installing-and-running-perl","title":"Installing and Running Perl","text":"<p>An example workflow using perl is demonstrated below.</p> <p>Note</p> <p>The perl container is built from a docker container which can be searched for in the docker hub. To search the singularity container library, use <code>singularity search SEARCHTERM</code>.</p> <ul> <li>Install specific version: <pre><code>&lt;singularity&gt; [flight@chead1 ~]$ singularity build --sandbox perl_5.30.simg docker://perl:5.30\nINFO:    Starting build...\nGetting image source signatures\nCopying blob sha256:4ae16bd4778367b46064f39554128dd2fda2803a5747fddeff74059f353391c9\n 48.05 MiB / 48.05 MiB [====================================================] 0s\nCopying blob sha256:bbab4ec87ac4f89eaabdf68dddbd1dd930e3ad43bded38d761b89abf9389a893\n 7.44 MiB / 7.44 MiB [======================================================] 0s\n&lt;-- snip --&gt;\nWriting manifest to image destination\nStoring signatures\nINFO:    Creating sandbox directory...\nINFO:    Build complete: perl_5.30.simg\n</code></pre></li> <li>Check installation location: <pre><code>&lt;singularity&gt; [flight@chead1 ~]$ singularity exec perl_5.30.simg which perl\n/usr/local/bin/perl\n</code></pre></li> <li>Install perl library (this may prompt for initial <code>cpan</code> configuration, once configuration is complete then the library will be installed): <pre><code>&lt;singularity&gt; [flight@chead1 ~]$ singularity exec -w perl_5.30.simg cpan File::Slurp\nINFO:    Convert SIF file to sandbox...\nperl: warning: Setting locale failed.\nperl: warning: Please check that your locale settings:\n    LANGUAGE = (unset),\n    LC_ALL = (unset),\n    LC_CTYPE = \"en_GB.UTF-8\",\n    LANG = \"en_GB.UTF-8\"\n    are supported and installed on your system.\nperl: warning: Falling back to the standard locale (\"C\").\nLoading internal null logger. Install Log::Log4perl for logging messages\nReading '/home/flight/.cpan/Metadata'\n  Database was generated on Wed, 11 Sep 2019 13:29:02 GMT\nRunning install for module 'File::Slurp'\n&lt;-- snip --&gt;\n</code></pre></li> <li>Check installation worked: <pre><code>&lt;singularity&gt; [flight@chead1 ~]$ singularity exec perl_5.30.simg cpan File::Slurp\nperl: warning: Setting locale failed.\nperl: warning: Please check that your locale settings:\n    LANGUAGE = (unset),\n    LC_ALL = (unset),\n    LC_CTYPE = \"en_GB.UTF-8\",\n    LANG = \"en_GB.UTF-8\"\n    are supported and installed on your system.\nperl: warning: Falling back to the standard locale (\"C\").\nLoading internal logger. Log::Log4perl recommended for better logging\nReading '/home/flight/.cpan/Metadata'\n  Database was generated on Wed, 11 Sep 2019 13:29:02 GMT\nFile::Slurp is up to date (9999.27).\n</code></pre></li> </ul>"},{"location":"docs/flight-environment/use-flight/flight-user-suite/flight-env/ecosystems/spack/","title":"Spack Usage Example","text":"<p>Spack is a package manager for supercomputers, Linux, and macOS. It makes installing scientific software easy. With Spack, you can build a package with multiple versions, configurations, platforms, and compilers, and all of these builds can coexist on the same machine.</p>"},{"location":"docs/flight-environment/use-flight/flight-user-suite/flight-env/ecosystems/spack/#creating-and-using-ecosystem","title":"Creating and Using Ecosystem","text":"<p>Flight Env provides quick setup methods to create a spack software ecosystem.</p> <p>To install and use spack:</p> <ul> <li>Activate the flight system.</li> <li>Create the spack installation for the user: <pre><code>[flight@chead1 ~]$ flight env create spack\nCreating environment spack@default\n   &gt; \u2705 Verifying prerequisites\n   &gt; \u2705 Fetching prerequisite (spack)\n   &gt; \u2705 Extracting Spack hierarchy (spack@default)\n   &gt; \u2705 Bootstrapping Spack environment (spack@default)\nEnvironment spack@default has been created\n</code></pre></li> <li>Activate the spack ecosystem: <pre><code>[flight@chead1 ~]$ flight env activate spack\n&lt;spack&gt; [flight@chead1 ~]$\n</code></pre></li> <li>Check that spack can be run: <pre><code>&lt;spack&gt; [flight@chead1 ~]$ spack --version\nspack 0.12.1\n</code></pre></li> </ul>"},{"location":"docs/flight-environment/use-flight/flight-user-suite/flight-env/ecosystems/spack/#installing-and-running-perl","title":"Installing and Running Perl","text":"<p>An example workflow using perl is demonstrated below.</p> <ul> <li>View available versions: <pre><code>&lt;spack&gt; [flight@chead1 ~]$ spack list perl\n==&gt; 148 packages.\nperl                          perl-extutils-pkgconfig     perl-math-cdf                    perl-sub-uplevel\nperl-algorithm-diff           perl-file-copy-recursive    perl-math-cephes                 perl-svg\nperl-app-cmd                  perl-file-listing           perl-math-matrixreal             perl-swissknife\nperl-array-utils              perl-file-pushd             perl-module-build                perl-task-weaken\nperl-b-hooks-endofscope       perl-file-sharedir-install  perl-module-implementation       perl-term-readkey\n&lt;-- snip --&gt;\n\n&lt;spack&gt; [flight@chead1 ~]$ spack info perl\nPackage:   perl\n\nDescription:\n    Perl 5 is a highly capable, feature-rich programming language with over\n    27 years of development.\n\nHomepage: http://www.perl.org\n\nTags:\n    None\n\nPreferred version:\n    5.26.2     http://www.cpan.org/src/5.0/perl-5.26.2.tar.gz\n\nSafe versions:\n    5.28.0     http://www.cpan.org/src/5.0/perl-5.28.0.tar.gz\n    5.26.2     http://www.cpan.org/src/5.0/perl-5.26.2.tar.gz\n</code></pre></li> <li>Install specific version: <pre><code>&lt;spack&gt; [flight@chead1 ~]$ spack install perl@5.26.2\n==&gt; Installing pkgconf\n==&gt; Searching for binary cache of pkgconf\n==&gt; Warning: No Spack mirrors are currently configured\n==&gt; No binary for pkgconf found: installing from source\n==&gt; Fetching http://distfiles.alpinelinux.org/distfiles/pkgconf-1.4.2.tar.xz\n&lt;-- snip --&gt;\n==&gt; Successfully installed perl\n  Fetch: 0.83s.  Build: 2m 31.21s.  Total: 2m 32.04s.\n[+] /home/flight/.local/share/flight/env/spack+default/opt/spack/linux-centos7-x86_64/gcc-4.8.5/perl-5.26.2-wavwojlef7lshvx2awf4zze2lrx5l7l4\n</code></pre></li> <li>Check installation location: <pre><code>&lt;spack&gt; [flight@chead1 ~]$ module load perl-5.26.2-gcc-4.8.5-wavwojl\n&lt;spack&gt; [flight@chead1 ~]$ which perl\n~/.local/share/flight/env/spack+default/opt/spack/linux-centos7-x86_64/gcc-4.8.5/perl-5.26.2-wavwojlef7lshvx2awf4zze2lrx5l7l4/bin/perl\n</code></pre></li> <li>Install perl library (this may prompt for initial <code>cpan</code> configuration, once configuration is complete then the library will be installed): <pre><code>&lt;spack&gt; [flight@chead1 ~]$ cpan File::Slurp\nLoading internal null logger. Install Log::Log4perl for logging messages\nReading '/home/flight/.cpan/Metadata'\n  Database was generated on Wed, 11 Sep 2019 14:41:02 GMT\nRunning install for module 'File::Slurp'\n&lt;-- snip --&gt;\n</code></pre></li> <li>Check installation worked: <pre><code>&lt;spack&gt; [flight@chead1 ~]$ cpan File::Slurp\nLoading internal null logger. Install Log::Log4perl for logging messages\nReading '/home/flight/.cpan/Metadata'\n  Database was generated on Wed, 11 Sep 2019 14:41:02 GMT\nFile::Slurp is up to date (9999.27).\n</code></pre></li> </ul>"},{"location":"docs/flight-environment/use-flight/flight-user-suite/flight-silo/","title":"Flight Silo Overview","text":"<p>Flight Silo allows users to connect to \"silos\" - cloud storage systems designed primarily to distribute software and projects, with more general file storage also available. Silo comes installed as part of the User Suite included with Flight Solo.</p> <p>This section details the various commands used to manage silos, files and software.</p>"},{"location":"docs/flight-environment/use-flight/flight-user-suite/flight-silo/#setting-up-platforms","title":"Setting up platforms","text":"<p>Use <code>flight silo type avail</code> to list all available platform types and their current state. e.g. <pre><code>[flight@chead1 ~]$ flight silo type avail\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Name      \u2502 Description                   \u2502 Prepared \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 aws       \u2502 Amazon Simple Storage Service \u2502 true     \u2502\n\u2502 openstack \u2502 Openstack Swift Storage       \u2502 true     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre></p> <p>Types are not prepared by default. To prepare a type you must become a root user, and then do <code>flight silo type prepare &lt;type&gt;</code>. e.g. <pre><code>[root@chead1 ~]# flight silo type prepare aws\nPreparing...\nType aws prepared for use\n</code></pre></p> <p>Types are the backend providers of storage, and each type can hold multiple silos. Silos are used to hold software and files. To find out more about these see the relevant page in this section, or use the command <code>flight silo help</code> to view available commands and command syntax.</p>"},{"location":"docs/flight-environment/use-flight/flight-user-suite/flight-silo/files/","title":"Managing Files","text":"<p>Silo provides simple file management, this allows users to save and restore their files </p>"},{"location":"docs/flight-environment/use-flight/flight-user-suite/flight-silo/files/#file-list-silodirectory","title":"<code>file list &lt;silo&gt;:&lt;directory&gt;</code>","text":"<p>List the files in the given silo and directory. If no silo is specified then the default will be used. If no directory is given then the top-level of the silo file storage will be shown. </p> Example 1 <pre><code>[flight@chead1 ~]$ flight silo file list openflight:/openfoam\ncavity-example.sh  motorBike.tar.gz  OpenFOAM-v2212.tar.gz\n</code></pre>"},{"location":"docs/flight-environment/use-flight/flight-user-suite/flight-silo/files/#file-push-source-silodestination","title":"<code>file push &lt;source&gt; &lt;silo&gt;:&lt;destination&gt;</code>","text":"<p>Upload a file to a silo.If no silo is specified then the default will be used.</p> <ul> <li><code>--recursive</code> - Uploads a whole directory and all of its contents. You must specify a directory rather than a file.</li> <li><code>--make-parent</code> - Create parent directories if they don't exist.</li> </ul> Example 1 <pre><code>[flight@chead1 ~]$ flight silo file push example.sh\nLocal file '/home/flight/example.sh' copied to remote '/example.sh'\n</code></pre> Example 2 <pre><code>[flight@chead1 ~]$ flight silo file push dir/subdir/example.sh --make-parent\nLocal file '/home/flight/example.sh' copied to remote '/dir/subdir/example.sh'\n</code></pre>"},{"location":"docs/flight-environment/use-flight/flight-user-suite/flight-silo/files/#file-pull-silodestination-local-destination","title":"<code>file pull &lt;silo&gt;:&lt;destination&gt; &lt;local destination&gt;</code>","text":"<p>Pull a file or directory from a silo. If no silo is specified then the default will be used. If no local destination is specified then the current working directory will be used.</p> <ul> <li><code>--recursive</code> - Pulls a whole directory and all of its contents. </li> </ul> Example 1 <pre><code>[flight@chead1 ~]$ flight silo file pull openflight:/kubernetes/pod-launch-test.yaml\nPulling 'openflight:/kubernetes/pod-launch-test.yaml' into '/home/flight'...\nFile(s) downloaded to /home/flight\n</code></pre> Example 2 <pre><code>[flight@aztest1 ~]$ flight silo file pull --recursive openflight:/openfoam/ openfoam-download-example\nPulling 'openflight:/openfoam/' into 'openfoam-download-example'...\nFile(s) downloaded to openfoam-download-example\n</code></pre>"},{"location":"docs/flight-environment/use-flight/flight-user-suite/flight-silo/files/#file-delete-silodestination","title":"<code>file delete &lt;silo&gt;:&lt;destination&gt;</code>","text":"<p>Delete a file in a silo. If no silo is specified then the default will be used.</p> <ul> <li><code>--recursive</code> - Deletes a whole directory and all of its contents. You must specify a directory rather than a file.</li> </ul> Example 1 <pre><code>[flight@chead1 ~]$ flight silo file delete example.sh\nDeleting remote file 'example.sh'...\nDeleted remote file 'example.sh'\n</code></pre>"},{"location":"docs/flight-environment/use-flight/flight-user-suite/flight-silo/migrations/","title":"Managing Migrations","text":"<p>Migrations provide a method for replicating Silo software installations on new clusters. This supports quicker time-to-science by reducing the admin work required to create duplicate environments in ephemeral cloud situations. </p> <p>Every software pull made by the user will be recorded in an \"archive\". This will detail what software version was installed from what silos and to which directories it was installed. </p>"},{"location":"docs/flight-environment/use-flight/flight-user-suite/flight-silo/migrations/#migration-view","title":"<code>migration view</code>","text":"<p>Show the state of the current migration archives and any others available to the system.</p> <ul> <li><code>--archive &lt;archive&gt;</code> - Specify the archive to view instead of the currently enabled one</li> </ul> Example 1 <pre><code>[flight@gateway1 ~]$ flight silo migration view\n\nArchives:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Archive  \u2502 Status  \u2502 Host Silo \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 wooldbfe \u2502 enabled \u2502 Undefined \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nEnabled archive details:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Type     \u2502 Name     \u2502 Version \u2502 Path                  \u2502 Absolute \u2502 Silo Name  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 software \u2502 OpenFOAM \u2502 22.12   \u2502 ~/apps/OpenFOAM/22.12 \u2502 false    \u2502 openflight \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Note</p> <p>Software install destinations within home directories are portable between different users as they are not marked as \"absolute\" paths so, when migrating, will be installed into the home directory of whichever user is applying the migration </p>"},{"location":"docs/flight-environment/use-flight/flight-user-suite/flight-silo/migrations/#migration-apply","title":"<code>migration apply</code>","text":"<p>Apply a migration archive to the current system. This will attempt to install the software within the archive to the recorded destinations. </p> <ul> <li><code>--archive &lt;archive&gt;</code> - Specify the archive ID to view instead of the currently enabled one</li> <li><code>--ignore-missing-item</code> - Don't fail the apply if a silo, software package or version of a software package within the archive are not present</li> <li><code>--overwrite</code> - Overwrite software locally if it already exists at the installation location</li> </ul> Example 1 <pre><code>[flight@gateway1 ~]$ flight silo migration apply --archive qsaoxvxj\nValidating Archive 'qsaoxvxj'...\nMigration for archive 'qsaoxvxj' started...\n\nMigrating software my-software 1.0.0...\n'my-software' '1.0.0' successfully migrated\n\n\nMigrating software my-software 2.0.0...\n'my-software' '2.0.0' successfully migrated\n\nMigration All Done \u221a\n</code></pre>"},{"location":"docs/flight-environment/use-flight/flight-user-suite/flight-silo/migrations/#migration-continue","title":"<code>migration continue</code>","text":"<p>Resume monitoring of <code>software pull</code> actions.</p> Example 1 <pre><code>[flight@gateway1 ~]$ flight silo migration continue\nMigration Monitoring Enabled \u221a\n</code></pre>"},{"location":"docs/flight-environment/use-flight/flight-user-suite/flight-silo/migrations/#migration-pause","title":"<code>migration pause</code>","text":"<p>Pause monitoring of <code>software pull</code> actions. After running this command any software installed with Silo will not be added to the currently enabled archive.</p> Example 1 <pre><code>[flight@gateway1 ~]$ flight silo migration pause\nMigration Monitoring Disabled \u221a\n</code></pre>"},{"location":"docs/flight-environment/use-flight/flight-user-suite/flight-silo/migrations/#migration-pull-archive","title":"<code>migration pull &lt;archive&gt;</code>","text":"<p>Retrieve updated archive details from a silo. </p> Example 1 <pre><code>[flight@gateway1 ~]$ flight silo migration pull mysilo1\nObtaining silo migration archives...\nDone \u221a\n</code></pre>"},{"location":"docs/flight-environment/use-flight/flight-user-suite/flight-silo/migrations/#migration-push","title":"<code>migration push</code>","text":"<p>Update remote silos with latest local archive information. Archives are pushed to their \"Host Silo\", those without \"Host Silos\" are uploaded to the default silo.</p> <ul> <li><code>--repo &lt;silo&gt;</code> - Specify a silo other than the default to store the local-only archives in</li> </ul> Example 1 <pre><code>[flight@gateway1 ~]$ flight silo migration push --repo mysilo1\nUpdating migration archives for Silo mysilo1...\nAll Done \u221a\n</code></pre>"},{"location":"docs/flight-environment/use-flight/flight-user-suite/flight-silo/migrations/#migration-remove-software-name-version","title":"<code>migration remove software &lt;name&gt; &lt;version&gt;</code>","text":"<p>Remove a software item from the current archive.</p> <ul> <li><code>--archive &lt;archive&gt;</code> - Remove from the specified archive instead of the current one.</li> <li><code>--all</code> - Remove the software version from all archives</li> </ul> Example 1 <pre><code>[flight@gateway1 ~]$ flight silo migration remove software OpenFOAM 22.12\nSoftware 'OpenFOAM 22.12' migration record has been removed from archive 'wooldbfe'\n</code></pre>"},{"location":"docs/flight-environment/use-flight/flight-user-suite/flight-silo/migrations/#migration-switch","title":"<code>migration switch</code>","text":"<p>Change from the currently enabled archive to a new, empty archive. </p> <ul> <li><code>--archive &lt;archive&gt;</code> - Instead of creating a new one, enable the specified archive</li> </ul> Example 1 <pre><code>[flight@gateway1 ~]$ flight silo migration switch\nEnabled archive has been switched to 'debkxnfs'.\n</code></pre>"},{"location":"docs/flight-environment/use-flight/flight-user-suite/flight-silo/silos/","title":"Managing Silos","text":"<p>A silo is a storage space that is the same regardless of platform used. Even though a silo may appear as a directory in your cloud storage, it is highly recommended that a silo should only be managed by the command line tool.</p>"},{"location":"docs/flight-environment/use-flight/flight-user-suite/flight-silo/silos/#openflight-silo","title":"OpenFlight Silo","text":"<p>The Flight Silo tool comes with the <code>openflight</code> silo, this is a read-only silo that provides some example files &amp; software. So long as the AWS platform is prepared then the <code>openflight</code> repository can be interacted with to pull files and software. </p>"},{"location":"docs/flight-environment/use-flight/flight-user-suite/flight-silo/silos/#viewing-silos","title":"Viewing Silos","text":"<p>To see the available silos on the system use the command <code>flight silo repo list</code>. </p> <pre><code>[flight@chead1 ~]$ flight silo repo list\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Name       \u2502 Description                       \u2502 Platform \u2502 Public? \u2502 ID         \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 mysilo1    \u2502                                   \u2502 aws      \u2502 false   \u2502 ABCDE123   \u2502\n\u2502 openflight \u2502 Openflight software and resources \u2502 aws      \u2502 true    \u2502 OPENFLIGHT \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Note</p> <p>Silos that are defined locally and no longer exist upstream will be marked in yellow. This can occur when a silo has been deleted from a different system.</p>"},{"location":"docs/flight-environment/use-flight/flight-user-suite/flight-silo/silos/#creating-and-adding-silos","title":"Creating and Adding Silos","text":"<p>To create a silo use the command <code>flight silo repo create</code>. This will take you through a series of questions:</p> <ul> <li>Provider type - Which provider the silo should be created on.</li> <li>Silo name - What the name of the silo should be.</li> </ul> <p>After this point, any further questions depend on the chosen platform.</p> AWS OpenStack <ul> <li>Region - The region the silo should be created in.</li> <li>Access key ID - The ID for a valid aws access key.</li> <li>Secret access key - The secret key for a valid aws access key.</li> </ul> <p>More information about AWS access keys can be found in the AWS documentation.</p> <ul> <li>Endpoint URL - The URL of the OpenStack Swift Storage service to store this silo </li> <li>Access key ID - The ID for a valid access key generated by OpenStack EC2-like credentials</li> <li>Secret access key - The secret key for the above access key</li> </ul> <p>You can add an already existing silo with the command <code>flight silo repo add</code>. All questions asked will be the same as for creation, except that the answers will be used to find an existing silo.</p>"},{"location":"docs/flight-environment/use-flight/flight-user-suite/flight-silo/silos/#removing-and-deleting-silos","title":"Removing and Deleting Silos","text":"<p>If you no longer wish to have access to a silo on your machine, you can run the command <code>flight silo repo remove &lt;name&gt;</code>. This means that in order to access the silo again you would need to use the <code>add</code> command.</p> <p>A silo can be deleted with <code>flight silo repo delete &lt;name&gt;</code>. Unlike the <code>remove</code> command, the silo could not be re-added later as it is fully deleted upstream along with all of its contents.</p>"},{"location":"docs/flight-environment/use-flight/flight-user-suite/flight-silo/silos/#setting-a-default-silo","title":"Setting a default silo","text":"<p>If a default silo has been set, then commands that require a silo to be specified in the argument will use the default instead. Set a default with the command <code>flight silo set-default</code> e.g. <pre><code>[flight@chead1 ~]$ flight silo set-default openflight\nDefault silo set to: openflight\n</code></pre></p>"},{"location":"docs/flight-environment/use-flight/flight-user-suite/flight-silo/silos/#editing-silo-information","title":"Editing Silo Information","text":"<p>When a silo is created it is named and the description is left blank. Both the name and description can be modified using the <code>flight silo repo edit</code> command. For example: <pre><code>[flight@chead1 ~]$ flight silo repo edit mysilo1\nSilo name: myfilesilo1\nSilo description: This is my silo for sharing files between ephemeral cloud systems!\nUpdating silo details...\nSilo details updated\n</code></pre></p> <p>Note</p> <p>Any other systems that have a silo added locally will detect upstream changes and will require <code>flight silo repo refresh</code> to be run to ensure names and descriptions are synchronised with the upstream silo.</p>"},{"location":"docs/flight-environment/use-flight/flight-user-suite/flight-silo/software/","title":"Managing Software","text":"<p>Software in the context of Flight Silo is software binaries that can be compiled in advance, and then kept in a silo so that they can be accessed from any number of HPC environments without needing to be recompiled. This provides users a method to manage and share their own software.</p> <p>This page describes several commands associated with software which are run in the format <code>flight silo &lt;command&gt; --&lt;option&gt;</code>.</p>"},{"location":"docs/flight-environment/use-flight/flight-user-suite/flight-silo/software/#software-delete-name-version","title":"<code>software delete &lt;name&gt; &lt;version&gt;</code>","text":"<p>Delete a software binary from the default silo.</p> <ul> <li><code>--repo &lt;silo&gt;</code> - Instead of using the default silo, specify which one to delete from.</li> </ul> Example 1 <pre><code>[flight@chead1 ~]$ flight silo software delete exampl 0.0\nDeleting software 'exampl' version '0.0'...\nDeleted software 'exampl' version '0.0'.\n</code></pre>"},{"location":"docs/flight-environment/use-flight/flight-user-suite/flight-silo/software/#software-pull-name-version","title":"<code>software pull &lt;name&gt; &lt;version&gt;</code>","text":"<p>Download and extract a software binary from the default silo.</p> <ul> <li><code>--repo &lt;silo&gt;</code> - Instead of using the default silo, specify which one to pull from.</li> <li><code>--dir &lt;path&gt;</code> - Install software within <code>&lt;path&gt;</code> instead of the standard location. The order in which the destination path is decided is based on the following (in priority order):<ol> <li>The value of the environment variable <code>flight_SILO_software_dir</code></li> <li>The <code>--dir</code> argument</li> <li>The value of <code>software_dir</code> in <code>~/.config/flight/silo/config.yml</code></li> <li>The value of <code>software_dir</code> in <code>/opt/flight/opt/silo/config.yml</code> (this path may differ if a non-standard installation location has been chosen or if the source code has been cloned elsewhere) </li> </ol> </li> <li><code>--overwrite</code> - Overwrite local software if it exists.</li> </ul> Example 1 <pre><code>[flight@chead1 ~]$ flight silo software pull exampl 0.0\nDownloading software 'exampl' version '0.0'...\nExtracting software to '/home/flight/.local/share/flight/silo/software'...\nExtracted software 'exampl' version '0.0 to '/home/flight/.local/share/flight/silo/software'...\n</code></pre>"},{"location":"docs/flight-environment/use-flight/flight-user-suite/flight-silo/software/#software-push-file-name-version","title":"<code>software push &lt;file&gt; &lt;name&gt; &lt;version&gt;</code>","text":"<p>Upload a software binary to the default silo.</p> <ul> <li><code>--repo &lt;silo&gt;</code> - Instead of using the default silo, specify which one to push to.</li> <li><code>--force</code> - Overwrite existing software in the silo if it exists.</li> </ul> Example 1 <pre><code>[flight@chead1 ~]$ flight silo software push exampl.tar.gz exampl 0.0\nUploading software 'exampl' version '0.0'...\nUploaded software 'exampl' version '0.0'.\n</code></pre>"},{"location":"docs/flight-environment/use-flight/flight-user-suite/flight-silo/software/#software-search-name","title":"<code>software search &lt;name&gt;</code>","text":"<p>List the software binaries in the default silo. If no name is given, then all software will be displayed.</p> <ul> <li><code>--repo &lt;silo&gt;</code> - Instead of using the default silo, specify which one to search on.</li> </ul> Example 1 - With Search Term (Shows Versions) <pre><code>[flight@chead1 ~]$ flight silo software search exampl\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Name   \u2502 Version \u2502 Size   \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 exampl \u2502 0.0     \u2502 500 MB \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> Example 2 - With No Search Term <pre><code>[flight@chead1 ~]$ flight silo software search\nShowing latest 5 versions...\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Name   \u2502 Version \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 exampl \u2502 0.0     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"docs/flight-environment/use-flight/flight-web-suite/","title":"Flight Web Suite","text":"<p>The Flight Web Suite is a collection of web applications that provide users with easy and intuitive ways to interact with their research environment. The purpose of these tools is to get researchers started with HPC as quickly as possible without needing to worry about configuring system access and desktop session creation, leaving them to do what they do best - research!</p>"},{"location":"docs/flight-environment/use-flight/flight-web-suite/console/","title":"Flight Console","text":"<p>The Flight Console provides an embedded terminal in the web browser to directly access the HPC environment. </p> <p>Tip</p> <p>The terminal does not persist when the console page is navigated away from, bear this in mind if performing any long-running commands as the session will need to remain active and connected. Utilising a tool like <code>screen</code> will detach processes from the terminal session and run them in a virtual terminal. </p>"},{"location":"docs/flight-environment/use-flight/flight-web-suite/console/#connecting-to-console","title":"Connecting to Console","text":"<p>After selecting the Console application from the Landing Page there will be a button prompting to \"Connect to the Terminal\".</p> <p>If it's the first time connecting to Flight Console then the will be prompted to install an SSH key to secure the connection between the browser and the console. </p> <p></p> <p>Once the key is installed the user will be presented with a terminal window mirroring the environment.</p> <p>There are a few options for the terminal view, such as, making it Full Screen, changing to Zen Mode (terminal focused with less widgets and buttons polluting the screen) and disconnecting the terminal session.</p>"},{"location":"docs/flight-environment/use-flight/flight-web-suite/desktop/","title":"Flight Desktop Web","text":"<p>The Flight Desktop web application provides a visual front-end to the Flight Desktop CLI tool, allowing users an intuitive solution to managing remote desktop sessions. </p>"},{"location":"docs/flight-environment/use-flight/flight-web-suite/desktop/#creating-desktop-session","title":"Creating Desktop Session","text":"<p>To launch a desktop session for the cluster, select \"Launch\" from the management page then select the desired desktop environment to launch.</p> <p></p> <p>Note</p> <p>The only session types that will be displayed here are those that have been prepared </p> <p>Upon selecting the desktop type various configuration options will appear, this allows the user to set the name and screen size of the session. </p> <p>After the user has set their options and selected launch, they will be connected to the session.</p> <p></p> <p>There are a few options for the desktop view, such as, making it Full Screen, changing to Zen Mode (terminal focused with less widgets and buttons polluting the screen), disconnecting from the desktop session and terminating the session completely.</p>"},{"location":"docs/flight-environment/use-flight/flight-web-suite/desktop/#a-note-on-pasting","title":"A Note on Pasting","text":"<p>The desktop web suite does allow for remote &amp; local clipboard integration. If text is copied on the remote session then it will be immediately available on the local clipboard.</p> <p>For copying local data to the remote session a little bit of a workaround is needed:</p> <ul> <li>Copy text</li> <li>Click \"Prepare Paste\" in the desktop webapp (located in the top right of the screen)     </li> <li>Paste text into the pop up box, and click \"ok\"</li> <li>Paste text in the remote session</li> </ul>"},{"location":"docs/flight-environment/use-flight/flight-web-suite/desktop/#managing-desktop-sessions","title":"Managing Desktop Sessions","text":"<p>Selecting the \"My Sessions\" link in the header will present all existing desktop sessions for the user, from here sessions can be connected to or terminated through the corresponding buttons. Additionally, this session manager will generate previews of the various desktop sessions to make it easier to identify the different active sessions.</p>"},{"location":"docs/flight-environment/use-flight/flight-web-suite/file-manager/","title":"Flight File Manager","text":"<p>The Flight File Manager provides a way to manage data on the HPC environment by providing a GUI that allows upload, download and even modification of files. </p>"},{"location":"docs/flight-environment/use-flight/flight-web-suite/file-manager/#using-the-file-manager","title":"Using the File Manager","text":"<p>To start a file manager session for the cluster, select \"Manage Files\" from the management page. The system will open a UI for managing files.</p> <p></p> <p>There are a few options for the file manager, such as, making it Full Screen, changing to Zen Mode (terminal focused with less widgets and buttons polluting the screen), and switching to console mode (opening the current directory in Flight Console).</p> <p>There are several buttons on the top part of the screen, they have been labelled below. All the greyed out buttons (except Bookmarks) can only be used when a file is selected. Additionally, the user can bring up the button menu by right clicking. Right clicking on a particular file or directory will show actions available for that file or directory.</p> <p>Bookmarks allow the user to quickly change to a bookmarked directory. Currently, bookmarks must be manually set by administrators with root privileges.</p> <p></p>"},{"location":"docs/flight-environment/use-flight/flight-web-suite/landing-page/","title":"Web Suite Landing Page","text":"<p>A landing page is presented when navigating to the web suite domain. This page gives a brief description of the HPC environment and presents the cluster name. </p> <p>Further to this, the landing page presents the available web applications. </p> <p> </p> <p>Example landing page on Flight Solo</p>"},{"location":"docs/flight-environment/use-flight/flight-web-suite/landing-page/#logging-in","title":"Logging In","text":"<p>Selecting the <code>LOG IN</code> button in the top right of the landing page will present a form to enter your user details. These will match the username and password usually used to access the HPC environment. Once logged in, the session will be authorised to access the rest of the flight web applications.</p> <p> </p> <p>Example login form on Flight Solo</p>"},{"location":"docs/flight-environment/use-flight/flight-web-suite/landing-page/#customising-the-landing-page","title":"Customising the Landing Page","text":"<p>The styling, content and assets used by the landing page can be customised and overridden. For more information on modifying the landing page see the landing page repository. </p> <p>If any changes are made to the landing page then the web assets will need to be recompiled: <pre><code>flight landing-page compile\n</code></pre></p>"},{"location":"docs/flight-environment/use-flight/flight-web-suite/landing-page/#adding-applications","title":"Adding Applications","text":"<p>The Flight Web Suite runs it's own Nginx server. This allows for services running on localhost to be accessed via the Web Suite acting as a reverse proxy. </p> <p>A brief outline of what is required to add your local service to Flight Web Suite is below.</p> <ul> <li>Install and run your application strictly on localhost on an available port</li> <li>Add link to application to the web-suite applications page by placing a file under <code>/opt/flight/opt/www/landing-page/default/content/apps/</code> named along the lines of <code>myapp.md</code> with content like (note: icons are FontAwesome v4):     <pre><code>---\ntitle: My App Name\nshort_title: App\nsubtitle: Do Things The App Does\npath: /route/to/server/location\nfa_icon: FontAwesomeIconName\n---\nMy App Name allows you to do app things in the app place and this is the description\nof what it does.\n</code></pre></li> <li>The application server can be defined as part of the <code>flight-www</code> webserver by installing an Nginx config file for the service to <code>/opt/flight/etc/www/server-https.d/</code> which could look something like the following     <pre><code>location ^~ /route/to/server/location {\n  proxy_pass http://127.0.0.1:PORT;\n  proxy_set_header X-Real-IP $remote_addr;\n  proxy_set_header Host $host;\n  proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n  # websocket headers\n  proxy_http_version 1.1;\n  proxy_set_header Upgrade $http_upgrade;\n  proxy_set_header Connection \"Upgrade\";\n  proxy_set_header X-Scheme $scheme;\n  proxy_buffering off;\n}\n</code></pre></li> <li>Recompile landing page     <pre><code>flight landing-page compile\n</code></pre></li> <li>Restart web server     <pre><code>flight service restart www\n</code></pre></li> </ul>"},{"location":"docs/flight-environment/use-flight/flight-web-suite/setup/","title":"Setting Up Flight Web Suite","text":""},{"location":"docs/flight-environment/use-flight/flight-web-suite/setup/#system-prerequisites","title":"System Prerequisites","text":"<p>In order to authenticate the user in the web interface, the following must be true:</p> <ul> <li>User has a password (can be set with the <code>passwd</code> command or through other user management software that is setup on the system)</li> <li>Ports 80 &amp; 443 on the gateway must be accessible (allowed through both the system firewall and cloud security group)</li> <li>SSH password authentication must be enabled (can be set in <code>/etc/ssh/sshd_config</code> in CentOS or through other access management software that is setup on the system)</li> </ul>"},{"location":"docs/flight-environment/use-flight/flight-web-suite/setup/#setting-domain-name","title":"Setting Domain Name","text":"<p>The domain name is what the Web-Suite will be accessed through, either a hostname or ip address. It is also used for certificate generation, and a publicly accessible value should be used if intending to use Lets Encrypt certificates.</p> <p>Set the domain name: <pre><code>flight web-suite set-domain chead1.mycluster1.example.com\n</code></pre></p> <p>Restart the web-suite to apply changes: <pre><code>flight web-suite restart\n</code></pre></p>"},{"location":"docs/flight-environment/use-flight/flight-web-suite/setup/#certificate-preparation","title":"Certificate Preparation","text":"<p>To secure the server connections, it is recommended to generate a certificate to be used by the web suite. The Flight Web Suite comes with tools that can generate either a \"self-signed\" or LetsEncrypt certificate. Alternatively, a certificate that has been created outside of the web suite can be used to secure the server.</p> Self-SignedLets EncryptExternal Certificate <p>A self-signed certificate, whilst not usually trusted by browsers, does still provide extra security to the web server over HTTP communication.</p> <p>A self-signed certificate is automatically created when setting the domain name. To generate and install the self-signed certificates, simply:</p> <pre><code>flight www cert-gen --cert-type self-signed --domain $(flight web-suite get-domain)\n</code></pre> <p>Note</p> <p>If <code>--domain</code> is omitted, a sensible default is selected. The default is taken from either the last <code>--domain</code> value given to <code>flight www cert-gen</code> or the last value given to <code>flight web-suite set-domain</code>. If neither of those have been given, the command will complain, and the domain will need to be specified.</p> <p>Note</p> <p>If <code>--cert-type</code> is omitted a sensible default is selected. The default is taken from the last <code>--cert-type</code> value given to <code>flight www cert-gen</code> or <code>self-signed</code> if none has been given before.</p> <p>After this has run, changes are applied on a service restart:</p> <pre><code>flight web-suite restart\n</code></pre> <p>To generate and install a Lets Encrypt certificate, run the following (replacing the domain and email with appropriate values): <pre><code>flight www cert-gen --cert-type lets-encrypt --domain &lt;chead1.mycluster1.example.com&gt; --email &lt;user@example.com&gt;\n</code></pre></p> <p>Note</p> <p>If <code>--domain</code> is omitted, a sensible default is selected. The default is taken from either the last <code>--domain</code> value given to <code>flight www cert-gen</code> or the last value given to <code>flight web-suite set-domain</code>. If neither of those have been given, the command will complain, and the domain will need to be specified.</p> <p>Warning</p> <p>Ensure that the domain/IP is publicly accessible in order for certificate generation to work</p> <p>The Let's Encrypt certificate is only valid for a limited time. Depending on how long a cluster is intended to live for, it may be useful to install a cron job to automate renewing the certificate.</p> <pre><code>flight www cron-renewal\n</code></pre> <p>The cronjob can be removed by running:</p> <pre><code>flight www cron-renewal --disable\n</code></pre> <p>After this has run, changes are applied on a service restart: <pre><code>flight web-suite restart\n</code></pre></p> <p>Externally generated certificates can be used by instructing <code>www</code> to install them, to do this you will need:</p> <ul> <li><code>fullchain.pem</code>: The full certificate </li> <li><code>privkey.pem</code>: The private key for the certificate </li> </ul> <p>Once you've obtained these files and placed them on the host, add them with:</p> <pre><code>flight www cert-install /path/to/privkey.pem /path/to/fullchain.pem\n</code></pre> <p>If the Web Suite was already running then restart it with:</p> <pre><code>flight web-suite restart\n</code></pre>"},{"location":"docs/flight-environment/use-flight/flight-web-suite/setup/#connecting-to-web-suite","title":"Connecting to Web Suite","text":"<p>Navigate to the external IP or hostname set for the gateway (that was provided to the <code>set-domain</code> command, for example, <code>https://51.104.217.61</code>)</p> <p>Log in with the same user details used for accessing the cluster from a CLI.</p> <p>If you find yourself being logged out when changing pages, you may need to add an entry in your hosts file..</p>"},{"location":"docs/flight-solo/","title":"Flight Solo","text":""},{"location":"docs/flight-solo/#what-is-flight-solo","title":"What is Flight Solo?","text":"<p>Flight Solo is an HPC-ready, platform-agnostic image approach to deploying HPC resources.</p> <p>Flight Solo provides a personal High Performance Computing (HPC) environment for research and scientific computing. Compatible with on-demand, reserved and spot instances, Flight Solo rapidly delivers a whole HPC software environment, ready to go and complete with job scheduler, applications and remote desktop service. Multiple software application frameworks are available for use including SPack, Easybuild and Conda containing many hundreds of different programs, libraries, compilers and MPIs. Data management tools for POSIX and S3 object storage are also included to help users transfer files and manage storage resources.</p> <p>Built on EL9, loaded with the Flight Environment and presented with useful cloud-init integrations - the Flight Solo image provides an image suitable for: </p> <ul> <li>Researchers &amp; End-Users to get going with HPC, fast</li> <li>Rapid deployment of replicable HPC environments</li> <li>Proof-of-concept cloud HPC environments</li> <li>Evaluating the Flight Environment</li> </ul>"},{"location":"docs/flight-solo/quickstart-guide/","title":"Getting Started with Flight Solo","text":"<p>There are a lot of moving parts to Flight Solo so it can seem a little daunting to begin with. This document will assist you in finding the resources you need to get started with Flight Solo. </p> What is Flight Solo is and how does it work? <p>Check out the videos on the OpenFlight YouTube channel or read through Understanding Flight Solo</p> I want to see Flight Solo for myself! <p>The quickest way to get started would be by launching the AWS Marketplace image and logging in and taking a look around </p> I want to build my own cluster! <p>We have guides for that! </p>"},{"location":"docs/flight-solo/cluster-build-methods/","title":"Cluster Build Methods","text":"<p>Building a cluster can seem like a daunting task. Flight Solo aims to make that process a lot simpler.</p> <p>Depending on your platform, cluster type and resource requirements there can be a lot of considerations and nuance to the configuration process. With the Flight Solo image being identical regardless of platform it makes post-launch configuration consistent and straightforward. </p> <p>Head over to the Choose Your Own Adventure page to find the right process for the cluster you'd like to run. </p>"},{"location":"docs/flight-solo/cluster-build-methods/jupyter-lab-standalone-alces-cloud/","title":"Jupyter Lab Standalone on Alces Cloud","text":""},{"location":"docs/flight-solo/cluster-build-methods/jupyter-lab-standalone-alces-cloud/#launch-login-node","title":"Launch Login Node","text":"CLIGUI <p>To set up a cluster, you will need to import a Flight Solo image.</p> <p>Before setting up a cluster on Alces Cloud, there are several required prerequisites:</p> <ul> <li>Your own keypair</li> <li>A network with a subnet and a router bridging the subnet to the external network</li> <li>A security group that allows traffic is given below (if creating the security group through the web interface then the \"Any\" protocol will need to be an \"Other Protocol\" rule with \"IP Protocol\" of <code>-1</code>)</li> </ul> Protocol Direction CIDR Port Range Any egress 0.0.0.0/0 any Any ingress Virtual Network CIDR any ICMP ingress 0.0.0.0/0 any SSH ingress 0.0.0.0/0 22 TCP ingress 0.0.0.0/0 80 TCP ingress 0.0.0.0/0 443 TCP ingress 0.0.0.0/0 5900-5903 <p>Note</p> <p>The \"Virtual Network CIDR\" is the subnet and netmask for the network that the nodes are using. For example, a node on the 11.11.11.0 network with a netmask of 255.255.255.0 would have a network CIDR of 11.11.11.0/24.</p> <ol> <li> <p>Launch a login node with a command similar to the following: </p> <pre><code>$ openstack server create --flavor p1.small \\\n                          --image \"Flight Solo VERSION\" \\\n                          --boot-from-volume 16 \\\n                          --network \"mycluster1-network\" \\\n                          --key-name \"MyKey\" \\\n                          --security-group \"mycluster1-sg\" \\\n                          --user-data myuserdata.yml \\\n                          login1\n</code></pre> <ul> <li> <p>Where:</p> <ul> <li><code>flavor</code> - Is the desired size of the instance</li> <li><code>image</code> - Is the Flight Solo image imported to Alces Cloud</li> <li><code>boot-from-volume</code> - Is the size of the system disk in GB</li> <li><code>network</code> - Is the name or ID of the network created for the cluster</li> <li><code>key-name</code> - Is the name of the SSH key to use</li> <li><code>security-group</code> - Is the name or ID of the security group created previously</li> <li><code>user-data</code> - Is the file containing cloud-init user-data (this is optional in standalone scenarios)</li> <li><code>login1</code> - Is the name of the system</li> </ul> </li> </ul> </li> <li> <p>Associate a floating IP, either by using an existing one or creating a new one</p> <ol> <li> <p>To use an existing floating IP</p> <ol> <li> <p>Identify the IP address of an available floating IP (<code>Port</code> will be <code>None</code>)</p> <pre><code>$ openstack floating ip list\n+---------------------+---------------------+------------------+---------------------+---------------------+-----------------------+\n| ID                  | Floating IP Address | Fixed IP Address | Port                | Floating Network    | Project               |\n+---------------------+---------------------+------------------+---------------------+---------------------+-----------------------+\n| 726318f4-4dbb-4d51- | 10.199.31.6         | None             | None                | c681d94b-e2ec-4b73- | dcd92da7538a4f64a42b0 |\n| b119-d9e53c47a9f5   |                     |                  |                     | 89bf-9943bcce3255   | d4d9ce8845f           |\n</code></pre> </li> <li> <p>Associate the floating IP with the instance</p> <pre><code>$ openstack server add floating ip login1 10.199.31.6\n</code></pre> </li> </ol> </li> <li> <p>To create a new floating IP </p> <ol> <li> <p>Create new floating IP and note the <code>floating_ip_address</code> </p> <pre><code>$ openstack floating ip create external1\n</code></pre> </li> <li> <p>Associate the floating IP with the instance (using the <code>floating_ip_address</code> from the previous output)</p> <pre><code>$ openstack server add floating ip login1 10.199.31.212\n</code></pre> </li> </ol> </li> </ol> </li> </ol> <p>Further detail on collecting the information from the above can be found in the Alces Cloud documentation.</p> <p>To set up a cluster, you will need to import a Flight Solo image.</p> <p>Before setting up a cluster on Alces Cloud, there are several required prerequisites:</p> <ul> <li>Your own keypair</li> <li>A network with a subnet and a router bridging the subnet to the external network</li> <li>A security group that allows traffic is given below (if creating the security group through the web interface then the \"Any\" protocol will need to be an \"Other Protocol\" rule with \"IP Protocol\" of <code>-1</code>)</li> </ul> Protocol Direction CIDR Port Range Any egress 0.0.0.0/0 any Any ingress Virtual Network CIDR any ICMP ingress 0.0.0.0/0 any SSH ingress 0.0.0.0/0 22 TCP ingress 0.0.0.0/0 80 TCP ingress 0.0.0.0/0 443 TCP ingress 0.0.0.0/0 5900-5903 <p>Note</p> <p>The \"Virtual Network CIDR\" is the subnet and netmask for the network that the nodes are using. For example, a node on the 11.11.11.0 network with a netmask of 255.255.255.0 would have a network CIDR of 11.11.11.0/24.</p> <p>To set up a cluster:</p> <ol> <li> <p>Go to the Alces Cloud instances page.</p> <p></p> </li> <li> <p>Click \"Launch Instance\", and the instance creation window will pop up.</p> </li> <li> <p>Fill in the instance name, and leave the number of instances as 1, then click next.</p> <p></p> </li> <li> <p>Choose the desired image to use by clicking the up arrow at the end of its row. It will be displayed in the \"Allocated\" section when selected.</p> <p></p> </li> <li> <p>Choose the desired instance size by clicking the up arrow at the end of its row. It will be displayed in the \"Allocated\" section when selected.</p> <p></p> </li> <li> <p>Choose a network in the same way as an image or instance size. Note that all nodes in a cluster must be on the same network.</p> <p></p> </li> <li> <p>Choose a security group in the same way as an image or instance size. Note that all nodes in a cluster must be in the same security group.</p> <p></p> </li> <li> <p>Choose the keypair in the same way as an image or instance size.</p> <p></p> </li> <li> <p>In the \"Configuration\" section, there is a \"Customisation Script\" section with a text box. This will be used to set your user data</p> <p></p> </li> <li> <p>When all options have been selected, press the \"Launch Instance\" button to launch. If the button is greyed out, then a mandatory setting has not been configured.</p> <p></p> </li> <li> <p>Go to the \"Instances\" page in the \"Compute\" section. The created node should be there and be finishing or have finished creation.</p> <p></p> </li> <li> <p>Click on the down arrow at the end of the instance row. This will bring up a drop-down menu.</p> </li> <li> <p>Select \"Associate Floating IP\", this will make the ip management window pop up.</p> <p></p> </li> <li> <p>Associate a floating IP, either by using an existing one or allocating a new one.</p> <ol> <li> <p>To use an existing floating IP:</p> <ol> <li> <p>Open the IP Address drop-down menu.</p> <p></p> </li> <li> <p>Select one of the IP Addresses.</p> </li> <li> <p>Click \"Associate\" to finish associating an IP.</p> </li> </ol> </li> <li> <p>To allocate a new floating IP:</p> <ol> <li> <p>Click the \"+\" next to the drop-down arrow to open the allocation menu.</p> <p></p> </li> <li> <p>Click \"Allocate IP\".</p> </li> </ol> </li> </ol> </li> <li> <p>Click \"Associate\" to finish associating an IP.</p> </li> </ol> <p>Further detail on collecting the information from the above can be found in the Alces Cloud documentation.</p>"},{"location":"docs/flight-solo/cluster-build-methods/jupyter-lab-standalone-alces-cloud/#general-configuration","title":"General Configuration","text":""},{"location":"docs/flight-solo/cluster-build-methods/jupyter-lab-standalone-alces-cloud/#create-node-inventory","title":"Create Node Inventory","text":"<ol> <li> <p>Parse your node(s) with the command <code>flight hunter parse</code>.</p> <ol> <li> <p>This will display a list of hunted nodes, for example     <pre><code>[flight@login-node.novalocal ~]$ flight hunter parse\nSelect nodes: (Scroll for more nodes)\n\u2023 \u2b21 login-node.novalocal - 10.10.0.1\n  \u2b21 compute-node-1.novalocal - 10.10.101.1\n</code></pre></p> </li> <li> <p>Select the desired node to be parsed with Space, and you will be taken to the label editor     <pre><code>Choose label: login-node.novalocal\n</code></pre></p> </li> <li> <p>Here, you can edit the label like plain text     <pre><code>Choose label: login1\n</code></pre></p> <p>Tip</p> <p>You can clear the current node name by pressing Down in the label editor.</p> </li> <li> <p>When done editing, press Enter to save. The modified node label will appear next to the ip address and original node label.     <pre><code>Select nodes: login-node.novalocal - 10.10.0.1 (login1) (Scroll for more nodes)\n\u2023 \u2b22 login-node.novalocal - 10.10.0.1 (login1)\n  \u2b21 compute-node-1.novalocal - 10.10.101.1\n</code></pre></p> </li> <li> <p>From this point, you can either hit Enter to finish parsing and process the selected nodes, or continue changing nodes. Either way, you can return to this list by running <code>flight hunter parse</code>.</p> </li> <li> <p>Save the node inventory before moving on to the next step.</p> <p>Tip</p> <p>See <code>flight hunter parse -h</code> for more ways to parse nodes.</p> </li> </ol> </li> </ol>"},{"location":"docs/flight-solo/cluster-build-methods/jupyter-lab-standalone-alces-cloud/#add-genders","title":"Add genders","text":"<ol> <li>Optionally, you may add genders to the newly parsed node. For example, in the case that the node should have the gender <code>cluster</code> and <code>all</code> then run the command:     <pre><code>flight hunter modify-groups --add cluster,all login1\n</code></pre></li> </ol>"},{"location":"docs/flight-solo/cluster-build-methods/jupyter-lab-standalone-alces-cloud/#jupyter-lab-standalone-configuration","title":"Jupyter Lab Standalone Configuration","text":"<ol> <li> <p>Configure profile</p> <pre><code>flight profile configure\n</code></pre> <ol> <li>This brings up a UI, where several options need to be set. Use up and down arrow keys to scroll through options and enter to move to the next option. Options in brackets coloured yellow are the default options that will be applied if nothing is entered.<ul> <li>Cluster type: The type of cluster setup needed, in this case select <code>Jupyter Standalone</code>.</li> <li>Cluster name: The name of the cluster.</li> <li>Default user: The user that you log in with.</li> <li>Set user password: Set a password to be used for the chosen default user.</li> <li>IP or FQDN for Web Access: As described here, this could be the public IP or public hostname.</li> </ul> </li> </ol> </li> <li> <p>Apply an identity by running the command <code>flight profile apply</code>, E.g.     <pre><code>flight profile apply standalone1 all-in-one\n</code></pre>     !!! tip     You can check all available identities for the current profile with <code>flight profile identities</code></p> </li> <li> <p>Wait for the identity to finish applying. You can check the status of all nodes with <code>flight profile list</code>.</p> <p>Tip</p> <p>You can watch the progress of the application with <code>flight profile view login1 --watch</code></p> </li> </ol> <p>Success</p> <p>Congratulations, you've now created a Jupyter Lab Standalone environment! Learn more about Jupyter Lab in their documentation.</p>"},{"location":"docs/flight-solo/cluster-build-methods/jupyter-lab-standalone-alces-cloud/#verifying-functionality","title":"Verifying Functionality","text":"<ol> <li> <p>Put the IP/FQDN used in configuration into your browser to access the Flight Web Suite. It should look something like this:     </p> </li> <li> <p>Under \"Quick Access\" click on \"Jupyter\" and enter the password set during configuration when requested; it will only need to be entered the first time you connect.     </p> </li> <li> <p>On the Jupyter home page, under the \"Notebook\" section, click on \"Python3\" to open a new notebook.     </p> </li> <li> <p>Enter this code, which will print out a message, wait for a bit, then print again.     <pre><code>import time\nprint(\"Starting running on Jupyter\")\ntime.sleep(3)\nprint(\"Finished running - goodbye from Jupyter\")\n</code></pre> </p> </li> <li> <p>Click the play button to run the cell, and wait for the result.</p> </li> </ol>"},{"location":"docs/flight-solo/cluster-build-methods/jupyter-lab-standalone-aws/","title":"Jupyter Lab Standalone on AWS","text":""},{"location":"docs/flight-solo/cluster-build-methods/jupyter-lab-standalone-aws/#launch-login-node","title":"Launch Login Node","text":"AWS MarketplaceAWS Imported <p>To set up a cluster, you will need to import a Flight Solo image.</p> <ol> <li> <p>Go the EC2 instance console</p> <p></p> </li> <li> <p>Click \"Launch\" to go to the EC2 instance setup page.</p> <p></p> </li> <li> <p>Set the number of instances to 1, and name of instance to something descriptive.</p> <p></p> </li> <li> <p>Confirm that the region(top right, next to username) is correct.</p> <p></p> </li> <li> <p>In the \"Application and OS Images\" section choose the \"My AMIs\" tab and select your imported solo AMI.</p> <p></p> </li> <li> <p>In the \"Instance type\" section, choose the required instance size.</p> <p></p> </li> <li> <p>In the \"Keypair\" section, select a keypair to use. It is good practice to use the same keypair for the login and compute nodes.</p> <p></p> </li> <li> <p>In the \"Network settings\" sections, click the \"Edit\" button to set the network and subnet. Remember what these are, as they should be the same for any associated compute nodes.</p> <p></p> <p></p> </li> <li> <p>Another thing needed is a security group to associate with all nodes on the cluster. It is recommended to use a security group with rules limiting traffic through:</p> <ul> <li>HTTP</li> <li>HTTPS</li> <li>SSH</li> <li>Port 8888</li> <li>Ports 5900 - 5903</li> <li>All traffic from within the security group should be allowed. (This rule can only be added after creation)</li> </ul> <p>Note</p> <p>If you already have a security group which does this, use it here and make sure to use it again for the compute nodes. Otherwise, a security group can be made from the launch page, or through the security groups page</p> <p>Describing exactly how to create a security group is out of scope for this documentation, but covered by the AWS documentation.</p> <p>However, here is an example security group that might be used for a Flight Solo cluster:</p> <p></p> </li> <li> <p>After a security group has been made, click \"Choose Existing\" select it from the drop down menu.</p> </li> <li> <p>In the \"Configure Storage\" section, allocate as much memory as needed. 8GB is the minimum required for Flight Solo, so it is likely the compute nodes will not need much more than that, as the login node hosts most data.</p> <p></p> </li> <li> <p>Finally, click \"Launch Instance\".</p> </li> </ol> <ol> <li> <p>Find the Flight Solo image here or by searching the marketplace for \"Flight Solo\".</p> </li> <li> <p>Click \"Continue to Subscribe\"</p> <p></p> </li> <li> <p>Read the terms and conditions, then click \"Continue to Configuration\"</p> <p></p> </li> <li> <p>Configure region, software version (if unsure use the latest), and fulfillment option (if unsure use the default). Then click \"Continue to Launch\". Make sure the region is the same for all nodes to be used in a cluster.</p> <p></p> </li> <li> <p>Click on \"Usage Instructions\" to see some instructions on how to get started, and a link to this documentation.</p> <p></p> </li> <li> <p>Select the \"Launch from Website\" action.</p> <p></p> </li> <li> <p>Choose an instance type to use.</p> <p></p> </li> <li> <p>Choose VPC settings. Remember what VPC was used to create this instance, as it should also be used for any associated compute nodes.</p> <p></p> </li> <li> <p>Choose a subnet. Remember what subnet was used to create this instance, as it should also be used for any associated compute nodes.</p> <p></p> </li> <li> <p>A security group is needed to associate with all nodes on the cluster. It is recommended to use a security group with rules limiting traffic through:</p> <ul> <li>HTTP</li> <li>HTTPS</li> <li>SSH</li> <li>Port 8888</li> <li>Ports 5900 - 5903</li> <li>All traffic from within the security group should be allowed. (This rule can only be added after creation)</li> </ul> <p>Note</p> <p>If you already have a security group which does this, use it here and make sure to use it again for the compute nodes. Otherwise, a security group can be made from the launch page, or through the security groups page</p> <p>Describing exactly how to create a security group is out of scope for this documentation, but covered by the AWS documentation.</p> <p>However, here is an example security group that might be used for a Flight Solo cluster:</p> <p></p> <p>Tip</p> <p>The seller's settings (shown below) can be used as a reference for creating a security group.</p> <p></p> </li> <li> <p>After a security group has been made, click \"Select existing security group\" select it from the drop down menu.</p> <p></p> </li> <li> <p>Choose what key pair to use. It is good practice for this to be the same on all nodes in a cluster.</p> <p></p> </li> <li> <p>Click Launch</p> <p></p> </li> </ol>"},{"location":"docs/flight-solo/cluster-build-methods/jupyter-lab-standalone-aws/#general-configuration","title":"General Configuration","text":""},{"location":"docs/flight-solo/cluster-build-methods/jupyter-lab-standalone-aws/#create-node-inventory","title":"Create Node Inventory","text":"<ol> <li> <p>Parse your node(s) with the command <code>flight hunter parse</code>.</p> <ol> <li> <p>This will display a list of hunted nodes, for example     <pre><code>[flight@login-node.novalocal ~]$ flight hunter parse\nSelect nodes: (Scroll for more nodes)\n\u2023 \u2b21 login-node.novalocal - 10.10.0.1\n  \u2b21 compute-node-1.novalocal - 10.10.101.1\n</code></pre></p> </li> <li> <p>Select the desired node to be parsed with Space, and you will be taken to the label editor     <pre><code>Choose label: login-node.novalocal\n</code></pre></p> </li> <li> <p>Here, you can edit the label like plain text     <pre><code>Choose label: login1\n</code></pre></p> <p>Tip</p> <p>You can clear the current node name by pressing Down in the label editor.</p> </li> <li> <p>When done editing, press Enter to save. The modified node label will appear next to the ip address and original node label.     <pre><code>Select nodes: login-node.novalocal - 10.10.0.1 (login1) (Scroll for more nodes)\n\u2023 \u2b22 login-node.novalocal - 10.10.0.1 (login1)\n  \u2b21 compute-node-1.novalocal - 10.10.101.1\n</code></pre></p> </li> <li> <p>From this point, you can either hit Enter to finish parsing and process the selected nodes, or continue changing nodes. Either way, you can return to this list by running <code>flight hunter parse</code>.</p> </li> <li> <p>Save the node inventory before moving on to the next step.</p> <p>Tip</p> <p>See <code>flight hunter parse -h</code> for more ways to parse nodes.</p> </li> </ol> </li> </ol>"},{"location":"docs/flight-solo/cluster-build-methods/jupyter-lab-standalone-aws/#add-genders","title":"Add genders","text":"<ol> <li>Optionally, you may add genders to the newly parsed node. For example, in the case that the node should have the gender <code>cluster</code> and <code>all</code> then run the command:     <pre><code>flight hunter modify-groups --add cluster,all login1\n</code></pre></li> </ol>"},{"location":"docs/flight-solo/cluster-build-methods/jupyter-lab-standalone-aws/#jupyter-lab-standalone-configuration","title":"Jupyter Lab Standalone Configuration","text":"<ol> <li> <p>Configure profile</p> <pre><code>flight profile configure\n</code></pre> <ol> <li>This brings up a UI, where several options need to be set. Use up and down arrow keys to scroll through options and enter to move to the next option. Options in brackets coloured yellow are the default options that will be applied if nothing is entered.<ul> <li>Cluster type: The type of cluster setup needed, in this case select <code>Jupyter Standalone</code>.</li> <li>Cluster name: The name of the cluster.</li> <li>Default user: The user that you log in with.</li> <li>Set user password: Set a password to be used for the chosen default user.</li> <li>IP or FQDN for Web Access: As described here, this could be the public IP or public hostname.</li> </ul> </li> </ol> </li> <li> <p>Apply an identity by running the command <code>flight profile apply</code>, E.g.     <pre><code>flight profile apply standalone1 all-in-one\n</code></pre>     !!! tip     You can check all available identities for the current profile with <code>flight profile identities</code></p> </li> <li> <p>Wait for the identity to finish applying. You can check the status of all nodes with <code>flight profile list</code>.</p> <p>Tip</p> <p>You can watch the progress of the application with <code>flight profile view login1 --watch</code></p> </li> </ol> <p>Success</p> <p>Congratulations, you've now created a Jupyter Lab Standalone environment! Learn more about Jupyter Lab in their documentation.</p>"},{"location":"docs/flight-solo/cluster-build-methods/jupyter-lab-standalone-aws/#verifying-functionality","title":"Verifying Functionality","text":"<ol> <li> <p>Put the IP/FQDN used in configuration into your browser to access the Flight Web Suite. It should look something like this:     </p> </li> <li> <p>Under \"Quick Access\" click on \"Jupyter\" and enter the password set during configuration when requested; it will only need to be entered the first time you connect.     </p> </li> <li> <p>On the Jupyter home page, under the \"Notebook\" section, click on \"Python3\" to open a new notebook.     </p> </li> <li> <p>Enter this code, which will print out a message, wait for a bit, then print again.     <pre><code>import time\nprint(\"Starting running on Jupyter\")\ntime.sleep(3)\nprint(\"Finished running - goodbye from Jupyter\")\n</code></pre> </p> </li> <li> <p>Click the play button to run the cell, and wait for the result.</p> </li> </ol>"},{"location":"docs/flight-solo/cluster-build-methods/jupyter-lab-standalone-azure/","title":"Jupyter Lab Standalone on Azure","text":""},{"location":"docs/flight-solo/cluster-build-methods/jupyter-lab-standalone-azure/#launch-login-node","title":"Launch Login Node","text":"<p>To set up a cluster, you will need to import a Flight Solo image.</p> <ol> <li> <p>Go to the Microsoft Azure portal.</p> <p></p> </li> <li> <p>Go to Virtual Machines, and click \"Create\".</p> <p></p> </li> <li> <p>Select \"Azure virtual machine\", which will load this page:</p> <p></p> </li> <li> <p>On the Basics page:</p> <ol> <li>Set Subscription to your subscription type.</li> <li>Set Resource Group to your desired resource group (where the vm will be kept after creation).</li> <li>Set Virtual machine name to any suitable name. (<code>-</code> does not work in a name)</li> <li>Set Image to the imported Flight Solo Image.<ol> <li>It may be necessary to open the drop-down and/or see all images in order to find the imported image.     </li> <li>Scroll down to see more options     </li> </ol> </li> <li>Set Size to your choice of size.</li> <li>Set Authentication type to <code>SSH public key</code></li> <li>Set Username to any suitable username.</li> <li>Set SSH public key source to the most suitable option, but remember what key was used if creating compute nodes later.</li> <li>Fill in the Key pair name/Stored key/Use existing key as appropriate to the chosen public key source.</li> <li>Allow traffic to selected ports, and select <code>SSH(22)</code>, <code>HTTP(80)</code> and <code>HTTPS(443)</code> as the allowed ports.</li> <li>Set the most appropriate license type.</li> </ol> </li> <li> <p>Continuing on to the next page, Disks, all necessary details should already be filled out, so this page can be skipped (unless you know what you want to change). However, it is recommended to select Delete with VM.</p> <p></p> </li> <li> <p>Go on to the networking tab and fill out the necessary options.</p> <p></p> <ol> <li>Set Virtual Network or create a new one by pressing \"Create new\" and setting a name. Remember what this is for if you create compute nodes.</li> <li>Set Subnet to one of the options in the drop-down menu, if it isn't already set. Remember what this is for if you create compute nodes.</li> <li>Set Public IP to an existing public IP or create a new one by pressing \"Create new\" and setting a name.</li> <li>Set NIC network security group to \"Advanced\", and press \"Create new\" to create a new security group.     <ol> <li>Click on \"Add an inbound rule\" to open the inbound rule creator     </li> <li>Create rules to allow <code>HTTP</code>, <code>HTTPS</code> and <code>SSH</code> traffic from your IP address to the security group.</li> <li>When complete, press \"OK\" at the bottom left of the screen to return to image creation.</li> </ol> </li> </ol> </li> <li> <p>The Management, Monitoring and Tags tabs have more options that aren't necessary for setup. Skip to the tab Advanced</p> </li> <li> <p>In the Custom data and cloud init section, there is a text box. This is where your user data can be specified</p> <p></p> </li> <li> <p>Azure will take some time to review your settings. If there are no issues click \"Create\" to finish creation.</p> </li> </ol>"},{"location":"docs/flight-solo/cluster-build-methods/jupyter-lab-standalone-azure/#general-configuration","title":"General Configuration","text":""},{"location":"docs/flight-solo/cluster-build-methods/jupyter-lab-standalone-azure/#create-node-inventory","title":"Create Node Inventory","text":"<ol> <li> <p>Parse your node(s) with the command <code>flight hunter parse</code>.</p> <ol> <li> <p>This will display a list of hunted nodes, for example     <pre><code>[flight@login-node.novalocal ~]$ flight hunter parse\nSelect nodes: (Scroll for more nodes)\n\u2023 \u2b21 login-node.novalocal - 10.10.0.1\n  \u2b21 compute-node-1.novalocal - 10.10.101.1\n</code></pre></p> </li> <li> <p>Select the desired node to be parsed with Space, and you will be taken to the label editor     <pre><code>Choose label: login-node.novalocal\n</code></pre></p> </li> <li> <p>Here, you can edit the label like plain text     <pre><code>Choose label: login1\n</code></pre></p> <p>Tip</p> <p>You can clear the current node name by pressing Down in the label editor.</p> </li> <li> <p>When done editing, press Enter to save. The modified node label will appear next to the ip address and original node label.     <pre><code>Select nodes: login-node.novalocal - 10.10.0.1 (login1) (Scroll for more nodes)\n\u2023 \u2b22 login-node.novalocal - 10.10.0.1 (login1)\n  \u2b21 compute-node-1.novalocal - 10.10.101.1\n</code></pre></p> </li> <li> <p>From this point, you can either hit Enter to finish parsing and process the selected nodes, or continue changing nodes. Either way, you can return to this list by running <code>flight hunter parse</code>.</p> </li> <li> <p>Save the node inventory before moving on to the next step.</p> <p>Tip</p> <p>See <code>flight hunter parse -h</code> for more ways to parse nodes.</p> </li> </ol> </li> </ol>"},{"location":"docs/flight-solo/cluster-build-methods/jupyter-lab-standalone-azure/#add-genders","title":"Add genders","text":"<ol> <li>Optionally, you may add genders to the newly parsed node. For example, in the case that the node should have the gender <code>cluster</code> and <code>all</code> then run the command:     <pre><code>flight hunter modify-groups --add cluster,all login1\n</code></pre></li> </ol>"},{"location":"docs/flight-solo/cluster-build-methods/jupyter-lab-standalone-azure/#jupyter-lab-standalone-configuration","title":"Jupyter Lab Standalone Configuration","text":"<ol> <li> <p>Configure profile</p> <pre><code>flight profile configure\n</code></pre> <ol> <li>This brings up a UI, where several options need to be set. Use up and down arrow keys to scroll through options and enter to move to the next option. Options in brackets coloured yellow are the default options that will be applied if nothing is entered.<ul> <li>Cluster type: The type of cluster setup needed, in this case select <code>Jupyter Standalone</code>.</li> <li>Cluster name: The name of the cluster.</li> <li>Default user: The user that you log in with.</li> <li>Set user password: Set a password to be used for the chosen default user.</li> <li>IP or FQDN for Web Access: As described here, this could be the public IP or public hostname.</li> </ul> </li> </ol> </li> <li> <p>Apply an identity by running the command <code>flight profile apply</code>, E.g.     <pre><code>flight profile apply standalone1 all-in-one\n</code></pre>     !!! tip     You can check all available identities for the current profile with <code>flight profile identities</code></p> </li> <li> <p>Wait for the identity to finish applying. You can check the status of all nodes with <code>flight profile list</code>.</p> <p>Tip</p> <p>You can watch the progress of the application with <code>flight profile view login1 --watch</code></p> </li> </ol> <p>Success</p> <p>Congratulations, you've now created a Jupyter Lab Standalone environment! Learn more about Jupyter Lab in their documentation.</p>"},{"location":"docs/flight-solo/cluster-build-methods/jupyter-lab-standalone-azure/#verifying-functionality","title":"Verifying Functionality","text":"<ol> <li> <p>Put the IP/FQDN used in configuration into your browser to access the Flight Web Suite. It should look something like this:     </p> </li> <li> <p>Under \"Quick Access\" click on \"Jupyter\" and enter the password set during configuration when requested; it will only need to be entered the first time you connect.     </p> </li> <li> <p>On the Jupyter home page, under the \"Notebook\" section, click on \"Python3\" to open a new notebook.     </p> </li> <li> <p>Enter this code, which will print out a message, wait for a bit, then print again.     <pre><code>import time\nprint(\"Starting running on Jupyter\")\ntime.sleep(3)\nprint(\"Finished running - goodbye from Jupyter\")\n</code></pre> </p> </li> <li> <p>Click the play button to run the cell, and wait for the result.</p> </li> </ol>"},{"location":"docs/flight-solo/cluster-build-methods/jupyter-lab-standalone-openstack/","title":"Jupyter Lab Standalone on OpenStack","text":""},{"location":"docs/flight-solo/cluster-build-methods/jupyter-lab-standalone-openstack/#launch-login-node","title":"Launch Login Node","text":"<p>To set up a cluster, you will need to import a Flight Solo image.</p> <p>Before setting up a cluster on Openstack, there are several required prerequisites:</p> <ul> <li>Your own keypair</li> <li>A network</li> <li>A router<ul> <li>With an interface both on the External Gateway network and an Internal Interface on the previously created network</li> </ul> </li> <li>A security group that allows traffic is given below (if creating the security group through the web interface then the \"Any\" protocol will need to be an \"Other Protocol\" rule with \"IP Protocol\" of <code>-1</code>)</li> </ul> Protocol Direction CIDR Port Range Any egress 0.0.0.0/0 any Any ingress Virtual Network CIDR any ICMP ingress 0.0.0.0/0 any SSH ingress 0.0.0.0/0 22 TCP ingress 0.0.0.0/0 80 TCP ingress 0.0.0.0/0 443 TCP ingress 0.0.0.0/0 5900-5903 <p>Note</p> <p>The \"Virtual Network CIDR\" is the subnet and netmask for the network that the nodes are using. For example, a node on the 11.11.11.0 network with a netmask of 255.255.255.0 would have a network CIDR of 11.11.11.0/24.</p> <p>The documentation includes instructions for importing an image to Openstack, and guides for setting up the other prerequisites can be found in the Openstack documentation</p> <p>To set up a cluster:</p> <ol> <li> <p>Go to the Openstack instances page.</p> <p></p> </li> <li> <p>Click \"Launch Instance\", and the instance creation window will pop up.</p> </li> <li> <p>Fill in the instance name, and leave the number of instances as 1, then click next.</p> <p></p> </li> <li> <p>Choose the desired image to use by clicking the up arrow at the end of its row. It will be displayed in the \"Allocated\" section when selected.</p> <p></p> </li> <li> <p>Choose the desired instance size by clicking the up arrow at the end of its row. It will be displayed in the \"Allocated\" section when selected.</p> <p></p> </li> <li> <p>Choose a network in the same way as an image or instance size. Note that all nodes in a cluster must be on the same network.</p> <p></p> </li> <li> <p>Choose a security group in the same way as an image or instance size. Note that all nodes in a cluster must be in the same security group.</p> <p></p> </li> <li> <p>Choose the keypair in the same way as an image or instance size.</p> <p></p> </li> <li> <p>In the \"Configuration\" section, there is a \"Customisation Script\" section with a text box. This will be used to set your user data</p> <p></p> </li> <li> <p>When all options have been selected, press the \"Launch Instance\" button to launch. If the button is greyed out, then a mandatory setting has not been configured.</p> <p></p> </li> <li> <p>Go to the \"Instances\" page in the \"Compute\" section. The created node should be there and be finishing or have finished creation.</p> <p></p> </li> <li> <p>Click on the down arrow at the end of the instance row. This will bring up a drop-down menu.</p> </li> <li> <p>Select \"Associate Floating IP\", this will make the ip management window pop up.</p> <p></p> </li> <li> <p>Associate a floating IP, either by using an existing one or allocating a new one.</p> <ol> <li> <p>To use an existing floating IP:</p> <ol> <li> <p>Open the IP Address drop-down menu.</p> <p></p> </li> <li> <p>Select one of the IP Addresses.</p> </li> <li> <p>Click \"Associate\" to finish associating an IP.</p> </li> </ol> </li> <li> <p>To allocate a new floating IP:</p> <ol> <li> <p>Click the \"+\" next to the drop-down arrow to open the allocation menu.</p> <p></p> </li> <li> <p>Click \"Allocate IP\".</p> </li> </ol> </li> </ol> </li> <li> <p>Click \"Associate\" to finish associating an IP.</p> </li> </ol>"},{"location":"docs/flight-solo/cluster-build-methods/jupyter-lab-standalone-openstack/#general-configuration","title":"General Configuration","text":""},{"location":"docs/flight-solo/cluster-build-methods/jupyter-lab-standalone-openstack/#create-node-inventory","title":"Create Node Inventory","text":"<ol> <li> <p>Parse your node(s) with the command <code>flight hunter parse</code>.</p> <ol> <li> <p>This will display a list of hunted nodes, for example     <pre><code>[flight@login-node.novalocal ~]$ flight hunter parse\nSelect nodes: (Scroll for more nodes)\n\u2023 \u2b21 login-node.novalocal - 10.10.0.1\n  \u2b21 compute-node-1.novalocal - 10.10.101.1\n</code></pre></p> </li> <li> <p>Select the desired node to be parsed with Space, and you will be taken to the label editor     <pre><code>Choose label: login-node.novalocal\n</code></pre></p> </li> <li> <p>Here, you can edit the label like plain text     <pre><code>Choose label: login1\n</code></pre></p> <p>Tip</p> <p>You can clear the current node name by pressing Down in the label editor.</p> </li> <li> <p>When done editing, press Enter to save. The modified node label will appear next to the ip address and original node label.     <pre><code>Select nodes: login-node.novalocal - 10.10.0.1 (login1) (Scroll for more nodes)\n\u2023 \u2b22 login-node.novalocal - 10.10.0.1 (login1)\n  \u2b21 compute-node-1.novalocal - 10.10.101.1\n</code></pre></p> </li> <li> <p>From this point, you can either hit Enter to finish parsing and process the selected nodes, or continue changing nodes. Either way, you can return to this list by running <code>flight hunter parse</code>.</p> </li> <li> <p>Save the node inventory before moving on to the next step.</p> <p>Tip</p> <p>See <code>flight hunter parse -h</code> for more ways to parse nodes.</p> </li> </ol> </li> </ol>"},{"location":"docs/flight-solo/cluster-build-methods/jupyter-lab-standalone-openstack/#add-genders","title":"Add genders","text":"<ol> <li>Optionally, you may add genders to the newly parsed node. For example, in the case that the node should have the gender <code>cluster</code> and <code>all</code> then run the command:     <pre><code>flight hunter modify-groups --add cluster,all login1\n</code></pre></li> </ol>"},{"location":"docs/flight-solo/cluster-build-methods/jupyter-lab-standalone-openstack/#jupyter-lab-standalone-configuration","title":"Jupyter Lab Standalone Configuration","text":"<ol> <li> <p>Configure profile</p> <pre><code>flight profile configure\n</code></pre> <ol> <li>This brings up a UI, where several options need to be set. Use up and down arrow keys to scroll through options and enter to move to the next option. Options in brackets coloured yellow are the default options that will be applied if nothing is entered.<ul> <li>Cluster type: The type of cluster setup needed, in this case select <code>Jupyter Standalone</code>.</li> <li>Cluster name: The name of the cluster.</li> <li>Default user: The user that you log in with.</li> <li>Set user password: Set a password to be used for the chosen default user.</li> <li>IP or FQDN for Web Access: As described here, this could be the public IP or public hostname.</li> </ul> </li> </ol> </li> <li> <p>Apply an identity by running the command <code>flight profile apply</code>, E.g.     <pre><code>flight profile apply standalone1 all-in-one\n</code></pre>     !!! tip     You can check all available identities for the current profile with <code>flight profile identities</code></p> </li> <li> <p>Wait for the identity to finish applying. You can check the status of all nodes with <code>flight profile list</code>.</p> <p>Tip</p> <p>You can watch the progress of the application with <code>flight profile view login1 --watch</code></p> </li> </ol> <p>Success</p> <p>Congratulations, you've now created a Jupyter Lab Standalone environment! Learn more about Jupyter Lab in their documentation.</p>"},{"location":"docs/flight-solo/cluster-build-methods/jupyter-lab-standalone-openstack/#verifying-functionality","title":"Verifying Functionality","text":"<ol> <li> <p>Put the IP/FQDN used in configuration into your browser to access the Flight Web Suite. It should look something like this:     </p> </li> <li> <p>Under \"Quick Access\" click on \"Jupyter\" and enter the password set during configuration when requested; it will only need to be entered the first time you connect.     </p> </li> <li> <p>On the Jupyter home page, under the \"Notebook\" section, click on \"Python3\" to open a new notebook.     </p> </li> <li> <p>Enter this code, which will print out a message, wait for a bit, then print again.     <pre><code>import time\nprint(\"Starting running on Jupyter\")\ntime.sleep(3)\nprint(\"Finished running - goodbye from Jupyter\")\n</code></pre> </p> </li> <li> <p>Click the play button to run the cell, and wait for the result.</p> </li> </ol>"},{"location":"docs/flight-solo/cluster-build-methods/kubernetes-multinode-alces-cloud/","title":"Kubernetes Multinode on Alces Cloud","text":""},{"location":"docs/flight-solo/cluster-build-methods/kubernetes-multinode-alces-cloud/#launch-login-node","title":"Launch Login Node","text":""},{"location":"docs/flight-solo/cluster-build-methods/kubernetes-multinode-alces-cloud/#prepare-user-data","title":"Prepare User Data","text":"<p>When launching a login node it is worth considering what user data options to provide. While it is not required, user data can provide powerful customisation at launch that can further streamline the cluster build process.</p> <p>There are several options that can be added to change how a compute node will contact nodes on startup.</p> <ul> <li>Sharing public ssh key to clients: <ul> <li>Instead of manually obtaining and sharing the root public SSH key (passwordless root ssh is required for flight profile) this can be shared over the local network with <code>SHAREPUBKEY=true</code></li> </ul> </li> <li>Add an auth key:<ul> <li>Add the line <code>AUTH_KEY=&lt;string&gt;</code>. This means that the node will only accept incoming flight hunter nodes that provide a matching authorisation key</li> </ul> </li> </ul> An example of all mentioned lines in a single cloud init script.<pre><code>#cloud-config\nwrite_files:\n  - content: |\n      SHAREPUBKEY=true\n      AUTH_KEY=banana\n    path: /opt/flight/cloudinit.in\n    permissions: '0600'\n    owner: root:root\n  - content: |\n      /opt/flight/bin/flight profile prepare openflight-kubernetes-multinode\n    path: /var/lib/firstrun/scripts/00-prepare-profile.bash\n    permissions: '0600'\n    owner: root:root\n</code></pre> <p>Note</p> <p>The section that writes the <code>/var/lib/firstrun/scripts/00-prepare-profile.bash</code> file sets up the necessary dependencies for Kubernetes automatically when the login node is launched</p> <p>Info</p> <p>More information on available user data options for Flight Solo via the user data documentation</p>"},{"location":"docs/flight-solo/cluster-build-methods/kubernetes-multinode-alces-cloud/#deploy","title":"Deploy","text":"CLIGUI <p>To set up a cluster, you will need to import a Flight Solo image.</p> <p>Before setting up a cluster on Alces Cloud, there are several required prerequisites:</p> <ul> <li>Your own keypair</li> <li>A network with a subnet and a router bridging the subnet to the external network</li> <li>A security group that allows traffic is given below (if creating the security group through the web interface then the \"Any\" protocol will need to be an \"Other Protocol\" rule with \"IP Protocol\" of <code>-1</code>)</li> </ul> Protocol Direction CIDR Port Range Any egress 0.0.0.0/0 any Any ingress Virtual Network CIDR any ICMP ingress 0.0.0.0/0 any SSH ingress 0.0.0.0/0 22 TCP ingress 0.0.0.0/0 80 TCP ingress 0.0.0.0/0 443 TCP ingress 0.0.0.0/0 5900-5903 <p>Note</p> <p>The \"Virtual Network CIDR\" is the subnet and netmask for the network that the nodes are using. For example, a node on the 11.11.11.0 network with a netmask of 255.255.255.0 would have a network CIDR of 11.11.11.0/24.</p> <ol> <li> <p>Launch a login node with a command similar to the following: </p> <pre><code>$ openstack server create --flavor p1.small \\\n                          --image \"Flight Solo VERSION\" \\\n                          --boot-from-volume 16 \\\n                          --network \"mycluster1-network\" \\\n                          --key-name \"MyKey\" \\\n                          --security-group \"mycluster1-sg\" \\\n                          --user-data myuserdata.yml \\\n                          login1\n</code></pre> <ul> <li> <p>Where:</p> <ul> <li><code>flavor</code> - Is the desired size of the instance</li> <li><code>image</code> - Is the Flight Solo image imported to Alces Cloud</li> <li><code>boot-from-volume</code> - Is the size of the system disk in GB</li> <li><code>network</code> - Is the name or ID of the network created for the cluster</li> <li><code>key-name</code> - Is the name of the SSH key to use</li> <li><code>security-group</code> - Is the name or ID of the security group created previously</li> <li><code>user-data</code> - Is the file containing cloud-init user-data (this is optional in standalone scenarios)</li> <li><code>login1</code> - Is the name of the system</li> </ul> </li> </ul> </li> <li> <p>Associate a floating IP, either by using an existing one or creating a new one</p> <ol> <li> <p>To use an existing floating IP</p> <ol> <li> <p>Identify the IP address of an available floating IP (<code>Port</code> will be <code>None</code>)</p> <pre><code>$ openstack floating ip list\n+---------------------+---------------------+------------------+---------------------+---------------------+-----------------------+\n| ID                  | Floating IP Address | Fixed IP Address | Port                | Floating Network    | Project               |\n+---------------------+---------------------+------------------+---------------------+---------------------+-----------------------+\n| 726318f4-4dbb-4d51- | 10.199.31.6         | None             | None                | c681d94b-e2ec-4b73- | dcd92da7538a4f64a42b0 |\n| b119-d9e53c47a9f5   |                     |                  |                     | 89bf-9943bcce3255   | d4d9ce8845f           |\n</code></pre> </li> <li> <p>Associate the floating IP with the instance</p> <pre><code>$ openstack server add floating ip login1 10.199.31.6\n</code></pre> </li> </ol> </li> <li> <p>To create a new floating IP </p> <ol> <li> <p>Create new floating IP and note the <code>floating_ip_address</code> </p> <pre><code>$ openstack floating ip create external1\n</code></pre> </li> <li> <p>Associate the floating IP with the instance (using the <code>floating_ip_address</code> from the previous output)</p> <pre><code>$ openstack server add floating ip login1 10.199.31.212\n</code></pre> </li> </ol> </li> </ol> </li> </ol> <p>Further detail on collecting the information from the above can be found in the Alces Cloud documentation.</p> <p>To set up a cluster, you will need to import a Flight Solo image.</p> <p>Before setting up a cluster on Alces Cloud, there are several required prerequisites:</p> <ul> <li>Your own keypair</li> <li>A network with a subnet and a router bridging the subnet to the external network</li> <li>A security group that allows traffic is given below (if creating the security group through the web interface then the \"Any\" protocol will need to be an \"Other Protocol\" rule with \"IP Protocol\" of <code>-1</code>)</li> </ul> Protocol Direction CIDR Port Range Any egress 0.0.0.0/0 any Any ingress Virtual Network CIDR any ICMP ingress 0.0.0.0/0 any SSH ingress 0.0.0.0/0 22 TCP ingress 0.0.0.0/0 80 TCP ingress 0.0.0.0/0 443 TCP ingress 0.0.0.0/0 5900-5903 <p>Note</p> <p>The \"Virtual Network CIDR\" is the subnet and netmask for the network that the nodes are using. For example, a node on the 11.11.11.0 network with a netmask of 255.255.255.0 would have a network CIDR of 11.11.11.0/24.</p> <p>To set up a cluster:</p> <ol> <li> <p>Go to the Alces Cloud instances page.</p> <p></p> </li> <li> <p>Click \"Launch Instance\", and the instance creation window will pop up.</p> </li> <li> <p>Fill in the instance name, and leave the number of instances as 1, then click next.</p> <p></p> </li> <li> <p>Choose the desired image to use by clicking the up arrow at the end of its row. It will be displayed in the \"Allocated\" section when selected.</p> <p></p> </li> <li> <p>Choose the desired instance size by clicking the up arrow at the end of its row. It will be displayed in the \"Allocated\" section when selected.</p> <p></p> </li> <li> <p>Choose a network in the same way as an image or instance size. Note that all nodes in a cluster must be on the same network.</p> <p></p> </li> <li> <p>Choose a security group in the same way as an image or instance size. Note that all nodes in a cluster must be in the same security group.</p> <p></p> </li> <li> <p>Choose the keypair in the same way as an image or instance size.</p> <p></p> </li> <li> <p>In the \"Configuration\" section, there is a \"Customisation Script\" section with a text box. This will be used to set your user data</p> <p></p> </li> <li> <p>When all options have been selected, press the \"Launch Instance\" button to launch. If the button is greyed out, then a mandatory setting has not been configured.</p> <p></p> </li> <li> <p>Go to the \"Instances\" page in the \"Compute\" section. The created node should be there and be finishing or have finished creation.</p> <p></p> </li> <li> <p>Click on the down arrow at the end of the instance row. This will bring up a drop-down menu.</p> </li> <li> <p>Select \"Associate Floating IP\", this will make the ip management window pop up.</p> <p></p> </li> <li> <p>Associate a floating IP, either by using an existing one or allocating a new one.</p> <ol> <li> <p>To use an existing floating IP:</p> <ol> <li> <p>Open the IP Address drop-down menu.</p> <p></p> </li> <li> <p>Select one of the IP Addresses.</p> </li> <li> <p>Click \"Associate\" to finish associating an IP.</p> </li> </ol> </li> <li> <p>To allocate a new floating IP:</p> <ol> <li> <p>Click the \"+\" next to the drop-down arrow to open the allocation menu.</p> <p></p> </li> <li> <p>Click \"Allocate IP\".</p> </li> </ol> </li> </ol> </li> <li> <p>Click \"Associate\" to finish associating an IP.</p> </li> </ol> <p>Further detail on collecting the information from the above can be found in the Alces Cloud documentation.</p>"},{"location":"docs/flight-solo/cluster-build-methods/kubernetes-multinode-alces-cloud/#launch-compute-nodes","title":"Launch Compute Nodes","text":""},{"location":"docs/flight-solo/cluster-build-methods/kubernetes-multinode-alces-cloud/#prepare-user-data_1","title":"Prepare User Data","text":"<p>Setting up compute nodes is done slightly differently than a login node. The basic steps are the same except subnets, networks and security groups need to match the ones used for the login node.</p> <p>This is the smallest amount of cloud init data necessary. It allows the login node to find the compute nodes as long as they are on the same network, and ssh into them from the root user (which is necessary for setup). <pre><code>#cloud-config\nusers:\n  - default\n  - name: root\n    ssh_authorized_keys:\n      - &lt;Content of ~/.ssh/id_alcescluster.pub from root user on login node&gt;\n</code></pre></p> <p>Tip</p> <p>The above is not required if the <code>SHAREPUBKEY</code> option was provided to the login node. If this was the case then the <code>SERVER</code> option provided to the compute node will be enough to enable root access from the login node.</p> <p>There are several options that can be added to change how a compute node will contact nodes on startup.</p> <ul> <li>Sending to a specific server:<ul> <li>Instead of broadcasting across a range, add the line <code>SERVER=&lt;private server IP&gt;</code> to send to specifically that node, which would be your login node.</li> </ul> </li> <li>Add an auth key:<ul> <li>Add the line <code>AUTH_KEY=&lt;string&gt;</code>. This means that the compute node will send it's flight hunter packet with this key. This must match the auth key provided to your login node</li> </ul> </li> </ul> An example of all mentioned lines in a single cloud init script.<pre><code>#cloud-config\nwrite_files:\n  - content: |\n      SERVER=10.10.0.1\n      AUTH_KEY=banana\n    path: /opt/flight/cloudinit.in\n    permissions: '0600'\n    owner: root:root\n  - content: |\n      /opt/flight/bin/flight profile prepare openflight-kubernetes-multinode\n    path: /var/lib/firstrun/scripts/00-prepare-profile.bash\n    permissions: '0600'\n    owner: root:root\nusers:\n  - default\n  - name: root\n    ssh_authorized_keys:\n      - &lt;Content of ~/.ssh/id_alcescluster.pub from root user on login node&gt;\n</code></pre> <p>Note</p> <p>The section that writes the <code>/var/lib/firstrun/scripts/00-prepare-profile.bash</code> file sets up the necessary dependencies for Kubernetes automatically when the compute node is launched</p> <p>Info</p> <p>More information on available user data options for Flight Solo via the user data documentation</p>"},{"location":"docs/flight-solo/cluster-build-methods/kubernetes-multinode-alces-cloud/#deploy_1","title":"Deploy","text":"CLIGUI <ol> <li> <p>Launch a compute node with a command similar to the following: </p> <pre><code>$ openstack server create --flavor p1.small \\\n                          --image \"Flight Solo VERSION\" \\\n                          --boot-from-volume 10 \\\n                          --network \"mycluster1-network\" \\\n                          --key-name \"MyKey\" \\\n                          --security-group \"mycluster1-sg\" \\\n                          --user-data myuserdata.yml \\\n                          --min 2 \\\n                          --max 2 \\\n                          node\n</code></pre> <ul> <li> <p>Where:</p> <ul> <li><code>flavor</code> - Is the desired size of the instance</li> <li><code>image</code> - Is the Flight Solo image imported to Alces Cloud</li> <li><code>boot-from-volume</code> - Is the size of the system disk in GB</li> <li><code>network</code> - Is the name or ID of the network created for the cluster</li> <li><code>key-name</code> - Is the name of the SSH key to use</li> <li><code>security-group</code> - Is the name or ID of the security group created previously</li> <li><code>user-data</code> - Is the file containing cloud-init user-data </li> <li><code>min</code> and <code>max</code> - Is the number of nodes to launch</li> <li><code>node</code> - Is the name of the deployment to have numbers appended to (e.g. this example creates <code>node-1</code> and <code>node-2</code>)</li> </ul> </li> </ul> </li> </ol> <p>Further detail on collecting the information from the above can be found in the Alces Cloud documentation.</p> <ol> <li> <p>Go to the Alces Cloud instances page.</p> <p></p> </li> <li> <p>Click \"Launch Instance\", and the instance creation window will pop up.</p> </li> <li> <p>Fill in the instance name, and set the number of instances to create, then click next.</p> <p></p> </li> <li> <p>Choose the desired image to use by clicking the up arrow at the end of its row. It will be displayed in the \"Allocated\" section when selected.</p> <p></p> </li> <li> <p>Choose the desired instance size by clicking the up arrow at the end of its row. It will be displayed in the \"Allocated\" section when selected.</p> <p></p> </li> <li> <p>Choose a network in the same way as an image or instance size. Note that this should be the same network as the login node.</p> <p></p> </li> <li> <p>Choose a security group in the same way as an image or instance size. Note that this should be the same security group as the login node.</p> <p></p> </li> <li> <p>Choose the keypair in the same way as an image or instance size.</p> <p></p> </li> <li> <p>In the \"Configuration\" section, there is a \"Customisation Script\" section with a text box. This will be used to set user data script prepared earlier</p> <p></p> </li> <li> <p>When all options have been selected, press the \"Launch Instance\" button to launch. If the button is greyed out, then a mandatory setting has not been configured.</p> <p></p> </li> </ol>"},{"location":"docs/flight-solo/cluster-build-methods/kubernetes-multinode-alces-cloud/#general-configuration","title":"General Configuration","text":""},{"location":"docs/flight-solo/cluster-build-methods/kubernetes-multinode-alces-cloud/#create-node-inventory","title":"Create Node Inventory","text":"<ol> <li> <p>Parse your node(s) with the command <code>flight hunter parse</code>.</p> <ol> <li> <p>This will display a list of hunted nodes, for example     <pre><code>[flight@login-node.novalocal ~]$ flight hunter parse\nSelect nodes: (Scroll for more nodes)\n\u2023 \u2b21 login-node.novalocal - 10.10.0.1\n  \u2b21 compute-node-1.novalocal - 10.10.101.1\n</code></pre></p> </li> <li> <p>Select the desired node to be parsed with Space, and you will be taken to the label editor     <pre><code>Choose label: login-node.novalocal\n</code></pre></p> </li> <li> <p>Here, you can edit the label like plain text     <pre><code>Choose label: login1\n</code></pre></p> <p>Tip</p> <p>You can clear the current node name by pressing Down in the label editor.</p> </li> <li> <p>When done editing, press Enter to save. The modified node label will appear next to the ip address and original node label.     <pre><code>Select nodes: login-node.novalocal - 10.10.0.1 (login1) (Scroll for more nodes)\n\u2023 \u2b22 login-node.novalocal - 10.10.0.1 (login1)\n  \u2b21 compute-node-1.novalocal - 10.10.101.1\n</code></pre></p> </li> <li> <p>From this point, you can either hit Enter to finish parsing and process the selected nodes, or continue changing nodes. Either way, you can return to this list by running <code>flight hunter parse</code>.</p> </li> <li> <p>Save the node inventory before moving on to the next step.</p> <p>Tip</p> <p>See <code>flight hunter parse -h</code> for more ways to parse nodes.</p> </li> </ol> </li> </ol>"},{"location":"docs/flight-solo/cluster-build-methods/kubernetes-multinode-alces-cloud/#add-genders","title":"Add genders","text":"<ol> <li>Optionally, you may add genders to the newly parsed node. For example, in the case that the node should have the gender <code>cluster</code> and <code>all</code> then run the command:     <pre><code>flight hunter modify-groups --add cluster,all login1\n</code></pre></li> </ol>"},{"location":"docs/flight-solo/cluster-build-methods/kubernetes-multinode-alces-cloud/#kubernetes-multinode-configuration","title":"Kubernetes Multinode Configuration","text":"<ol> <li> <p>Configure profile</p> <p><pre><code>flight profile configure\n</code></pre> 1. This brings up a UI, where several options need to be set. Use up and down arrow keys to scroll through options and enter to move to the next option. Options in brackets coloured yellow are the default options that will be applied if nothing is entered.     - Cluster type: The type of cluster setup needed, in this case <code>Openflight Kubernetes Multinode</code>.     - Cluster name: The name of the cluster.     - Default user: The user that you log in with.     - IP range of compute nodes: The IP range of the compute nodes used, remember to add the netmask. E.g. <code>172.31.16.0/20</code>     - IP range of Kubernetes pods: The IP range that the kubernetes pods should use, make sure this is different from the IP range of the compute nodes, and remember to add the net mask. E.g. <code>192.168.0.0/16</code></p> </li> <li> <p>Apply identities by running the command <code>flight profile apply</code></p> <ol> <li>First apply an identity to the login node     <pre><code>flight profile apply login1 master\n</code></pre></li> <li> <p>Wait for the login node identity to finish applying. You can check the status of all nodes with <code>flight profile list</code>.</p> <p>Tip</p> <p>You can watch the progress of the application with <code>flight profile view login1 --watch</code></p> </li> <li> <p>Apply an identity to the each of the compute nodes.  E.g.     <pre><code>flight profile apply node01,node02 worker\n</code></pre></p> <p>Tip</p> <p>You can check all available identities for the current profile with <code>flight profile identities</code></p> </li> </ol> </li> </ol> <p>Success</p> <p>Congratulations, you've now created a Kubernetes Multinode environment! Learn more about Kubernetes in their documentation.</p>"},{"location":"docs/flight-solo/cluster-build-methods/kubernetes-multinode-alces-cloud/#verifying-functionality","title":"Verifying Functionality","text":""},{"location":"docs/flight-solo/cluster-build-methods/kubernetes-multinode-alces-cloud/#check-nodes-runningready","title":"Check Nodes Running/Ready","text":"<ol> <li>As the <code>default_username</code> (unless this was changed, it will be <code>flight</code>) check nodes are \"Ready\"     <pre><code>kubectl get nodes\n</code></pre></li> </ol>"},{"location":"docs/flight-solo/cluster-build-methods/kubernetes-multinode-alces-cloud/#launching-a-pod","title":"Launching a \"Pod\"","text":"<ol> <li> <p>Get test yaml file for the VM <pre><code>flight silo file pull openflight:kubernetes/pod-launch-test.yaml\n</code></pre></p> </li> <li> <p>Launch a pod (this will create an ubuntu VM that sleeps for 10 minutes then exits)     <pre><code>kubectl apply -f pod-launch-test.yaml\n</code></pre></p> </li> <li> <p>Check that the pod is running     <pre><code>kubectl get pods -o wide\n</code></pre></p> </li> <li> <p>The pod should be running without issues.</p> </li> </ol>"},{"location":"docs/flight-solo/cluster-build-methods/kubernetes-multinode-alces-cloud/#perform-network-test","title":"Perform Network Test","text":"<ol> <li>Create yaml file for a <code>php-apache</code> service     <pre><code>flight silo file pull openflight:kubernetes/php-apache.yaml\n</code></pre></li> <li>Launch pod service     <pre><code>kubectl apply -f php-apache.yaml\n</code></pre></li> <li>Get yaml file for VM to verify connection from     <pre><code>flight silo file pull openflight:kubernetes/busybox-wget.yaml\n</code></pre></li> <li>Launch pod     <pre><code>kubectl apply -f busybox-wget.yaml\n</code></pre></li> <li>View output of <code>wget</code> pod (this should show <code>OK!</code>)     <pre><code>kubectl logs busybox-wget\n</code></pre></li> </ol>"},{"location":"docs/flight-solo/cluster-build-methods/kubernetes-multinode-aws/","title":"Kubernetes Multinode on AWS","text":""},{"location":"docs/flight-solo/cluster-build-methods/kubernetes-multinode-aws/#launch-login-node","title":"Launch Login Node","text":""},{"location":"docs/flight-solo/cluster-build-methods/kubernetes-multinode-aws/#prepare-user-data","title":"Prepare User Data","text":"<p>When launching a login node it is worth considering what user data options to provide. While it is not required, user data can provide powerful customisation at launch that can further streamline the cluster build process.</p> <p>There are several options that can be added to change how a compute node will contact nodes on startup.</p> <ul> <li>Sharing public ssh key to clients: <ul> <li>Instead of manually obtaining and sharing the root public SSH key (passwordless root ssh is required for flight profile) this can be shared over the local network with <code>SHAREPUBKEY=true</code></li> </ul> </li> <li>Add an auth key:<ul> <li>Add the line <code>AUTH_KEY=&lt;string&gt;</code>. This means that the node will only accept incoming flight hunter nodes that provide a matching authorisation key</li> </ul> </li> </ul> An example of all mentioned lines in a single cloud init script.<pre><code>#cloud-config\nwrite_files:\n  - content: |\n      SHAREPUBKEY=true\n      AUTH_KEY=banana\n    path: /opt/flight/cloudinit.in\n    permissions: '0600'\n    owner: root:root\n  - content: |\n      /opt/flight/bin/flight profile prepare openflight-kubernetes-multinode\n    path: /var/lib/firstrun/scripts/00-prepare-profile.bash\n    permissions: '0600'\n    owner: root:root\n</code></pre> <p>Note</p> <p>The section that writes the <code>/var/lib/firstrun/scripts/00-prepare-profile.bash</code> file sets up the necessary dependencies for Kubernetes automatically when the login node is launched</p> <p>Info</p> <p>More information on available user data options for Flight Solo via the user data documentation</p>"},{"location":"docs/flight-solo/cluster-build-methods/kubernetes-multinode-aws/#deploy","title":"Deploy","text":"AWS MarketplaceAWS Imported <ol> <li> <p>Find the Flight Solo image here or by searching the marketplace for \"Flight Solo\".</p> </li> <li> <p>Click \"Continue to Subscribe\"</p> <p></p> </li> <li> <p>Read the terms and conditions, then click \"Continue to Configuration\"</p> <p></p> </li> <li> <p>Configure region, software version (if unsure use the latest), and fulfillment option (if unsure use the default). Then click \"Continue to Launch\". Make sure the region is the same for all nodes to be used in a cluster.</p> <p></p> </li> <li> <p>Click on \"Usage Instructions\" to see some instructions on how to get started, and a link to this documentation.</p> <p></p> </li> <li> <p>Select the \"Launch from Website\" action.</p> <p></p> </li> <li> <p>Choose an instance type to use.</p> <p></p> </li> <li> <p>Choose VPC settings. Remember what VPC was used to create this instance, as it should also be used for any associated compute nodes.</p> <p></p> </li> <li> <p>Choose a subnet. Remember what subnet was used to create this instance, as it should also be used for any associated compute nodes.</p> <p></p> </li> <li> <p>A security group is needed to associate with all nodes on the cluster. It is recommended to use a security group with rules limiting traffic through:</p> <ul> <li>HTTP</li> <li>HTTPS</li> <li>SSH</li> <li>Port 8888</li> <li>Ports 5900 - 5903</li> <li>All traffic from within the security group should be allowed. (This rule can only be added after creation)</li> </ul> <p>Note</p> <p>If you already have a security group which does this, use it here and make sure to use it again for the compute nodes. Otherwise, a security group can be made from the launch page, or through the security groups page</p> <p>Describing exactly how to create a security group is out of scope for this documentation, but covered by the AWS documentation.</p> <p>However, here is an example security group that might be used for a Flight Solo cluster:</p> <p></p> <p>Tip</p> <p>The seller's settings (shown below) can be used as a reference for creating a security group.</p> <p></p> </li> <li> <p>After a security group has been made, click \"Select existing security group\" select it from the drop down menu.</p> <p></p> </li> <li> <p>Choose what key pair to use. It is good practice for this to be the same on all nodes in a cluster.</p> <p></p> </li> <li> <p>Click Launch</p> <p></p> </li> </ol> <p>To set up a cluster, you will need to import a Flight Solo image.</p> <ol> <li> <p>Go the EC2 instance console</p> <p></p> </li> <li> <p>Click \"Launch\" to go to the EC2 instance setup page.</p> <p></p> </li> <li> <p>Set the number of instances to 1, and name of instance to something descriptive.</p> <p></p> </li> <li> <p>Confirm that the region(top right, next to username) is correct.</p> <p></p> </li> <li> <p>In the \"Application and OS Images\" section choose the \"My AMIs\" tab and select your imported solo AMI.</p> <p></p> </li> <li> <p>In the \"Instance type\" section, choose the required instance size.</p> <p></p> </li> <li> <p>In the \"Keypair\" section, select a keypair to use. It is good practice to use the same keypair for the login and compute nodes.</p> <p></p> </li> <li> <p>In the \"Network settings\" sections, click the \"Edit\" button to set the network and subnet. Remember what these are, as they should be the same for any associated compute nodes.</p> <p></p> <p></p> </li> <li> <p>Another thing needed is a security group to associate with all nodes on the cluster. It is recommended to use a security group with rules limiting traffic through:</p> <ul> <li>HTTP</li> <li>HTTPS</li> <li>SSH</li> <li>Port 8888</li> <li>Ports 5900 - 5903</li> <li>All traffic from within the security group should be allowed. (This rule can only be added after creation)</li> </ul> <p>Note</p> <p>If you already have a security group which does this, use it here and make sure to use it again for the compute nodes. Otherwise, a security group can be made from the launch page, or through the security groups page</p> <p>Describing exactly how to create a security group is out of scope for this documentation, but covered by the AWS documentation.</p> <p>However, here is an example security group that might be used for a Flight Solo cluster:</p> <p></p> </li> <li> <p>After a security group has been made, click \"Choose Existing\" select it from the drop down menu.</p> </li> <li> <p>In the \"Configure Storage\" section, allocate as much memory as needed. 8GB is the minimum required for Flight Solo, so it is likely the compute nodes will not need much more than that, as the login node hosts most data.</p> <p></p> </li> <li> <p>Finally, click \"Launch Instance\".</p> </li> </ol>"},{"location":"docs/flight-solo/cluster-build-methods/kubernetes-multinode-aws/#launch-compute-nodes","title":"Launch Compute Nodes","text":""},{"location":"docs/flight-solo/cluster-build-methods/kubernetes-multinode-aws/#prepare-user-data_1","title":"Prepare User Data","text":"<p>Setting up compute nodes is done slightly differently than a login node. The basic steps are the same except subnets, networks and security groups need to match the ones used for the login node.</p> <p>This is the smallest amount of cloud init data necessary. It allows the login node to find the compute nodes as long as they are on the same network, and ssh into them from the root user (which is necessary for setup). <pre><code>#cloud-config\nusers:\n  - default\n  - name: root\n    ssh_authorized_keys:\n      - &lt;Content of ~/.ssh/id_alcescluster.pub from root user on login node&gt;\n</code></pre></p> <p>Tip</p> <p>The above is not required if the <code>SHAREPUBKEY</code> option was provided to the login node. If this was the case then the <code>SERVER</code> option provided to the compute node will be enough to enable root access from the login node.</p> <p>There are several options that can be added to change how a compute node will contact nodes on startup.</p> <ul> <li>Sending to a specific server:<ul> <li>Instead of broadcasting across a range, add the line <code>SERVER=&lt;private server IP&gt;</code> to send to specifically that node, which would be your login node.</li> </ul> </li> <li>Add an auth key:<ul> <li>Add the line <code>AUTH_KEY=&lt;string&gt;</code>. This means that the compute node will send it's flight hunter packet with this key. This must match the auth key provided to your login node</li> </ul> </li> </ul> An example of all mentioned lines in a single cloud init script.<pre><code>#cloud-config\nwrite_files:\n  - content: |\n      SERVER=10.10.0.1\n      AUTH_KEY=banana\n    path: /opt/flight/cloudinit.in\n    permissions: '0600'\n    owner: root:root\n  - content: |\n      /opt/flight/bin/flight profile prepare openflight-kubernetes-multinode\n    path: /var/lib/firstrun/scripts/00-prepare-profile.bash\n    permissions: '0600'\n    owner: root:root\nusers:\n  - default\n  - name: root\n    ssh_authorized_keys:\n      - &lt;Content of ~/.ssh/id_alcescluster.pub from root user on login node&gt;\n</code></pre> <p>Note</p> <p>The section that writes the <code>/var/lib/firstrun/scripts/00-prepare-profile.bash</code> file sets up the necessary dependencies for Kubernetes automatically when the compute node is launched</p> <p>Info</p> <p>More information on available user data options for Flight Solo via the user data documentation</p>"},{"location":"docs/flight-solo/cluster-build-methods/kubernetes-multinode-aws/#deploy_1","title":"Deploy","text":"AWS MarketplaceAWS Imported <ol> <li> <p>Go to the EC2 instance setup page through marketplace.</p> <ol> <li> <p>Find the Flight Solo image here or by searching the marketplace for \"Flight Solo\".</p> </li> <li> <p>Click \"Continue to Subscribe\"</p> <p></p> </li> <li> <p>Read the terms and conditions, then click \"Continue to Configuration\"</p> <p></p> </li> <li> <p>Configure region, software version (if unsure use the latest), and fulfillment option (if unsure use the default). Then click \"Continue to Launch\". Make sure the region is the same for all nodes to be used in a cluster.</p> <p></p> </li> <li> <p>Click on \"Usage Instructions\" to see some instructions on how to get started, and a link to this documentation.</p> <p></p> </li> <li> <p>Select the \"Launch from EC2\" action</p> <p></p> </li> <li> <p>Click \"Launch\" to go to the EC2 instance setup page.</p> <p></p> </li> </ol> </li> <li> <p>Set the instance name and number of instances.</p> <p></p> </li> <li> <p>Confirm that the region(top right, next to username) is the same as the region the login node was created in.</p> <p></p> </li> <li> <p>In the \"Application and OS Images\" section, confirm that Flight Solo is the selected AMI.</p> <p></p> </li> <li> <p>In the \"Instance type\" section, choose the required instance size.</p> <p></p> </li> <li> <p>In the \"Keypair\" section, select a keypair to use. It is good practice to use the same keypair for the login and compute nodes.</p> <p></p> </li> <li> <p>In the \"Network settings\" section, select the same network, subnet, and security group as the login node.</p> <p></p> <ol> <li> <p>To change the network and subnet, click the \"Edit\" button, and then use the drop downs to find the correct network and subnet.</p> <p></p> </li> </ol> </li> <li> <p>In the \"Configure Storage\" section, allocate as much memory as needed. 8GB is the minimum required for Flight Solo, so it is likely the compute nodes will not need much more than that, as the login node hosts most data.</p> <p></p> </li> <li> <p>In the \"Advanced details\" section there are many settings, but at the bottom is a text box labeled \"User data\".</p> <p></p> <ol> <li> <p>Write a cloud init script in the user data section, see here for details:</p> </li> <li> <p>To get the information necessary for the cloud init script. Go to the EC2 console. Make sure your region is set to the one used for login and compute nodes.</p> </li> <li> <p>Select the created login node to see more details about it, including the private ip.</p> <p></p> </li> <li> <p>Log in to the login node.</p> </li> <li> <p>Become the root user and open the file <code>~/.ssh/id_alcescluster.pub</code>, copy the contents to the cloud init script.</p> <p>Tip</p> <p>If the login node is launched using the <code>SHAREPUBKEY</code> then there is no need to perform steps <code>d</code> and <code>e</code> as this will be performed by the systems.</p> </li> </ol> </li> <li> <p>Back on the compute node creation page, click \"Launch Instance\".</p> </li> </ol> <p>Note</p> <p>Repeat this process for any other types of nodes that need to be added to the cluster.</p> <ol> <li> <p>Go the EC2 instance console</p> <p></p> <ol> <li> <p>Click \"Launch Instance\" to go to the EC2 instance setup page.</p> <p></p> </li> </ol> </li> <li> <p>Set the instance name and number of instances.</p> <p></p> </li> <li> <p>Confirm that the region(top right, next to username) is the same as the region the login node was created in.</p> <p></p> </li> <li> <p>In the \"Application and OS Images\" section choose the \"My AMIs\" tab and select your imported solo AMI.</p> <p></p> </li> <li> <p>In the \"Instance type\" section, choose the required instance size.</p> <p></p> </li> <li> <p>In the \"Keypair\" section, select a keypair to use. It is good practice to use the same keypair for the login and compute nodes.</p> <p></p> </li> <li> <p>In the \"Network settings\" section, select the same network, subnet, and security group as the login node.</p> <p></p> <ol> <li> <p>To change the network and subnet, click the \"Edit\" button, and then use the drop downs to find the correct network and subnet.</p> <p></p> </li> </ol> </li> <li> <p>In the \"Configure Storage\" section, allocate as much memory as needed. 8GB is the minimum required for Flight Solo, so it is likely the compute nodes will not need much more than that, as the login node hosts most data.</p> <p></p> </li> <li> <p>In the \"Advanced details\" section there are many settings, but at the bottom is a text box labeled \"User data\".</p> <p></p> <ol> <li> <p>Write a cloud init script in the user data section, see here for details:</p> </li> <li> <p>To get the information necessary for the cloud init script. Go to the EC2 console.</p> </li> <li> <p>Select the created login node to see more details about it, including the private ip.</p> <p></p> </li> <li> <p>Log in to the login node.</p> </li> <li> <p>Become the root user and open the file <code>~/.ssh/id_alcescluster.pub</code>, copy the contents to the cloud init script.</p> <p>Tip</p> <p>If the login node is launched using the <code>SHAREPUBKEY</code> then there is no need to perform steps <code>d</code> and <code>e</code> as this will be performed by the systems.</p> </li> </ol> </li> <li> <p>Back on the compute node creation page, click \"Launch Instance\".</p> </li> </ol> <p>Note</p> <p>Repeat this process for any other types of nodes that need to be added to the cluster.</p>"},{"location":"docs/flight-solo/cluster-build-methods/kubernetes-multinode-aws/#general-configuration","title":"General Configuration","text":""},{"location":"docs/flight-solo/cluster-build-methods/kubernetes-multinode-aws/#create-node-inventory","title":"Create Node Inventory","text":"<ol> <li> <p>Parse your node(s) with the command <code>flight hunter parse</code>.</p> <ol> <li> <p>This will display a list of hunted nodes, for example     <pre><code>[flight@login-node.novalocal ~]$ flight hunter parse\nSelect nodes: (Scroll for more nodes)\n\u2023 \u2b21 login-node.novalocal - 10.10.0.1\n  \u2b21 compute-node-1.novalocal - 10.10.101.1\n</code></pre></p> </li> <li> <p>Select the desired node to be parsed with Space, and you will be taken to the label editor     <pre><code>Choose label: login-node.novalocal\n</code></pre></p> </li> <li> <p>Here, you can edit the label like plain text     <pre><code>Choose label: login1\n</code></pre></p> <p>Tip</p> <p>You can clear the current node name by pressing Down in the label editor.</p> </li> <li> <p>When done editing, press Enter to save. The modified node label will appear next to the ip address and original node label.     <pre><code>Select nodes: login-node.novalocal - 10.10.0.1 (login1) (Scroll for more nodes)\n\u2023 \u2b22 login-node.novalocal - 10.10.0.1 (login1)\n  \u2b21 compute-node-1.novalocal - 10.10.101.1\n</code></pre></p> </li> <li> <p>From this point, you can either hit Enter to finish parsing and process the selected nodes, or continue changing nodes. Either way, you can return to this list by running <code>flight hunter parse</code>.</p> </li> <li> <p>Save the node inventory before moving on to the next step.</p> <p>Tip</p> <p>See <code>flight hunter parse -h</code> for more ways to parse nodes.</p> </li> </ol> </li> </ol>"},{"location":"docs/flight-solo/cluster-build-methods/kubernetes-multinode-aws/#add-genders","title":"Add genders","text":"<ol> <li>Optionally, you may add genders to the newly parsed node. For example, in the case that the node should have the gender <code>cluster</code> and <code>all</code> then run the command:     <pre><code>flight hunter modify-groups --add cluster,all login1\n</code></pre></li> </ol>"},{"location":"docs/flight-solo/cluster-build-methods/kubernetes-multinode-aws/#kubernetes-multinode-configuration","title":"Kubernetes Multinode Configuration","text":"<ol> <li> <p>Configure profile</p> <p><pre><code>flight profile configure\n</code></pre> 1. This brings up a UI, where several options need to be set. Use up and down arrow keys to scroll through options and enter to move to the next option. Options in brackets coloured yellow are the default options that will be applied if nothing is entered.     - Cluster type: The type of cluster setup needed, in this case <code>Openflight Kubernetes Multinode</code>.     - Cluster name: The name of the cluster.     - Default user: The user that you log in with.     - IP range of compute nodes: The IP range of the compute nodes used, remember to add the netmask. E.g. <code>172.31.16.0/20</code>     - IP range of Kubernetes pods: The IP range that the kubernetes pods should use, make sure this is different from the IP range of the compute nodes, and remember to add the net mask. E.g. <code>192.168.0.0/16</code></p> </li> <li> <p>Apply identities by running the command <code>flight profile apply</code></p> <ol> <li>First apply an identity to the login node     <pre><code>flight profile apply login1 master\n</code></pre></li> <li> <p>Wait for the login node identity to finish applying. You can check the status of all nodes with <code>flight profile list</code>.</p> <p>Tip</p> <p>You can watch the progress of the application with <code>flight profile view login1 --watch</code></p> </li> <li> <p>Apply an identity to the each of the compute nodes.  E.g.     <pre><code>flight profile apply node01,node02 worker\n</code></pre></p> <p>Tip</p> <p>You can check all available identities for the current profile with <code>flight profile identities</code></p> </li> </ol> </li> </ol> <p>Success</p> <p>Congratulations, you've now created a Kubernetes Multinode environment! Learn more about Kubernetes in their documentation.</p>"},{"location":"docs/flight-solo/cluster-build-methods/kubernetes-multinode-aws/#verifying-functionality","title":"Verifying Functionality","text":""},{"location":"docs/flight-solo/cluster-build-methods/kubernetes-multinode-aws/#check-nodes-runningready","title":"Check Nodes Running/Ready","text":"<ol> <li>As the <code>default_username</code> (unless this was changed, it will be <code>flight</code>) check nodes are \"Ready\"     <pre><code>kubectl get nodes\n</code></pre></li> </ol>"},{"location":"docs/flight-solo/cluster-build-methods/kubernetes-multinode-aws/#launching-a-pod","title":"Launching a \"Pod\"","text":"<ol> <li> <p>Get test yaml file for the VM <pre><code>flight silo file pull openflight:kubernetes/pod-launch-test.yaml\n</code></pre></p> </li> <li> <p>Launch a pod (this will create an ubuntu VM that sleeps for 10 minutes then exits)     <pre><code>kubectl apply -f pod-launch-test.yaml\n</code></pre></p> </li> <li> <p>Check that the pod is running     <pre><code>kubectl get pods -o wide\n</code></pre></p> </li> <li> <p>The pod should be running without issues.</p> </li> </ol>"},{"location":"docs/flight-solo/cluster-build-methods/kubernetes-multinode-aws/#perform-network-test","title":"Perform Network Test","text":"<ol> <li>Create yaml file for a <code>php-apache</code> service     <pre><code>flight silo file pull openflight:kubernetes/php-apache.yaml\n</code></pre></li> <li>Launch pod service     <pre><code>kubectl apply -f php-apache.yaml\n</code></pre></li> <li>Get yaml file for VM to verify connection from     <pre><code>flight silo file pull openflight:kubernetes/busybox-wget.yaml\n</code></pre></li> <li>Launch pod     <pre><code>kubectl apply -f busybox-wget.yaml\n</code></pre></li> <li>View output of <code>wget</code> pod (this should show <code>OK!</code>)     <pre><code>kubectl logs busybox-wget\n</code></pre></li> </ol>"},{"location":"docs/flight-solo/cluster-build-methods/kubernetes-multinode-azure/","title":"Kubernetes Multinode on Azure","text":""},{"location":"docs/flight-solo/cluster-build-methods/kubernetes-multinode-azure/#launch-login-node","title":"Launch Login Node","text":""},{"location":"docs/flight-solo/cluster-build-methods/kubernetes-multinode-azure/#prepare-user-data","title":"Prepare User Data","text":"<p>When launching a login node it is worth considering what user data options to provide. While it is not required, user data can provide powerful customisation at launch that can further streamline the cluster build process.</p> <p>There are several options that can be added to change how a compute node will contact nodes on startup.</p> <ul> <li>Sharing public ssh key to clients: <ul> <li>Instead of manually obtaining and sharing the root public SSH key (passwordless root ssh is required for flight profile) this can be shared over the local network with <code>SHAREPUBKEY=true</code></li> </ul> </li> <li>Add an auth key:<ul> <li>Add the line <code>AUTH_KEY=&lt;string&gt;</code>. This means that the node will only accept incoming flight hunter nodes that provide a matching authorisation key</li> </ul> </li> </ul> An example of all mentioned lines in a single cloud init script.<pre><code>#cloud-config\nwrite_files:\n  - content: |\n      SHAREPUBKEY=true\n      AUTH_KEY=banana\n    path: /opt/flight/cloudinit.in\n    permissions: '0600'\n    owner: root:root\n  - content: |\n      /opt/flight/bin/flight profile prepare openflight-kubernetes-multinode\n    path: /var/lib/firstrun/scripts/00-prepare-profile.bash\n    permissions: '0600'\n    owner: root:root\n</code></pre> <p>Note</p> <p>The section that writes the <code>/var/lib/firstrun/scripts/00-prepare-profile.bash</code> file sets up the necessary dependencies for Kubernetes automatically when the login node is launched</p> <p>Info</p> <p>More information on available user data options for Flight Solo via the user data documentation</p>"},{"location":"docs/flight-solo/cluster-build-methods/kubernetes-multinode-azure/#deploy","title":"Deploy","text":"<p>To set up a cluster, you will need to import a Flight Solo image.</p> <ol> <li> <p>Go to the Microsoft Azure portal.</p> <p></p> </li> <li> <p>Go to Virtual Machines, and click \"Create\".</p> <p></p> </li> <li> <p>Select \"Azure virtual machine\", which will load this page:</p> <p></p> </li> <li> <p>On the Basics page:</p> <ol> <li>Set Subscription to your subscription type.</li> <li>Set Resource Group to your desired resource group (where the vm will be kept after creation).</li> <li>Set Virtual machine name to any suitable name. (<code>-</code> does not work in a name)</li> <li>Set Image to the imported Flight Solo Image.<ol> <li>It may be necessary to open the drop-down and/or see all images in order to find the imported image.     </li> <li>Scroll down to see more options     </li> </ol> </li> <li>Set Size to your choice of size.</li> <li>Set Authentication type to <code>SSH public key</code></li> <li>Set Username to any suitable username.</li> <li>Set SSH public key source to the most suitable option, but remember what key was used if creating compute nodes later.</li> <li>Fill in the Key pair name/Stored key/Use existing key as appropriate to the chosen public key source.</li> <li>Allow traffic to selected ports, and select <code>SSH(22)</code>, <code>HTTP(80)</code> and <code>HTTPS(443)</code> as the allowed ports.</li> <li>Set the most appropriate license type.</li> </ol> </li> <li> <p>Continuing on to the next page, Disks, all necessary details should already be filled out, so this page can be skipped (unless you know what you want to change). However, it is recommended to select Delete with VM.</p> <p></p> </li> <li> <p>Go on to the networking tab and fill out the necessary options.</p> <p></p> <ol> <li>Set Virtual Network or create a new one by pressing \"Create new\" and setting a name. Remember what this is for if you create compute nodes.</li> <li>Set Subnet to one of the options in the drop-down menu, if it isn't already set. Remember what this is for if you create compute nodes.</li> <li>Set Public IP to an existing public IP or create a new one by pressing \"Create new\" and setting a name.</li> <li>Set NIC network security group to \"Advanced\", and press \"Create new\" to create a new security group.     <ol> <li>Click on \"Add an inbound rule\" to open the inbound rule creator     </li> <li>Create rules to allow <code>HTTP</code>, <code>HTTPS</code> and <code>SSH</code> traffic from your IP address to the security group.</li> <li>When complete, press \"OK\" at the bottom left of the screen to return to image creation.</li> </ol> </li> </ol> </li> <li> <p>The Management, Monitoring and Tags tabs have more options that aren't necessary for setup. Skip to the tab Advanced</p> </li> <li> <p>In the Custom data and cloud init section, there is a text box. This is where your user data can be specified</p> <p></p> </li> <li> <p>Azure will take some time to review your settings. If there are no issues click \"Create\" to finish creation.</p> </li> </ol>"},{"location":"docs/flight-solo/cluster-build-methods/kubernetes-multinode-azure/#launch-compute-nodes","title":"Launch Compute Nodes","text":""},{"location":"docs/flight-solo/cluster-build-methods/kubernetes-multinode-azure/#prepare-user-data_1","title":"Prepare User Data","text":"<p>Setting up compute nodes is done slightly differently than a login node. The basic steps are the same except subnets, networks and security groups need to match the ones used for the login node.</p> <p>This is the smallest amount of cloud init data necessary. It allows the login node to find the compute nodes as long as they are on the same network, and ssh into them from the root user (which is necessary for setup). <pre><code>#cloud-config\nusers:\n  - default\n  - name: root\n    ssh_authorized_keys:\n      - &lt;Content of ~/.ssh/id_alcescluster.pub from root user on login node&gt;\n</code></pre></p> <p>Tip</p> <p>The above is not required if the <code>SHAREPUBKEY</code> option was provided to the login node. If this was the case then the <code>SERVER</code> option provided to the compute node will be enough to enable root access from the login node.</p> <p>There are several options that can be added to change how a compute node will contact nodes on startup.</p> <ul> <li>Sending to a specific server:<ul> <li>Instead of broadcasting across a range, add the line <code>SERVER=&lt;private server IP&gt;</code> to send to specifically that node, which would be your login node.</li> </ul> </li> <li>Add an auth key:<ul> <li>Add the line <code>AUTH_KEY=&lt;string&gt;</code>. This means that the compute node will send it's flight hunter packet with this key. This must match the auth key provided to your login node</li> </ul> </li> </ul> An example of all mentioned lines in a single cloud init script.<pre><code>#cloud-config\nwrite_files:\n  - content: |\n      SERVER=10.10.0.1\n      AUTH_KEY=banana\n    path: /opt/flight/cloudinit.in\n    permissions: '0600'\n    owner: root:root\n  - content: |\n      /opt/flight/bin/flight profile prepare openflight-kubernetes-multinode\n    path: /var/lib/firstrun/scripts/00-prepare-profile.bash\n    permissions: '0600'\n    owner: root:root\nusers:\n  - default\n  - name: root\n    ssh_authorized_keys:\n      - &lt;Content of ~/.ssh/id_alcescluster.pub from root user on login node&gt;\n</code></pre> <p>Note</p> <p>The section that writes the <code>/var/lib/firstrun/scripts/00-prepare-profile.bash</code> file sets up the necessary dependencies for Kubernetes automatically when the compute node is launched</p> <p>Info</p> <p>More information on available user data options for Flight Solo via the user data documentation</p>"},{"location":"docs/flight-solo/cluster-build-methods/kubernetes-multinode-azure/#deploy_1","title":"Deploy","text":"<ol> <li> <p>Go to the Microsoft Azure portal.</p> <p></p> </li> <li> <p>Go to Virtual Machines, and click \"Create\".</p> <p></p> </li> <li> <p>Select \"Azure virtual machine\", which will load this page:</p> <p></p> </li> <li> <p>On the Basics page:</p> <ol> <li>Set Subscription to your subscription type.</li> <li>Set Resource Group to the same as the login node </li> <li>Set Virtual machine name to any suitable name.</li> <li>Set Image to the imported Flight Solo Image.<ol> <li>It may be necessary to open the drop-down and/or see all images in order to find the imported image.     </li> <li>Scroll down to see more options     </li> </ol> </li> <li>Set Size to your choice of size.</li> <li>Set Authentication type to <code>SSH public key</code></li> <li>Set Username to the same username as with the login node.</li> <li>Set SSH public key source to the same key that was used for the login node.</li> <li>Fill in the Key pair name/Stored key/Use existing key as appropriate to the chosen public key source.</li> <li>Allow traffic to selected ports, and select <code>SSH(22</code>, <code>HTTP(80)</code> and <code>HTTPS(443)</code> as the allowed ports.</li> <li>Set the most appropriate license type.</li> </ol> </li> <li> <p>Continuing on to the next page, Disks, all necessary details should already be filled out, so this page can be skipped (unless you know what you want to change). However, it is recommended to select Delete with VM.</p> <p></p> </li> <li> <p>Go on to the networking tab and fill out the necessary options.</p> <ol> <li>Set Virtual Network to the same network that was used for the login node.</li> <li>Set Subnet to the same subnet that was used for the login node.</li> <li>Set NIC network security group to the same subnet that was used for login node.</li> <li>When complete, press \"OK\" at the bottom left of the screen to return to image creation.</li> </ol> </li> <li> <p>The Management and Monitoring tabs have more options that aren't necessary for setup. Skip to the Advanced tab.</p> <p></p> </li> <li> <p>In the Custom data and cloud init section, there is a text box. Write a cloud init script as prepared earlier in the custom data section</p> <p></p> </li> <li> <p>Skip to the Review + Create section. Azure will take some time to review your settings. If there are no issues click \"Create\" to finish creation.</p> </li> </ol>"},{"location":"docs/flight-solo/cluster-build-methods/kubernetes-multinode-azure/#general-configuration","title":"General Configuration","text":""},{"location":"docs/flight-solo/cluster-build-methods/kubernetes-multinode-azure/#create-node-inventory","title":"Create Node Inventory","text":"<ol> <li> <p>Parse your node(s) with the command <code>flight hunter parse</code>.</p> <ol> <li> <p>This will display a list of hunted nodes, for example     <pre><code>[flight@login-node.novalocal ~]$ flight hunter parse\nSelect nodes: (Scroll for more nodes)\n\u2023 \u2b21 login-node.novalocal - 10.10.0.1\n  \u2b21 compute-node-1.novalocal - 10.10.101.1\n</code></pre></p> </li> <li> <p>Select the desired node to be parsed with Space, and you will be taken to the label editor     <pre><code>Choose label: login-node.novalocal\n</code></pre></p> </li> <li> <p>Here, you can edit the label like plain text     <pre><code>Choose label: login1\n</code></pre></p> <p>Tip</p> <p>You can clear the current node name by pressing Down in the label editor.</p> </li> <li> <p>When done editing, press Enter to save. The modified node label will appear next to the ip address and original node label.     <pre><code>Select nodes: login-node.novalocal - 10.10.0.1 (login1) (Scroll for more nodes)\n\u2023 \u2b22 login-node.novalocal - 10.10.0.1 (login1)\n  \u2b21 compute-node-1.novalocal - 10.10.101.1\n</code></pre></p> </li> <li> <p>From this point, you can either hit Enter to finish parsing and process the selected nodes, or continue changing nodes. Either way, you can return to this list by running <code>flight hunter parse</code>.</p> </li> <li> <p>Save the node inventory before moving on to the next step.</p> <p>Tip</p> <p>See <code>flight hunter parse -h</code> for more ways to parse nodes.</p> </li> </ol> </li> </ol>"},{"location":"docs/flight-solo/cluster-build-methods/kubernetes-multinode-azure/#add-genders","title":"Add genders","text":"<ol> <li>Optionally, you may add genders to the newly parsed node. For example, in the case that the node should have the gender <code>cluster</code> and <code>all</code> then run the command:     <pre><code>flight hunter modify-groups --add cluster,all login1\n</code></pre></li> </ol>"},{"location":"docs/flight-solo/cluster-build-methods/kubernetes-multinode-azure/#kubernetes-multinode-configuration","title":"Kubernetes Multinode Configuration","text":"<ol> <li> <p>Configure profile</p> <p><pre><code>flight profile configure\n</code></pre> 1. This brings up a UI, where several options need to be set. Use up and down arrow keys to scroll through options and enter to move to the next option. Options in brackets coloured yellow are the default options that will be applied if nothing is entered.     - Cluster type: The type of cluster setup needed, in this case <code>Openflight Kubernetes Multinode</code>.     - Cluster name: The name of the cluster.     - Default user: The user that you log in with.     - IP range of compute nodes: The IP range of the compute nodes used, remember to add the netmask. E.g. <code>172.31.16.0/20</code>     - IP range of Kubernetes pods: The IP range that the kubernetes pods should use, make sure this is different from the IP range of the compute nodes, and remember to add the net mask. E.g. <code>192.168.0.0/16</code></p> </li> <li> <p>Apply identities by running the command <code>flight profile apply</code></p> <ol> <li>First apply an identity to the login node     <pre><code>flight profile apply login1 master\n</code></pre></li> <li> <p>Wait for the login node identity to finish applying. You can check the status of all nodes with <code>flight profile list</code>.</p> <p>Tip</p> <p>You can watch the progress of the application with <code>flight profile view login1 --watch</code></p> </li> <li> <p>Apply an identity to the each of the compute nodes.  E.g.     <pre><code>flight profile apply node01,node02 worker\n</code></pre></p> <p>Tip</p> <p>You can check all available identities for the current profile with <code>flight profile identities</code></p> </li> </ol> </li> </ol> <p>Success</p> <p>Congratulations, you've now created a Kubernetes Multinode environment! Learn more about Kubernetes in their documentation.</p>"},{"location":"docs/flight-solo/cluster-build-methods/kubernetes-multinode-azure/#verifying-functionality","title":"Verifying Functionality","text":""},{"location":"docs/flight-solo/cluster-build-methods/kubernetes-multinode-azure/#check-nodes-runningready","title":"Check Nodes Running/Ready","text":"<ol> <li>As the <code>default_username</code> (unless this was changed, it will be <code>flight</code>) check nodes are \"Ready\"     <pre><code>kubectl get nodes\n</code></pre></li> </ol>"},{"location":"docs/flight-solo/cluster-build-methods/kubernetes-multinode-azure/#launching-a-pod","title":"Launching a \"Pod\"","text":"<ol> <li> <p>Get test yaml file for the VM <pre><code>flight silo file pull openflight:kubernetes/pod-launch-test.yaml\n</code></pre></p> </li> <li> <p>Launch a pod (this will create an ubuntu VM that sleeps for 10 minutes then exits)     <pre><code>kubectl apply -f pod-launch-test.yaml\n</code></pre></p> </li> <li> <p>Check that the pod is running     <pre><code>kubectl get pods -o wide\n</code></pre></p> </li> <li> <p>The pod should be running without issues.</p> </li> </ol>"},{"location":"docs/flight-solo/cluster-build-methods/kubernetes-multinode-azure/#perform-network-test","title":"Perform Network Test","text":"<ol> <li>Create yaml file for a <code>php-apache</code> service     <pre><code>flight silo file pull openflight:kubernetes/php-apache.yaml\n</code></pre></li> <li>Launch pod service     <pre><code>kubectl apply -f php-apache.yaml\n</code></pre></li> <li>Get yaml file for VM to verify connection from     <pre><code>flight silo file pull openflight:kubernetes/busybox-wget.yaml\n</code></pre></li> <li>Launch pod     <pre><code>kubectl apply -f busybox-wget.yaml\n</code></pre></li> <li>View output of <code>wget</code> pod (this should show <code>OK!</code>)     <pre><code>kubectl logs busybox-wget\n</code></pre></li> </ol>"},{"location":"docs/flight-solo/cluster-build-methods/kubernetes-multinode-openstack/","title":"Kubernetes Multinode on OpenStack","text":""},{"location":"docs/flight-solo/cluster-build-methods/kubernetes-multinode-openstack/#launch-login-node","title":"Launch Login Node","text":""},{"location":"docs/flight-solo/cluster-build-methods/kubernetes-multinode-openstack/#prepare-user-data","title":"Prepare User Data","text":"<p>When launching a login node it is worth considering what user data options to provide. While it is not required, user data can provide powerful customisation at launch that can further streamline the cluster build process.</p> <p>There are several options that can be added to change how a compute node will contact nodes on startup.</p> <ul> <li>Sharing public ssh key to clients: <ul> <li>Instead of manually obtaining and sharing the root public SSH key (passwordless root ssh is required for flight profile) this can be shared over the local network with <code>SHAREPUBKEY=true</code></li> </ul> </li> <li>Add an auth key:<ul> <li>Add the line <code>AUTH_KEY=&lt;string&gt;</code>. This means that the node will only accept incoming flight hunter nodes that provide a matching authorisation key</li> </ul> </li> </ul> An example of all mentioned lines in a single cloud init script.<pre><code>#cloud-config\nwrite_files:\n  - content: |\n      SHAREPUBKEY=true\n      AUTH_KEY=banana\n    path: /opt/flight/cloudinit.in\n    permissions: '0600'\n    owner: root:root\n  - content: |\n      /opt/flight/bin/flight profile prepare openflight-kubernetes-multinode\n    path: /var/lib/firstrun/scripts/00-prepare-profile.bash\n    permissions: '0600'\n    owner: root:root\n</code></pre> <p>Note</p> <p>The section that writes the <code>/var/lib/firstrun/scripts/00-prepare-profile.bash</code> file sets up the necessary dependencies for Kubernetes automatically when the login node is launched</p> <p>Info</p> <p>More information on available user data options for Flight Solo via the user data documentation</p>"},{"location":"docs/flight-solo/cluster-build-methods/kubernetes-multinode-openstack/#deploy","title":"Deploy","text":"<p>To set up a cluster, you will need to import a Flight Solo image.</p> <p>Before setting up a cluster on Openstack, there are several required prerequisites:</p> <ul> <li>Your own keypair</li> <li>A network</li> <li>A router<ul> <li>With an interface both on the External Gateway network and an Internal Interface on the previously created network</li> </ul> </li> <li>A security group that allows traffic is given below (if creating the security group through the web interface then the \"Any\" protocol will need to be an \"Other Protocol\" rule with \"IP Protocol\" of <code>-1</code>)</li> </ul> Protocol Direction CIDR Port Range Any egress 0.0.0.0/0 any Any ingress Virtual Network CIDR any ICMP ingress 0.0.0.0/0 any SSH ingress 0.0.0.0/0 22 TCP ingress 0.0.0.0/0 80 TCP ingress 0.0.0.0/0 443 TCP ingress 0.0.0.0/0 5900-5903 <p>Note</p> <p>The \"Virtual Network CIDR\" is the subnet and netmask for the network that the nodes are using. For example, a node on the 11.11.11.0 network with a netmask of 255.255.255.0 would have a network CIDR of 11.11.11.0/24.</p> <p>The documentation includes instructions for importing an image to Openstack, and guides for setting up the other prerequisites can be found in the Openstack documentation</p> <p>To set up a cluster:</p> <ol> <li> <p>Go to the Openstack instances page.</p> <p></p> </li> <li> <p>Click \"Launch Instance\", and the instance creation window will pop up.</p> </li> <li> <p>Fill in the instance name, and leave the number of instances as 1, then click next.</p> <p></p> </li> <li> <p>Choose the desired image to use by clicking the up arrow at the end of its row. It will be displayed in the \"Allocated\" section when selected.</p> <p></p> </li> <li> <p>Choose the desired instance size by clicking the up arrow at the end of its row. It will be displayed in the \"Allocated\" section when selected.</p> <p></p> </li> <li> <p>Choose a network in the same way as an image or instance size. Note that all nodes in a cluster must be on the same network.</p> <p></p> </li> <li> <p>Choose a security group in the same way as an image or instance size. Note that all nodes in a cluster must be in the same security group.</p> <p></p> </li> <li> <p>Choose the keypair in the same way as an image or instance size.</p> <p></p> </li> <li> <p>In the \"Configuration\" section, there is a \"Customisation Script\" section with a text box. This will be used to set your user data</p> <p></p> </li> <li> <p>When all options have been selected, press the \"Launch Instance\" button to launch. If the button is greyed out, then a mandatory setting has not been configured.</p> <p></p> </li> <li> <p>Go to the \"Instances\" page in the \"Compute\" section. The created node should be there and be finishing or have finished creation.</p> <p></p> </li> <li> <p>Click on the down arrow at the end of the instance row. This will bring up a drop-down menu.</p> </li> <li> <p>Select \"Associate Floating IP\", this will make the ip management window pop up.</p> <p></p> </li> <li> <p>Associate a floating IP, either by using an existing one or allocating a new one.</p> <ol> <li> <p>To use an existing floating IP:</p> <ol> <li> <p>Open the IP Address drop-down menu.</p> <p></p> </li> <li> <p>Select one of the IP Addresses.</p> </li> <li> <p>Click \"Associate\" to finish associating an IP.</p> </li> </ol> </li> <li> <p>To allocate a new floating IP:</p> <ol> <li> <p>Click the \"+\" next to the drop-down arrow to open the allocation menu.</p> <p></p> </li> <li> <p>Click \"Allocate IP\".</p> </li> </ol> </li> </ol> </li> <li> <p>Click \"Associate\" to finish associating an IP.</p> </li> </ol>"},{"location":"docs/flight-solo/cluster-build-methods/kubernetes-multinode-openstack/#launch-compute-nodes","title":"Launch Compute Nodes","text":""},{"location":"docs/flight-solo/cluster-build-methods/kubernetes-multinode-openstack/#prepare-user-data_1","title":"Prepare User Data","text":"<p>Setting up compute nodes is done slightly differently than a login node. The basic steps are the same except subnets, networks and security groups need to match the ones used for the login node.</p> <p>This is the smallest amount of cloud init data necessary. It allows the login node to find the compute nodes as long as they are on the same network, and ssh into them from the root user (which is necessary for setup). <pre><code>#cloud-config\nusers:\n  - default\n  - name: root\n    ssh_authorized_keys:\n      - &lt;Content of ~/.ssh/id_alcescluster.pub from root user on login node&gt;\n</code></pre></p> <p>Tip</p> <p>The above is not required if the <code>SHAREPUBKEY</code> option was provided to the login node. If this was the case then the <code>SERVER</code> option provided to the compute node will be enough to enable root access from the login node.</p> <p>There are several options that can be added to change how a compute node will contact nodes on startup.</p> <ul> <li>Sending to a specific server:<ul> <li>Instead of broadcasting across a range, add the line <code>SERVER=&lt;private server IP&gt;</code> to send to specifically that node, which would be your login node.</li> </ul> </li> <li>Add an auth key:<ul> <li>Add the line <code>AUTH_KEY=&lt;string&gt;</code>. This means that the compute node will send it's flight hunter packet with this key. This must match the auth key provided to your login node</li> </ul> </li> </ul> An example of all mentioned lines in a single cloud init script.<pre><code>#cloud-config\nwrite_files:\n  - content: |\n      SERVER=10.10.0.1\n      AUTH_KEY=banana\n    path: /opt/flight/cloudinit.in\n    permissions: '0600'\n    owner: root:root\n  - content: |\n      /opt/flight/bin/flight profile prepare openflight-kubernetes-multinode\n    path: /var/lib/firstrun/scripts/00-prepare-profile.bash\n    permissions: '0600'\n    owner: root:root\nusers:\n  - default\n  - name: root\n    ssh_authorized_keys:\n      - &lt;Content of ~/.ssh/id_alcescluster.pub from root user on login node&gt;\n</code></pre> <p>Note</p> <p>The section that writes the <code>/var/lib/firstrun/scripts/00-prepare-profile.bash</code> file sets up the necessary dependencies for Kubernetes automatically when the compute node is launched</p> <p>Info</p> <p>More information on available user data options for Flight Solo via the user data documentation</p>"},{"location":"docs/flight-solo/cluster-build-methods/kubernetes-multinode-openstack/#deploy_1","title":"Deploy","text":"<ol> <li> <p>Go to the Openstack instances page.</p> <p></p> </li> <li> <p>Click \"Launch Instance\", and the instance creation window will pop up.</p> </li> <li> <p>Fill in the instance name, and set the number of instances to create, then click next.</p> <p></p> </li> <li> <p>Choose the desired image to use by clicking the up arrow at the end of its row. It will be displayed in the \"Allocated\" section when selected.</p> <p></p> </li> <li> <p>Choose the desired instance size by clicking the up arrow at the end of its row. It will be displayed in the \"Allocated\" section when selected.</p> <p></p> </li> <li> <p>Choose a network in the same way as an image or instance size. Note that this should be the same network as the login node.</p> <p></p> </li> <li> <p>Choose a security group in the same way as an image or instance size. Note that this should be the same security group as the login node.</p> <p></p> </li> <li> <p>Choose the keypair in the same way as an image or instance size.</p> <p></p> </li> <li> <p>In the \"Configuration\" section, there is a \"Customisation Script\" section with a text box. This will be used to set user data script prepared earlier</p> <p></p> </li> <li> <p>When all options have been selected, press the \"Launch Instance\" button to launch. If the button is greyed out, then a mandatory setting has not been configured.</p> <p></p> </li> </ol>"},{"location":"docs/flight-solo/cluster-build-methods/kubernetes-multinode-openstack/#general-configuration","title":"General Configuration","text":""},{"location":"docs/flight-solo/cluster-build-methods/kubernetes-multinode-openstack/#create-node-inventory","title":"Create Node Inventory","text":"<ol> <li> <p>Parse your node(s) with the command <code>flight hunter parse</code>.</p> <ol> <li> <p>This will display a list of hunted nodes, for example     <pre><code>[flight@login-node.novalocal ~]$ flight hunter parse\nSelect nodes: (Scroll for more nodes)\n\u2023 \u2b21 login-node.novalocal - 10.10.0.1\n  \u2b21 compute-node-1.novalocal - 10.10.101.1\n</code></pre></p> </li> <li> <p>Select the desired node to be parsed with Space, and you will be taken to the label editor     <pre><code>Choose label: login-node.novalocal\n</code></pre></p> </li> <li> <p>Here, you can edit the label like plain text     <pre><code>Choose label: login1\n</code></pre></p> <p>Tip</p> <p>You can clear the current node name by pressing Down in the label editor.</p> </li> <li> <p>When done editing, press Enter to save. The modified node label will appear next to the ip address and original node label.     <pre><code>Select nodes: login-node.novalocal - 10.10.0.1 (login1) (Scroll for more nodes)\n\u2023 \u2b22 login-node.novalocal - 10.10.0.1 (login1)\n  \u2b21 compute-node-1.novalocal - 10.10.101.1\n</code></pre></p> </li> <li> <p>From this point, you can either hit Enter to finish parsing and process the selected nodes, or continue changing nodes. Either way, you can return to this list by running <code>flight hunter parse</code>.</p> </li> <li> <p>Save the node inventory before moving on to the next step.</p> <p>Tip</p> <p>See <code>flight hunter parse -h</code> for more ways to parse nodes.</p> </li> </ol> </li> </ol>"},{"location":"docs/flight-solo/cluster-build-methods/kubernetes-multinode-openstack/#add-genders","title":"Add genders","text":"<ol> <li>Optionally, you may add genders to the newly parsed node. For example, in the case that the node should have the gender <code>cluster</code> and <code>all</code> then run the command:     <pre><code>flight hunter modify-groups --add cluster,all login1\n</code></pre></li> </ol>"},{"location":"docs/flight-solo/cluster-build-methods/kubernetes-multinode-openstack/#kubernetes-multinode-configuration","title":"Kubernetes Multinode Configuration","text":"<ol> <li> <p>Configure profile</p> <p><pre><code>flight profile configure\n</code></pre> 1. This brings up a UI, where several options need to be set. Use up and down arrow keys to scroll through options and enter to move to the next option. Options in brackets coloured yellow are the default options that will be applied if nothing is entered.     - Cluster type: The type of cluster setup needed, in this case <code>Openflight Kubernetes Multinode</code>.     - Cluster name: The name of the cluster.     - Default user: The user that you log in with.     - IP range of compute nodes: The IP range of the compute nodes used, remember to add the netmask. E.g. <code>172.31.16.0/20</code>     - IP range of Kubernetes pods: The IP range that the kubernetes pods should use, make sure this is different from the IP range of the compute nodes, and remember to add the net mask. E.g. <code>192.168.0.0/16</code></p> </li> <li> <p>Apply identities by running the command <code>flight profile apply</code></p> <ol> <li>First apply an identity to the login node     <pre><code>flight profile apply login1 master\n</code></pre></li> <li> <p>Wait for the login node identity to finish applying. You can check the status of all nodes with <code>flight profile list</code>.</p> <p>Tip</p> <p>You can watch the progress of the application with <code>flight profile view login1 --watch</code></p> </li> <li> <p>Apply an identity to the each of the compute nodes.  E.g.     <pre><code>flight profile apply node01,node02 worker\n</code></pre></p> <p>Tip</p> <p>You can check all available identities for the current profile with <code>flight profile identities</code></p> </li> </ol> </li> </ol> <p>Success</p> <p>Congratulations, you've now created a Kubernetes Multinode environment! Learn more about Kubernetes in their documentation.</p>"},{"location":"docs/flight-solo/cluster-build-methods/kubernetes-multinode-openstack/#verifying-functionality","title":"Verifying Functionality","text":""},{"location":"docs/flight-solo/cluster-build-methods/kubernetes-multinode-openstack/#check-nodes-runningready","title":"Check Nodes Running/Ready","text":"<ol> <li>As the <code>default_username</code> (unless this was changed, it will be <code>flight</code>) check nodes are \"Ready\"     <pre><code>kubectl get nodes\n</code></pre></li> </ol>"},{"location":"docs/flight-solo/cluster-build-methods/kubernetes-multinode-openstack/#launching-a-pod","title":"Launching a \"Pod\"","text":"<ol> <li> <p>Get test yaml file for the VM <pre><code>flight silo file pull openflight:kubernetes/pod-launch-test.yaml\n</code></pre></p> </li> <li> <p>Launch a pod (this will create an ubuntu VM that sleeps for 10 minutes then exits)     <pre><code>kubectl apply -f pod-launch-test.yaml\n</code></pre></p> </li> <li> <p>Check that the pod is running     <pre><code>kubectl get pods -o wide\n</code></pre></p> </li> <li> <p>The pod should be running without issues.</p> </li> </ol>"},{"location":"docs/flight-solo/cluster-build-methods/kubernetes-multinode-openstack/#perform-network-test","title":"Perform Network Test","text":"<ol> <li>Create yaml file for a <code>php-apache</code> service     <pre><code>flight silo file pull openflight:kubernetes/php-apache.yaml\n</code></pre></li> <li>Launch pod service     <pre><code>kubectl apply -f php-apache.yaml\n</code></pre></li> <li>Get yaml file for VM to verify connection from     <pre><code>flight silo file pull openflight:kubernetes/busybox-wget.yaml\n</code></pre></li> <li>Launch pod     <pre><code>kubectl apply -f busybox-wget.yaml\n</code></pre></li> <li>View output of <code>wget</code> pod (this should show <code>OK!</code>)     <pre><code>kubectl logs busybox-wget\n</code></pre></li> </ol>"},{"location":"docs/flight-solo/cluster-build-methods/modular/","title":"Modular Cluster Workflow","text":"<p>Use the tabs below to assist in selection of the correct cluster build document.</p> Which cluster type? SLURMKubernetesJupyter Lab Which cluster size? StandaloneMultinode Which platform? AWS Azure OpenStack  Alces Cloud <p>Build SLURM Standalone on AWS</p> <p>Build SLURM Standalone on Azure</p> <p>Build SLURM Standalone on OpenStack</p> <p>Build SLURM Standalone on Alces Cloud</p> Which platform? AWS Azure OpenStack  Alces Cloud <p>Build SLURM Multinode on AWS</p> <p>Build SLURM Multinode on Azure</p> <p>Build SLURM Multinode on OpenStack</p> <p>Build SLURM Multinode on Alces Cloud</p> Which cluster size? Multinode Which platform? AWS Azure OpenStack  Alces Cloud <p>Build Kubernetes Multinode on AWS</p> <p>Build Kubernetes Multinode on Azure</p> <p>Build Kubernetes Multinode on OpenStack</p> <p>Build Kubernetes Multinode on Alces Cloud</p> Which cluster size? Standalone Which platform? AWS Azure OpenStack  Alces Cloud <p>Build Jupyter Lab Standalone on AWS</p> <p>Build Jupyter Lab Standalone on Azure</p> <p>Build Jupyter Lab Standalone on OpenStack</p> <p>Build Jupyter Lab Standalone on Alces Cloud</p>"},{"location":"docs/flight-solo/cluster-build-methods/slurm-multinode-alces-cloud/","title":"SLURM Multinode on Alces Cloud","text":""},{"location":"docs/flight-solo/cluster-build-methods/slurm-multinode-alces-cloud/#launch-login-node","title":"Launch Login Node","text":""},{"location":"docs/flight-solo/cluster-build-methods/slurm-multinode-alces-cloud/#prepare-user-data","title":"Prepare User Data","text":"<p>When launching a login node it is worth considering what user data options to provide. While it is not required, user data can provide powerful customisation at launch that can further streamline the cluster build process.</p> <p>There are several options that can be added to change how a compute node will contact nodes on startup.</p> <ul> <li>Sharing public ssh key to clients: <ul> <li>Instead of manually obtaining and sharing the root public SSH key (passwordless root ssh is required for flight profile) this can be shared over the local network with <code>SHAREPUBKEY=true</code></li> </ul> </li> <li>Add an auth key:<ul> <li>Add the line <code>AUTH_KEY=&lt;string&gt;</code>. This means that the node will only accept incoming flight hunter nodes that provide a matching authorisation key</li> </ul> </li> </ul> An example of all mentioned lines in a single cloud init script.<pre><code>#cloud-config\nwrite_files:\n  - content: |\n      SHAREPUBKEY=true\n      AUTH_KEY=banana\n    path: /opt/flight/cloudinit.in\n    permissions: '0600'\n    owner: root:root\n</code></pre> <p>Info</p> <p>More information on available user data options for Flight Solo via the user data documentation</p>"},{"location":"docs/flight-solo/cluster-build-methods/slurm-multinode-alces-cloud/#deploy","title":"Deploy","text":"CLIGUI <p>To set up a cluster, you will need to import a Flight Solo image.</p> <p>Before setting up a cluster on Alces Cloud, there are several required prerequisites:</p> <ul> <li>Your own keypair</li> <li>A network with a subnet and a router bridging the subnet to the external network</li> <li>A security group that allows traffic is given below (if creating the security group through the web interface then the \"Any\" protocol will need to be an \"Other Protocol\" rule with \"IP Protocol\" of <code>-1</code>)</li> </ul> Protocol Direction CIDR Port Range Any egress 0.0.0.0/0 any Any ingress Virtual Network CIDR any ICMP ingress 0.0.0.0/0 any SSH ingress 0.0.0.0/0 22 TCP ingress 0.0.0.0/0 80 TCP ingress 0.0.0.0/0 443 TCP ingress 0.0.0.0/0 5900-5903 <p>Note</p> <p>The \"Virtual Network CIDR\" is the subnet and netmask for the network that the nodes are using. For example, a node on the 11.11.11.0 network with a netmask of 255.255.255.0 would have a network CIDR of 11.11.11.0/24.</p> <ol> <li> <p>Launch a login node with a command similar to the following: </p> <pre><code>$ openstack server create --flavor p1.small \\\n                          --image \"Flight Solo VERSION\" \\\n                          --boot-from-volume 16 \\\n                          --network \"mycluster1-network\" \\\n                          --key-name \"MyKey\" \\\n                          --security-group \"mycluster1-sg\" \\\n                          --user-data myuserdata.yml \\\n                          login1\n</code></pre> <ul> <li> <p>Where:</p> <ul> <li><code>flavor</code> - Is the desired size of the instance</li> <li><code>image</code> - Is the Flight Solo image imported to Alces Cloud</li> <li><code>boot-from-volume</code> - Is the size of the system disk in GB</li> <li><code>network</code> - Is the name or ID of the network created for the cluster</li> <li><code>key-name</code> - Is the name of the SSH key to use</li> <li><code>security-group</code> - Is the name or ID of the security group created previously</li> <li><code>user-data</code> - Is the file containing cloud-init user-data (this is optional in standalone scenarios)</li> <li><code>login1</code> - Is the name of the system</li> </ul> </li> </ul> </li> <li> <p>Associate a floating IP, either by using an existing one or creating a new one</p> <ol> <li> <p>To use an existing floating IP</p> <ol> <li> <p>Identify the IP address of an available floating IP (<code>Port</code> will be <code>None</code>)</p> <pre><code>$ openstack floating ip list\n+---------------------+---------------------+------------------+---------------------+---------------------+-----------------------+\n| ID                  | Floating IP Address | Fixed IP Address | Port                | Floating Network    | Project               |\n+---------------------+---------------------+------------------+---------------------+---------------------+-----------------------+\n| 726318f4-4dbb-4d51- | 10.199.31.6         | None             | None                | c681d94b-e2ec-4b73- | dcd92da7538a4f64a42b0 |\n| b119-d9e53c47a9f5   |                     |                  |                     | 89bf-9943bcce3255   | d4d9ce8845f           |\n</code></pre> </li> <li> <p>Associate the floating IP with the instance</p> <pre><code>$ openstack server add floating ip login1 10.199.31.6\n</code></pre> </li> </ol> </li> <li> <p>To create a new floating IP </p> <ol> <li> <p>Create new floating IP and note the <code>floating_ip_address</code> </p> <pre><code>$ openstack floating ip create external1\n</code></pre> </li> <li> <p>Associate the floating IP with the instance (using the <code>floating_ip_address</code> from the previous output)</p> <pre><code>$ openstack server add floating ip login1 10.199.31.212\n</code></pre> </li> </ol> </li> </ol> </li> </ol> <p>Further detail on collecting the information from the above can be found in the Alces Cloud documentation.</p> <p>To set up a cluster, you will need to import a Flight Solo image.</p> <p>Before setting up a cluster on Alces Cloud, there are several required prerequisites:</p> <ul> <li>Your own keypair</li> <li>A network with a subnet and a router bridging the subnet to the external network</li> <li>A security group that allows traffic is given below (if creating the security group through the web interface then the \"Any\" protocol will need to be an \"Other Protocol\" rule with \"IP Protocol\" of <code>-1</code>)</li> </ul> Protocol Direction CIDR Port Range Any egress 0.0.0.0/0 any Any ingress Virtual Network CIDR any ICMP ingress 0.0.0.0/0 any SSH ingress 0.0.0.0/0 22 TCP ingress 0.0.0.0/0 80 TCP ingress 0.0.0.0/0 443 TCP ingress 0.0.0.0/0 5900-5903 <p>Note</p> <p>The \"Virtual Network CIDR\" is the subnet and netmask for the network that the nodes are using. For example, a node on the 11.11.11.0 network with a netmask of 255.255.255.0 would have a network CIDR of 11.11.11.0/24.</p> <p>To set up a cluster:</p> <ol> <li> <p>Go to the Alces Cloud instances page.</p> <p></p> </li> <li> <p>Click \"Launch Instance\", and the instance creation window will pop up.</p> </li> <li> <p>Fill in the instance name, and leave the number of instances as 1, then click next.</p> <p></p> </li> <li> <p>Choose the desired image to use by clicking the up arrow at the end of its row. It will be displayed in the \"Allocated\" section when selected.</p> <p></p> </li> <li> <p>Choose the desired instance size by clicking the up arrow at the end of its row. It will be displayed in the \"Allocated\" section when selected.</p> <p></p> </li> <li> <p>Choose a network in the same way as an image or instance size. Note that all nodes in a cluster must be on the same network.</p> <p></p> </li> <li> <p>Choose a security group in the same way as an image or instance size. Note that all nodes in a cluster must be in the same security group.</p> <p></p> </li> <li> <p>Choose the keypair in the same way as an image or instance size.</p> <p></p> </li> <li> <p>In the \"Configuration\" section, there is a \"Customisation Script\" section with a text box. This will be used to set your user data</p> <p></p> </li> <li> <p>When all options have been selected, press the \"Launch Instance\" button to launch. If the button is greyed out, then a mandatory setting has not been configured.</p> <p></p> </li> <li> <p>Go to the \"Instances\" page in the \"Compute\" section. The created node should be there and be finishing or have finished creation.</p> <p></p> </li> <li> <p>Click on the down arrow at the end of the instance row. This will bring up a drop-down menu.</p> </li> <li> <p>Select \"Associate Floating IP\", this will make the ip management window pop up.</p> <p></p> </li> <li> <p>Associate a floating IP, either by using an existing one or allocating a new one.</p> <ol> <li> <p>To use an existing floating IP:</p> <ol> <li> <p>Open the IP Address drop-down menu.</p> <p></p> </li> <li> <p>Select one of the IP Addresses.</p> </li> <li> <p>Click \"Associate\" to finish associating an IP.</p> </li> </ol> </li> <li> <p>To allocate a new floating IP:</p> <ol> <li> <p>Click the \"+\" next to the drop-down arrow to open the allocation menu.</p> <p></p> </li> <li> <p>Click \"Allocate IP\".</p> </li> </ol> </li> </ol> </li> <li> <p>Click \"Associate\" to finish associating an IP.</p> </li> </ol> <p>Further detail on collecting the information from the above can be found in the Alces Cloud documentation.</p>"},{"location":"docs/flight-solo/cluster-build-methods/slurm-multinode-alces-cloud/#launch-compute-nodes","title":"Launch Compute Nodes","text":""},{"location":"docs/flight-solo/cluster-build-methods/slurm-multinode-alces-cloud/#prepare-user-data_1","title":"Prepare User Data","text":"<p>Setting up compute nodes is done slightly differently than a login node. The basic steps are the same except subnets, networks and security groups need to match the ones used for the login node.</p> <p>This is the smallest amount of cloud init data necessary. It allows the login node to find the compute nodes as long as they are on the same network, and ssh into them from the root user (which is necessary for setup). <pre><code>#cloud-config\nusers:\n  - default\n  - name: root\n    ssh_authorized_keys:\n      - &lt;Content of ~/.ssh/id_alcescluster.pub from root user on login node&gt;\n</code></pre></p> <p>Tip</p> <p>The above is not required if the <code>SHAREPUBKEY</code> option was provided to the login node. If this was the case then the <code>SERVER</code> option provided to the compute node will be enough to enable root access from the login node.</p> <p>There are several options that can be added to change how a compute node will contact nodes on startup.</p> <ul> <li>Sending to a specific server:<ul> <li>Instead of broadcasting across a range, add the line <code>SERVER=&lt;private server IP&gt;</code> to send to specifically that node, which would be your login node.</li> </ul> </li> <li>Add an auth key:<ul> <li>Add the line <code>AUTH_KEY=&lt;string&gt;</code>. This means that the compute node will send it's flight hunter packet with this key. This must match the auth key provided to your login node</li> </ul> </li> </ul> An example of all mentioned lines in a single cloud init script.<pre><code>#cloud-config\nwrite_files:\n  - content: |\n      SERVER=10.10.0.1\n      AUTH_KEY=banana\n    path: /opt/flight/cloudinit.in\n    permissions: '0600'\n    owner: root:root\nusers:\n  - default\n  - name: root\n    ssh_authorized_keys:\n      - &lt;Content of ~/.ssh/id_alcescluster.pub from root user on login node&gt;\n</code></pre> <p>Info</p> <p>More information on available user data options for Flight Solo via the user data documentation</p>"},{"location":"docs/flight-solo/cluster-build-methods/slurm-multinode-alces-cloud/#deploy_1","title":"Deploy","text":"CLIGUI <ol> <li> <p>Launch a compute node with a command similar to the following: </p> <pre><code>$ openstack server create --flavor p1.small \\\n                          --image \"Flight Solo VERSION\" \\\n                          --boot-from-volume 10 \\\n                          --network \"mycluster1-network\" \\\n                          --key-name \"MyKey\" \\\n                          --security-group \"mycluster1-sg\" \\\n                          --user-data myuserdata.yml \\\n                          --min 2 \\\n                          --max 2 \\\n                          node\n</code></pre> <ul> <li> <p>Where:</p> <ul> <li><code>flavor</code> - Is the desired size of the instance</li> <li><code>image</code> - Is the Flight Solo image imported to Alces Cloud</li> <li><code>boot-from-volume</code> - Is the size of the system disk in GB</li> <li><code>network</code> - Is the name or ID of the network created for the cluster</li> <li><code>key-name</code> - Is the name of the SSH key to use</li> <li><code>security-group</code> - Is the name or ID of the security group created previously</li> <li><code>user-data</code> - Is the file containing cloud-init user-data </li> <li><code>min</code> and <code>max</code> - Is the number of nodes to launch</li> <li><code>node</code> - Is the name of the deployment to have numbers appended to (e.g. this example creates <code>node-1</code> and <code>node-2</code>)</li> </ul> </li> </ul> </li> </ol> <p>Further detail on collecting the information from the above can be found in the Alces Cloud documentation.</p> <ol> <li> <p>Go to the Alces Cloud instances page.</p> <p></p> </li> <li> <p>Click \"Launch Instance\", and the instance creation window will pop up.</p> </li> <li> <p>Fill in the instance name, and set the number of instances to create, then click next.</p> <p></p> </li> <li> <p>Choose the desired image to use by clicking the up arrow at the end of its row. It will be displayed in the \"Allocated\" section when selected.</p> <p></p> </li> <li> <p>Choose the desired instance size by clicking the up arrow at the end of its row. It will be displayed in the \"Allocated\" section when selected.</p> <p></p> </li> <li> <p>Choose a network in the same way as an image or instance size. Note that this should be the same network as the login node.</p> <p></p> </li> <li> <p>Choose a security group in the same way as an image or instance size. Note that this should be the same security group as the login node.</p> <p></p> </li> <li> <p>Choose the keypair in the same way as an image or instance size.</p> <p></p> </li> <li> <p>In the \"Configuration\" section, there is a \"Customisation Script\" section with a text box. This will be used to set user data script prepared earlier</p> <p></p> </li> <li> <p>When all options have been selected, press the \"Launch Instance\" button to launch. If the button is greyed out, then a mandatory setting has not been configured.</p> <p></p> </li> </ol>"},{"location":"docs/flight-solo/cluster-build-methods/slurm-multinode-alces-cloud/#general-configuration","title":"General Configuration","text":""},{"location":"docs/flight-solo/cluster-build-methods/slurm-multinode-alces-cloud/#create-node-inventory","title":"Create Node Inventory","text":"<ol> <li> <p>Parse your node(s) with the command <code>flight hunter parse</code>.</p> <ol> <li> <p>This will display a list of hunted nodes, for example     <pre><code>[flight@login-node.novalocal ~]$ flight hunter parse\nSelect nodes: (Scroll for more nodes)\n\u2023 \u2b21 login-node.novalocal - 10.10.0.1\n  \u2b21 compute-node-1.novalocal - 10.10.101.1\n</code></pre></p> </li> <li> <p>Select the desired node to be parsed with Space, and you will be taken to the label editor     <pre><code>Choose label: login-node.novalocal\n</code></pre></p> </li> <li> <p>Here, you can edit the label like plain text     <pre><code>Choose label: login1\n</code></pre></p> <p>Tip</p> <p>You can clear the current node name by pressing Down in the label editor.</p> </li> <li> <p>When done editing, press Enter to save. The modified node label will appear next to the ip address and original node label.     <pre><code>Select nodes: login-node.novalocal - 10.10.0.1 (login1) (Scroll for more nodes)\n\u2023 \u2b22 login-node.novalocal - 10.10.0.1 (login1)\n  \u2b21 compute-node-1.novalocal - 10.10.101.1\n</code></pre></p> </li> <li> <p>From this point, you can either hit Enter to finish parsing and process the selected nodes, or continue changing nodes. Either way, you can return to this list by running <code>flight hunter parse</code>.</p> </li> <li> <p>Save the node inventory before moving on to the next step.</p> <p>Tip</p> <p>See <code>flight hunter parse -h</code> for more ways to parse nodes.</p> </li> </ol> </li> </ol>"},{"location":"docs/flight-solo/cluster-build-methods/slurm-multinode-alces-cloud/#add-genders","title":"Add genders","text":"<ol> <li>Optionally, you may add genders to the newly parsed node. For example, in the case that the node should have the gender <code>cluster</code> and <code>all</code> then run the command:     <pre><code>flight hunter modify-groups --add cluster,all login1\n</code></pre></li> </ol>"},{"location":"docs/flight-solo/cluster-build-methods/slurm-multinode-alces-cloud/#slurm-multinode-configuration","title":"SLURM Multinode Configuration","text":"<ol> <li> <p>Configure profile</p> <pre><code>flight profile configure\n</code></pre> <ol> <li>This brings up a UI, where several options need to be set. Use up and down arrow keys to scroll through options and enter to move to the next option. Options in brackets coloured yellow are the default options that will be applied if nothing is entered.<ul> <li>Cluster type: The type of cluster setup needed, in this case <code>Slurm Multinode</code>.</li> <li>Cluster name: The name of the cluster.</li> <li>Setup Multi User Environment with IPA?: Boolean value to determine whether to configure a multi-user environment with IPA. If set to true then the following will need to be filled in<ul> <li>IPA domain: The domain for the IPA server to use.</li> <li>IPA secure admin password: The password to be used by the <code>admin</code> user of the IPA installation to manage the server.</li> </ul> </li> <li>Default user: The user that you log in with.</li> <li>Set user password: Set a password to be used for the chosen default user.</li> <li>IP or FQDN for Web Access: As described here, this could be the public IP or public hostname.</li> <li>IP range of compute nodes: The IP range of the compute nodes used, remember to add the netmask. E.g. <code>172.31.16.0/20</code></li> </ul> </li> </ol> </li> <li> <p>Apply identities by running the command <code>flight profile apply</code></p> <ol> <li> <p>First apply an identity to the login node     <pre><code>flight profile apply login1 login\n</code></pre></p> </li> <li> <p>Wait for the login node identity to finish applying. You can check the status of all nodes with <code>flight profile list</code>.</p> <p>Tip</p> <p>You can watch the progress of the application with <code>flight profile view login1 --watch</code></p> </li> <li> <p>Apply an identity to the each of the compute nodes (in this example, genders-style syntax is used to apply to <code>node01</code> and <code>node02</code>)      <pre><code>flight profile apply node[01-02] compute\n</code></pre></p> <p>Tip</p> <p>You can check all available identities for the current profile with <code>flight profile identities</code></p> </li> </ol> </li> </ol> <p>Success</p> <p>Congratulations, you've now created a SLURM Multinode environment! Learn more about SLURM in the HPC Environment docs.</p>"},{"location":"docs/flight-solo/cluster-build-methods/slurm-multinode-alces-cloud/#verifying-functionality","title":"Verifying Functionality","text":"<ol> <li> <p>Create a file called <code>simplejobscript.sh</code>, and copy this into it:     <pre><code>#!/bin/bash -l\necho \"Starting running on host $HOSTNAME\"\nsleep 30\necho \"Finished running - goodbye from $HOSTNAME\"\n</code></pre></p> </li> <li> <p>Run the script with <code>sbatch simplejobscript.sh</code>, and to test all your nodes try queuing up enough jobs that all nodes will have to run.</p> </li> <li> <p>In the directory that the job was submitted from there should be a <code>slurm-X.out</code> where <code>X</code> is the Job ID returned from the <code>sbatch</code> command. This will contain the echo messages from the script created in step 1 </p> </li> </ol>"},{"location":"docs/flight-solo/cluster-build-methods/slurm-multinode-aws/","title":"SLURM Multinode on AWS","text":""},{"location":"docs/flight-solo/cluster-build-methods/slurm-multinode-aws/#launch-login-node","title":"Launch Login Node","text":""},{"location":"docs/flight-solo/cluster-build-methods/slurm-multinode-aws/#prepare-user-data","title":"Prepare User Data","text":"<p>When launching a login node it is worth considering what user data options to provide. While it is not required, user data can provide powerful customisation at launch that can further streamline the cluster build process.</p> <p>There are several options that can be added to change how a compute node will contact nodes on startup.</p> <ul> <li>Sharing public ssh key to clients: <ul> <li>Instead of manually obtaining and sharing the root public SSH key (passwordless root ssh is required for flight profile) this can be shared over the local network with <code>SHAREPUBKEY=true</code></li> </ul> </li> <li>Add an auth key:<ul> <li>Add the line <code>AUTH_KEY=&lt;string&gt;</code>. This means that the node will only accept incoming flight hunter nodes that provide a matching authorisation key</li> </ul> </li> </ul> An example of all mentioned lines in a single cloud init script.<pre><code>#cloud-config\nwrite_files:\n  - content: |\n      SHAREPUBKEY=true\n      AUTH_KEY=banana\n    path: /opt/flight/cloudinit.in\n    permissions: '0600'\n    owner: root:root\n</code></pre> <p>Info</p> <p>More information on available user data options for Flight Solo via the user data documentation</p>"},{"location":"docs/flight-solo/cluster-build-methods/slurm-multinode-aws/#deploy","title":"Deploy","text":"AWS MarketplaceAWS Imported <ol> <li> <p>Find the Flight Solo image here or by searching the marketplace for \"Flight Solo\".</p> </li> <li> <p>Click \"Continue to Subscribe\"</p> <p></p> </li> <li> <p>Read the terms and conditions, then click \"Continue to Configuration\"</p> <p></p> </li> <li> <p>Configure region, software version (if unsure use the latest), and fulfillment option (if unsure use the default). Then click \"Continue to Launch\". Make sure the region is the same for all nodes to be used in a cluster.</p> <p></p> </li> <li> <p>Click on \"Usage Instructions\" to see some instructions on how to get started, and a link to this documentation.</p> <p></p> </li> <li> <p>Select the \"Launch from Website\" action.</p> <p></p> </li> <li> <p>Choose an instance type to use.</p> <p></p> </li> <li> <p>Choose VPC settings. Remember what VPC was used to create this instance, as it should also be used for any associated compute nodes.</p> <p></p> </li> <li> <p>Choose a subnet. Remember what subnet was used to create this instance, as it should also be used for any associated compute nodes.</p> <p></p> </li> <li> <p>A security group is needed to associate with all nodes on the cluster. It is recommended to use a security group with rules limiting traffic through:</p> <ul> <li>HTTP</li> <li>HTTPS</li> <li>SSH</li> <li>Port 8888</li> <li>Ports 5900 - 5903</li> <li>All traffic from within the security group should be allowed. (This rule can only be added after creation)</li> </ul> <p>Note</p> <p>If you already have a security group which does this, use it here and make sure to use it again for the compute nodes. Otherwise, a security group can be made from the launch page, or through the security groups page</p> <p>Describing exactly how to create a security group is out of scope for this documentation, but covered by the AWS documentation.</p> <p>However, here is an example security group that might be used for a Flight Solo cluster:</p> <p></p> <p>Tip</p> <p>The seller's settings (shown below) can be used as a reference for creating a security group.</p> <p></p> </li> <li> <p>After a security group has been made, click \"Select existing security group\" select it from the drop down menu.</p> <p></p> </li> <li> <p>Choose what key pair to use. It is good practice for this to be the same on all nodes in a cluster.</p> <p></p> </li> <li> <p>Click Launch</p> <p></p> </li> </ol> <p>To set up a cluster, you will need to import a Flight Solo image.</p> <ol> <li> <p>Go the EC2 instance console</p> <p></p> </li> <li> <p>Click \"Launch\" to go to the EC2 instance setup page.</p> <p></p> </li> <li> <p>Set the number of instances to 1, and name of instance to something descriptive.</p> <p></p> </li> <li> <p>Confirm that the region(top right, next to username) is correct.</p> <p></p> </li> <li> <p>In the \"Application and OS Images\" section choose the \"My AMIs\" tab and select your imported solo AMI.</p> <p></p> </li> <li> <p>In the \"Instance type\" section, choose the required instance size.</p> <p></p> </li> <li> <p>In the \"Keypair\" section, select a keypair to use. It is good practice to use the same keypair for the login and compute nodes.</p> <p></p> </li> <li> <p>In the \"Network settings\" sections, click the \"Edit\" button to set the network and subnet. Remember what these are, as they should be the same for any associated compute nodes.</p> <p></p> <p></p> </li> <li> <p>Another thing needed is a security group to associate with all nodes on the cluster. It is recommended to use a security group with rules limiting traffic through:</p> <ul> <li>HTTP</li> <li>HTTPS</li> <li>SSH</li> <li>Port 8888</li> <li>Ports 5900 - 5903</li> <li>All traffic from within the security group should be allowed. (This rule can only be added after creation)</li> </ul> <p>Note</p> <p>If you already have a security group which does this, use it here and make sure to use it again for the compute nodes. Otherwise, a security group can be made from the launch page, or through the security groups page</p> <p>Describing exactly how to create a security group is out of scope for this documentation, but covered by the AWS documentation.</p> <p>However, here is an example security group that might be used for a Flight Solo cluster:</p> <p></p> </li> <li> <p>After a security group has been made, click \"Choose Existing\" select it from the drop down menu.</p> </li> <li> <p>In the \"Configure Storage\" section, allocate as much memory as needed. 8GB is the minimum required for Flight Solo, so it is likely the compute nodes will not need much more than that, as the login node hosts most data.</p> <p></p> </li> <li> <p>Finally, click \"Launch Instance\".</p> </li> </ol>"},{"location":"docs/flight-solo/cluster-build-methods/slurm-multinode-aws/#launch-compute-nodes","title":"Launch Compute Nodes","text":""},{"location":"docs/flight-solo/cluster-build-methods/slurm-multinode-aws/#prepare-user-data_1","title":"Prepare User Data","text":"<p>Setting up compute nodes is done slightly differently than a login node. The basic steps are the same except subnets, networks and security groups need to match the ones used for the login node.</p> <p>This is the smallest amount of cloud init data necessary. It allows the login node to find the compute nodes as long as they are on the same network, and ssh into them from the root user (which is necessary for setup). <pre><code>#cloud-config\nusers:\n  - default\n  - name: root\n    ssh_authorized_keys:\n      - &lt;Content of ~/.ssh/id_alcescluster.pub from root user on login node&gt;\n</code></pre></p> <p>Tip</p> <p>The above is not required if the <code>SHAREPUBKEY</code> option was provided to the login node. If this was the case then the <code>SERVER</code> option provided to the compute node will be enough to enable root access from the login node.</p> <p>There are several options that can be added to change how a compute node will contact nodes on startup.</p> <ul> <li>Sending to a specific server:<ul> <li>Instead of broadcasting across a range, add the line <code>SERVER=&lt;private server IP&gt;</code> to send to specifically that node, which would be your login node.</li> </ul> </li> <li>Add an auth key:<ul> <li>Add the line <code>AUTH_KEY=&lt;string&gt;</code>. This means that the compute node will send it's flight hunter packet with this key. This must match the auth key provided to your login node</li> </ul> </li> </ul> An example of all mentioned lines in a single cloud init script.<pre><code>#cloud-config\nwrite_files:\n  - content: |\n      SERVER=10.10.0.1\n      AUTH_KEY=banana\n    path: /opt/flight/cloudinit.in\n    permissions: '0600'\n    owner: root:root\nusers:\n  - default\n  - name: root\n    ssh_authorized_keys:\n      - &lt;Content of ~/.ssh/id_alcescluster.pub from root user on login node&gt;\n</code></pre> <p>Info</p> <p>More information on available user data options for Flight Solo via the user data documentation</p>"},{"location":"docs/flight-solo/cluster-build-methods/slurm-multinode-aws/#deploy_1","title":"Deploy","text":"AWS MarketplaceAWS Imported <ol> <li> <p>Go to the EC2 instance setup page through marketplace.</p> <ol> <li> <p>Find the Flight Solo image here or by searching the marketplace for \"Flight Solo\".</p> </li> <li> <p>Click \"Continue to Subscribe\"</p> <p></p> </li> <li> <p>Read the terms and conditions, then click \"Continue to Configuration\"</p> <p></p> </li> <li> <p>Configure region, software version (if unsure use the latest), and fulfillment option (if unsure use the default). Then click \"Continue to Launch\". Make sure the region is the same for all nodes to be used in a cluster.</p> <p></p> </li> <li> <p>Click on \"Usage Instructions\" to see some instructions on how to get started, and a link to this documentation.</p> <p></p> </li> <li> <p>Select the \"Launch from EC2\" action</p> <p></p> </li> <li> <p>Click \"Launch\" to go to the EC2 instance setup page.</p> <p></p> </li> </ol> </li> <li> <p>Set the instance name and number of instances.</p> <p></p> </li> <li> <p>Confirm that the region(top right, next to username) is the same as the region the login node was created in.</p> <p></p> </li> <li> <p>In the \"Application and OS Images\" section, confirm that Flight Solo is the selected AMI.</p> <p></p> </li> <li> <p>In the \"Instance type\" section, choose the required instance size.</p> <p></p> </li> <li> <p>In the \"Keypair\" section, select a keypair to use. It is good practice to use the same keypair for the login and compute nodes.</p> <p></p> </li> <li> <p>In the \"Network settings\" section, select the same network, subnet, and security group as the login node.</p> <p></p> <ol> <li> <p>To change the network and subnet, click the \"Edit\" button, and then use the drop downs to find the correct network and subnet.</p> <p></p> </li> </ol> </li> <li> <p>In the \"Configure Storage\" section, allocate as much memory as needed. 8GB is the minimum required for Flight Solo, so it is likely the compute nodes will not need much more than that, as the login node hosts most data.</p> <p></p> </li> <li> <p>In the \"Advanced details\" section there are many settings, but at the bottom is a text box labeled \"User data\".</p> <p></p> <ol> <li> <p>Write a cloud init script in the user data section, see here for details:</p> </li> <li> <p>To get the information necessary for the cloud init script. Go to the EC2 console. Make sure your region is set to the one used for login and compute nodes.</p> </li> <li> <p>Select the created login node to see more details about it, including the private ip.</p> <p></p> </li> <li> <p>Log in to the login node.</p> </li> <li> <p>Become the root user and open the file <code>~/.ssh/id_alcescluster.pub</code>, copy the contents to the cloud init script.</p> <p>Tip</p> <p>If the login node is launched using the <code>SHAREPUBKEY</code> then there is no need to perform steps <code>d</code> and <code>e</code> as this will be performed by the systems.</p> </li> </ol> </li> <li> <p>Back on the compute node creation page, click \"Launch Instance\".</p> </li> </ol> <p>Note</p> <p>Repeat this process for any other types of nodes that need to be added to the cluster.</p> <ol> <li> <p>Go the EC2 instance console</p> <p></p> <ol> <li> <p>Click \"Launch Instance\" to go to the EC2 instance setup page.</p> <p></p> </li> </ol> </li> <li> <p>Set the instance name and number of instances.</p> <p></p> </li> <li> <p>Confirm that the region(top right, next to username) is the same as the region the login node was created in.</p> <p></p> </li> <li> <p>In the \"Application and OS Images\" section choose the \"My AMIs\" tab and select your imported solo AMI.</p> <p></p> </li> <li> <p>In the \"Instance type\" section, choose the required instance size.</p> <p></p> </li> <li> <p>In the \"Keypair\" section, select a keypair to use. It is good practice to use the same keypair for the login and compute nodes.</p> <p></p> </li> <li> <p>In the \"Network settings\" section, select the same network, subnet, and security group as the login node.</p> <p></p> <ol> <li> <p>To change the network and subnet, click the \"Edit\" button, and then use the drop downs to find the correct network and subnet.</p> <p></p> </li> </ol> </li> <li> <p>In the \"Configure Storage\" section, allocate as much memory as needed. 8GB is the minimum required for Flight Solo, so it is likely the compute nodes will not need much more than that, as the login node hosts most data.</p> <p></p> </li> <li> <p>In the \"Advanced details\" section there are many settings, but at the bottom is a text box labeled \"User data\".</p> <p></p> <ol> <li> <p>Write a cloud init script in the user data section, see here for details:</p> </li> <li> <p>To get the information necessary for the cloud init script. Go to the EC2 console.</p> </li> <li> <p>Select the created login node to see more details about it, including the private ip.</p> <p></p> </li> <li> <p>Log in to the login node.</p> </li> <li> <p>Become the root user and open the file <code>~/.ssh/id_alcescluster.pub</code>, copy the contents to the cloud init script.</p> <p>Tip</p> <p>If the login node is launched using the <code>SHAREPUBKEY</code> then there is no need to perform steps <code>d</code> and <code>e</code> as this will be performed by the systems.</p> </li> </ol> </li> <li> <p>Back on the compute node creation page, click \"Launch Instance\".</p> </li> </ol> <p>Note</p> <p>Repeat this process for any other types of nodes that need to be added to the cluster.</p>"},{"location":"docs/flight-solo/cluster-build-methods/slurm-multinode-aws/#general-configuration","title":"General Configuration","text":""},{"location":"docs/flight-solo/cluster-build-methods/slurm-multinode-aws/#create-node-inventory","title":"Create Node Inventory","text":"<ol> <li> <p>Parse your node(s) with the command <code>flight hunter parse</code>.</p> <ol> <li> <p>This will display a list of hunted nodes, for example     <pre><code>[flight@login-node.novalocal ~]$ flight hunter parse\nSelect nodes: (Scroll for more nodes)\n\u2023 \u2b21 login-node.novalocal - 10.10.0.1\n  \u2b21 compute-node-1.novalocal - 10.10.101.1\n</code></pre></p> </li> <li> <p>Select the desired node to be parsed with Space, and you will be taken to the label editor     <pre><code>Choose label: login-node.novalocal\n</code></pre></p> </li> <li> <p>Here, you can edit the label like plain text     <pre><code>Choose label: login1\n</code></pre></p> <p>Tip</p> <p>You can clear the current node name by pressing Down in the label editor.</p> </li> <li> <p>When done editing, press Enter to save. The modified node label will appear next to the ip address and original node label.     <pre><code>Select nodes: login-node.novalocal - 10.10.0.1 (login1) (Scroll for more nodes)\n\u2023 \u2b22 login-node.novalocal - 10.10.0.1 (login1)\n  \u2b21 compute-node-1.novalocal - 10.10.101.1\n</code></pre></p> </li> <li> <p>From this point, you can either hit Enter to finish parsing and process the selected nodes, or continue changing nodes. Either way, you can return to this list by running <code>flight hunter parse</code>.</p> </li> <li> <p>Save the node inventory before moving on to the next step.</p> <p>Tip</p> <p>See <code>flight hunter parse -h</code> for more ways to parse nodes.</p> </li> </ol> </li> </ol>"},{"location":"docs/flight-solo/cluster-build-methods/slurm-multinode-aws/#add-genders","title":"Add genders","text":"<ol> <li>Optionally, you may add genders to the newly parsed node. For example, in the case that the node should have the gender <code>cluster</code> and <code>all</code> then run the command:     <pre><code>flight hunter modify-groups --add cluster,all login1\n</code></pre></li> </ol>"},{"location":"docs/flight-solo/cluster-build-methods/slurm-multinode-aws/#slurm-multinode-configuration","title":"SLURM Multinode Configuration","text":"<ol> <li> <p>Configure profile</p> <pre><code>flight profile configure\n</code></pre> <ol> <li>This brings up a UI, where several options need to be set. Use up and down arrow keys to scroll through options and enter to move to the next option. Options in brackets coloured yellow are the default options that will be applied if nothing is entered.<ul> <li>Cluster type: The type of cluster setup needed, in this case <code>Slurm Multinode</code>.</li> <li>Cluster name: The name of the cluster.</li> <li>Setup Multi User Environment with IPA?: Boolean value to determine whether to configure a multi-user environment with IPA. If set to true then the following will need to be filled in<ul> <li>IPA domain: The domain for the IPA server to use.</li> <li>IPA secure admin password: The password to be used by the <code>admin</code> user of the IPA installation to manage the server.</li> </ul> </li> <li>Default user: The user that you log in with.</li> <li>Set user password: Set a password to be used for the chosen default user.</li> <li>IP or FQDN for Web Access: As described here, this could be the public IP or public hostname.</li> <li>IP range of compute nodes: The IP range of the compute nodes used, remember to add the netmask. E.g. <code>172.31.16.0/20</code></li> </ul> </li> </ol> </li> <li> <p>Apply identities by running the command <code>flight profile apply</code></p> <ol> <li> <p>First apply an identity to the login node     <pre><code>flight profile apply login1 login\n</code></pre></p> </li> <li> <p>Wait for the login node identity to finish applying. You can check the status of all nodes with <code>flight profile list</code>.</p> <p>Tip</p> <p>You can watch the progress of the application with <code>flight profile view login1 --watch</code></p> </li> <li> <p>Apply an identity to the each of the compute nodes (in this example, genders-style syntax is used to apply to <code>node01</code> and <code>node02</code>)      <pre><code>flight profile apply node[01-02] compute\n</code></pre></p> <p>Tip</p> <p>You can check all available identities for the current profile with <code>flight profile identities</code></p> </li> </ol> </li> </ol> <p>Success</p> <p>Congratulations, you've now created a SLURM Multinode environment! Learn more about SLURM in the HPC Environment docs.</p>"},{"location":"docs/flight-solo/cluster-build-methods/slurm-multinode-aws/#verifying-functionality","title":"Verifying Functionality","text":"<ol> <li> <p>Create a file called <code>simplejobscript.sh</code>, and copy this into it:     <pre><code>#!/bin/bash -l\necho \"Starting running on host $HOSTNAME\"\nsleep 30\necho \"Finished running - goodbye from $HOSTNAME\"\n</code></pre></p> </li> <li> <p>Run the script with <code>sbatch simplejobscript.sh</code>, and to test all your nodes try queuing up enough jobs that all nodes will have to run.</p> </li> <li> <p>In the directory that the job was submitted from there should be a <code>slurm-X.out</code> where <code>X</code> is the Job ID returned from the <code>sbatch</code> command. This will contain the echo messages from the script created in step 1 </p> </li> </ol>"},{"location":"docs/flight-solo/cluster-build-methods/slurm-multinode-azure/","title":"SLURM Multinode on Azure","text":""},{"location":"docs/flight-solo/cluster-build-methods/slurm-multinode-azure/#launch-login-node","title":"Launch Login Node","text":""},{"location":"docs/flight-solo/cluster-build-methods/slurm-multinode-azure/#prepare-user-data","title":"Prepare User Data","text":"<p>When launching a login node it is worth considering what user data options to provide. While it is not required, user data can provide powerful customisation at launch that can further streamline the cluster build process.</p> <p>There are several options that can be added to change how a compute node will contact nodes on startup.</p> <ul> <li>Sharing public ssh key to clients: <ul> <li>Instead of manually obtaining and sharing the root public SSH key (passwordless root ssh is required for flight profile) this can be shared over the local network with <code>SHAREPUBKEY=true</code></li> </ul> </li> <li>Add an auth key:<ul> <li>Add the line <code>AUTH_KEY=&lt;string&gt;</code>. This means that the node will only accept incoming flight hunter nodes that provide a matching authorisation key</li> </ul> </li> </ul> An example of all mentioned lines in a single cloud init script.<pre><code>#cloud-config\nwrite_files:\n  - content: |\n      SHAREPUBKEY=true\n      AUTH_KEY=banana\n    path: /opt/flight/cloudinit.in\n    permissions: '0600'\n    owner: root:root\n</code></pre> <p>Info</p> <p>More information on available user data options for Flight Solo via the user data documentation</p>"},{"location":"docs/flight-solo/cluster-build-methods/slurm-multinode-azure/#deploy","title":"Deploy","text":"<p>To set up a cluster, you will need to import a Flight Solo image.</p> <ol> <li> <p>Go to the Microsoft Azure portal.</p> <p></p> </li> <li> <p>Go to Virtual Machines, and click \"Create\".</p> <p></p> </li> <li> <p>Select \"Azure virtual machine\", which will load this page:</p> <p></p> </li> <li> <p>On the Basics page:</p> <ol> <li>Set Subscription to your subscription type.</li> <li>Set Resource Group to your desired resource group (where the vm will be kept after creation).</li> <li>Set Virtual machine name to any suitable name. (<code>-</code> does not work in a name)</li> <li>Set Image to the imported Flight Solo Image.<ol> <li>It may be necessary to open the drop-down and/or see all images in order to find the imported image.     </li> <li>Scroll down to see more options     </li> </ol> </li> <li>Set Size to your choice of size.</li> <li>Set Authentication type to <code>SSH public key</code></li> <li>Set Username to any suitable username.</li> <li>Set SSH public key source to the most suitable option, but remember what key was used if creating compute nodes later.</li> <li>Fill in the Key pair name/Stored key/Use existing key as appropriate to the chosen public key source.</li> <li>Allow traffic to selected ports, and select <code>SSH(22)</code>, <code>HTTP(80)</code> and <code>HTTPS(443)</code> as the allowed ports.</li> <li>Set the most appropriate license type.</li> </ol> </li> <li> <p>Continuing on to the next page, Disks, all necessary details should already be filled out, so this page can be skipped (unless you know what you want to change). However, it is recommended to select Delete with VM.</p> <p></p> </li> <li> <p>Go on to the networking tab and fill out the necessary options.</p> <p></p> <ol> <li>Set Virtual Network or create a new one by pressing \"Create new\" and setting a name. Remember what this is for if you create compute nodes.</li> <li>Set Subnet to one of the options in the drop-down menu, if it isn't already set. Remember what this is for if you create compute nodes.</li> <li>Set Public IP to an existing public IP or create a new one by pressing \"Create new\" and setting a name.</li> <li>Set NIC network security group to \"Advanced\", and press \"Create new\" to create a new security group.     <ol> <li>Click on \"Add an inbound rule\" to open the inbound rule creator     </li> <li>Create rules to allow <code>HTTP</code>, <code>HTTPS</code> and <code>SSH</code> traffic from your IP address to the security group.</li> <li>When complete, press \"OK\" at the bottom left of the screen to return to image creation.</li> </ol> </li> </ol> </li> <li> <p>The Management, Monitoring and Tags tabs have more options that aren't necessary for setup. Skip to the tab Advanced</p> </li> <li> <p>In the Custom data and cloud init section, there is a text box. This is where your user data can be specified</p> <p></p> </li> <li> <p>Azure will take some time to review your settings. If there are no issues click \"Create\" to finish creation.</p> </li> </ol>"},{"location":"docs/flight-solo/cluster-build-methods/slurm-multinode-azure/#launch-compute-nodes","title":"Launch Compute Nodes","text":""},{"location":"docs/flight-solo/cluster-build-methods/slurm-multinode-azure/#prepare-user-data_1","title":"Prepare User Data","text":"<p>Setting up compute nodes is done slightly differently than a login node. The basic steps are the same except subnets, networks and security groups need to match the ones used for the login node.</p> <p>This is the smallest amount of cloud init data necessary. It allows the login node to find the compute nodes as long as they are on the same network, and ssh into them from the root user (which is necessary for setup). <pre><code>#cloud-config\nusers:\n  - default\n  - name: root\n    ssh_authorized_keys:\n      - &lt;Content of ~/.ssh/id_alcescluster.pub from root user on login node&gt;\n</code></pre></p> <p>Tip</p> <p>The above is not required if the <code>SHAREPUBKEY</code> option was provided to the login node. If this was the case then the <code>SERVER</code> option provided to the compute node will be enough to enable root access from the login node.</p> <p>There are several options that can be added to change how a compute node will contact nodes on startup.</p> <ul> <li>Sending to a specific server:<ul> <li>Instead of broadcasting across a range, add the line <code>SERVER=&lt;private server IP&gt;</code> to send to specifically that node, which would be your login node.</li> </ul> </li> <li>Add an auth key:<ul> <li>Add the line <code>AUTH_KEY=&lt;string&gt;</code>. This means that the compute node will send it's flight hunter packet with this key. This must match the auth key provided to your login node</li> </ul> </li> </ul> An example of all mentioned lines in a single cloud init script.<pre><code>#cloud-config\nwrite_files:\n  - content: |\n      SERVER=10.10.0.1\n      AUTH_KEY=banana\n    path: /opt/flight/cloudinit.in\n    permissions: '0600'\n    owner: root:root\nusers:\n  - default\n  - name: root\n    ssh_authorized_keys:\n      - &lt;Content of ~/.ssh/id_alcescluster.pub from root user on login node&gt;\n</code></pre> <p>Info</p> <p>More information on available user data options for Flight Solo via the user data documentation</p>"},{"location":"docs/flight-solo/cluster-build-methods/slurm-multinode-azure/#deploy_1","title":"Deploy","text":"<ol> <li> <p>Go to the Microsoft Azure portal.</p> <p></p> </li> <li> <p>Go to Virtual Machines, and click \"Create\".</p> <p></p> </li> <li> <p>Select \"Azure virtual machine\", which will load this page:</p> <p></p> </li> <li> <p>On the Basics page:</p> <ol> <li>Set Subscription to your subscription type.</li> <li>Set Resource Group to the same as the login node </li> <li>Set Virtual machine name to any suitable name.</li> <li>Set Image to the imported Flight Solo Image.<ol> <li>It may be necessary to open the drop-down and/or see all images in order to find the imported image.     </li> <li>Scroll down to see more options     </li> </ol> </li> <li>Set Size to your choice of size.</li> <li>Set Authentication type to <code>SSH public key</code></li> <li>Set Username to the same username as with the login node.</li> <li>Set SSH public key source to the same key that was used for the login node.</li> <li>Fill in the Key pair name/Stored key/Use existing key as appropriate to the chosen public key source.</li> <li>Allow traffic to selected ports, and select <code>SSH(22</code>, <code>HTTP(80)</code> and <code>HTTPS(443)</code> as the allowed ports.</li> <li>Set the most appropriate license type.</li> </ol> </li> <li> <p>Continuing on to the next page, Disks, all necessary details should already be filled out, so this page can be skipped (unless you know what you want to change). However, it is recommended to select Delete with VM.</p> <p></p> </li> <li> <p>Go on to the networking tab and fill out the necessary options.</p> <ol> <li>Set Virtual Network to the same network that was used for the login node.</li> <li>Set Subnet to the same subnet that was used for the login node.</li> <li>Set NIC network security group to the same subnet that was used for login node.</li> <li>When complete, press \"OK\" at the bottom left of the screen to return to image creation.</li> </ol> </li> <li> <p>The Management and Monitoring tabs have more options that aren't necessary for setup. Skip to the Advanced tab.</p> <p></p> </li> <li> <p>In the Custom data and cloud init section, there is a text box. Write a cloud init script as prepared earlier in the custom data section</p> <p></p> </li> <li> <p>Skip to the Review + Create section. Azure will take some time to review your settings. If there are no issues click \"Create\" to finish creation.</p> </li> </ol>"},{"location":"docs/flight-solo/cluster-build-methods/slurm-multinode-azure/#general-configuration","title":"General Configuration","text":""},{"location":"docs/flight-solo/cluster-build-methods/slurm-multinode-azure/#create-node-inventory","title":"Create Node Inventory","text":"<ol> <li> <p>Parse your node(s) with the command <code>flight hunter parse</code>.</p> <ol> <li> <p>This will display a list of hunted nodes, for example     <pre><code>[flight@login-node.novalocal ~]$ flight hunter parse\nSelect nodes: (Scroll for more nodes)\n\u2023 \u2b21 login-node.novalocal - 10.10.0.1\n  \u2b21 compute-node-1.novalocal - 10.10.101.1\n</code></pre></p> </li> <li> <p>Select the desired node to be parsed with Space, and you will be taken to the label editor     <pre><code>Choose label: login-node.novalocal\n</code></pre></p> </li> <li> <p>Here, you can edit the label like plain text     <pre><code>Choose label: login1\n</code></pre></p> <p>Tip</p> <p>You can clear the current node name by pressing Down in the label editor.</p> </li> <li> <p>When done editing, press Enter to save. The modified node label will appear next to the ip address and original node label.     <pre><code>Select nodes: login-node.novalocal - 10.10.0.1 (login1) (Scroll for more nodes)\n\u2023 \u2b22 login-node.novalocal - 10.10.0.1 (login1)\n  \u2b21 compute-node-1.novalocal - 10.10.101.1\n</code></pre></p> </li> <li> <p>From this point, you can either hit Enter to finish parsing and process the selected nodes, or continue changing nodes. Either way, you can return to this list by running <code>flight hunter parse</code>.</p> </li> <li> <p>Save the node inventory before moving on to the next step.</p> <p>Tip</p> <p>See <code>flight hunter parse -h</code> for more ways to parse nodes.</p> </li> </ol> </li> </ol>"},{"location":"docs/flight-solo/cluster-build-methods/slurm-multinode-azure/#add-genders","title":"Add genders","text":"<ol> <li>Optionally, you may add genders to the newly parsed node. For example, in the case that the node should have the gender <code>cluster</code> and <code>all</code> then run the command:     <pre><code>flight hunter modify-groups --add cluster,all login1\n</code></pre></li> </ol>"},{"location":"docs/flight-solo/cluster-build-methods/slurm-multinode-azure/#slurm-multinode-configuration","title":"SLURM Multinode Configuration","text":"<ol> <li> <p>Configure profile</p> <pre><code>flight profile configure\n</code></pre> <ol> <li>This brings up a UI, where several options need to be set. Use up and down arrow keys to scroll through options and enter to move to the next option. Options in brackets coloured yellow are the default options that will be applied if nothing is entered.<ul> <li>Cluster type: The type of cluster setup needed, in this case <code>Slurm Multinode</code>.</li> <li>Cluster name: The name of the cluster.</li> <li>Setup Multi User Environment with IPA?: Boolean value to determine whether to configure a multi-user environment with IPA. If set to true then the following will need to be filled in<ul> <li>IPA domain: The domain for the IPA server to use.</li> <li>IPA secure admin password: The password to be used by the <code>admin</code> user of the IPA installation to manage the server.</li> </ul> </li> <li>Default user: The user that you log in with.</li> <li>Set user password: Set a password to be used for the chosen default user.</li> <li>IP or FQDN for Web Access: As described here, this could be the public IP or public hostname.</li> <li>IP range of compute nodes: The IP range of the compute nodes used, remember to add the netmask. E.g. <code>172.31.16.0/20</code></li> </ul> </li> </ol> </li> <li> <p>Apply identities by running the command <code>flight profile apply</code></p> <ol> <li> <p>First apply an identity to the login node     <pre><code>flight profile apply login1 login\n</code></pre></p> </li> <li> <p>Wait for the login node identity to finish applying. You can check the status of all nodes with <code>flight profile list</code>.</p> <p>Tip</p> <p>You can watch the progress of the application with <code>flight profile view login1 --watch</code></p> </li> <li> <p>Apply an identity to the each of the compute nodes (in this example, genders-style syntax is used to apply to <code>node01</code> and <code>node02</code>)      <pre><code>flight profile apply node[01-02] compute\n</code></pre></p> <p>Tip</p> <p>You can check all available identities for the current profile with <code>flight profile identities</code></p> </li> </ol> </li> </ol> <p>Success</p> <p>Congratulations, you've now created a SLURM Multinode environment! Learn more about SLURM in the HPC Environment docs.</p>"},{"location":"docs/flight-solo/cluster-build-methods/slurm-multinode-azure/#verifying-functionality","title":"Verifying Functionality","text":"<ol> <li> <p>Create a file called <code>simplejobscript.sh</code>, and copy this into it:     <pre><code>#!/bin/bash -l\necho \"Starting running on host $HOSTNAME\"\nsleep 30\necho \"Finished running - goodbye from $HOSTNAME\"\n</code></pre></p> </li> <li> <p>Run the script with <code>sbatch simplejobscript.sh</code>, and to test all your nodes try queuing up enough jobs that all nodes will have to run.</p> </li> <li> <p>In the directory that the job was submitted from there should be a <code>slurm-X.out</code> where <code>X</code> is the Job ID returned from the <code>sbatch</code> command. This will contain the echo messages from the script created in step 1 </p> </li> </ol>"},{"location":"docs/flight-solo/cluster-build-methods/slurm-multinode-openstack/","title":"SLURM Multinode on OpenStack","text":""},{"location":"docs/flight-solo/cluster-build-methods/slurm-multinode-openstack/#launch-login-node","title":"Launch Login Node","text":""},{"location":"docs/flight-solo/cluster-build-methods/slurm-multinode-openstack/#prepare-user-data","title":"Prepare User Data","text":"<p>When launching a login node it is worth considering what user data options to provide. While it is not required, user data can provide powerful customisation at launch that can further streamline the cluster build process.</p> <p>There are several options that can be added to change how a compute node will contact nodes on startup.</p> <ul> <li>Sharing public ssh key to clients: <ul> <li>Instead of manually obtaining and sharing the root public SSH key (passwordless root ssh is required for flight profile) this can be shared over the local network with <code>SHAREPUBKEY=true</code></li> </ul> </li> <li>Add an auth key:<ul> <li>Add the line <code>AUTH_KEY=&lt;string&gt;</code>. This means that the node will only accept incoming flight hunter nodes that provide a matching authorisation key</li> </ul> </li> </ul> An example of all mentioned lines in a single cloud init script.<pre><code>#cloud-config\nwrite_files:\n  - content: |\n      SHAREPUBKEY=true\n      AUTH_KEY=banana\n    path: /opt/flight/cloudinit.in\n    permissions: '0600'\n    owner: root:root\n</code></pre> <p>Info</p> <p>More information on available user data options for Flight Solo via the user data documentation</p>"},{"location":"docs/flight-solo/cluster-build-methods/slurm-multinode-openstack/#deploy","title":"Deploy","text":"<p>To set up a cluster, you will need to import a Flight Solo image.</p> <p>Before setting up a cluster on Openstack, there are several required prerequisites:</p> <ul> <li>Your own keypair</li> <li>A network</li> <li>A router<ul> <li>With an interface both on the External Gateway network and an Internal Interface on the previously created network</li> </ul> </li> <li>A security group that allows traffic is given below (if creating the security group through the web interface then the \"Any\" protocol will need to be an \"Other Protocol\" rule with \"IP Protocol\" of <code>-1</code>)</li> </ul> Protocol Direction CIDR Port Range Any egress 0.0.0.0/0 any Any ingress Virtual Network CIDR any ICMP ingress 0.0.0.0/0 any SSH ingress 0.0.0.0/0 22 TCP ingress 0.0.0.0/0 80 TCP ingress 0.0.0.0/0 443 TCP ingress 0.0.0.0/0 5900-5903 <p>Note</p> <p>The \"Virtual Network CIDR\" is the subnet and netmask for the network that the nodes are using. For example, a node on the 11.11.11.0 network with a netmask of 255.255.255.0 would have a network CIDR of 11.11.11.0/24.</p> <p>The documentation includes instructions for importing an image to Openstack, and guides for setting up the other prerequisites can be found in the Openstack documentation</p> <p>To set up a cluster:</p> <ol> <li> <p>Go to the Openstack instances page.</p> <p></p> </li> <li> <p>Click \"Launch Instance\", and the instance creation window will pop up.</p> </li> <li> <p>Fill in the instance name, and leave the number of instances as 1, then click next.</p> <p></p> </li> <li> <p>Choose the desired image to use by clicking the up arrow at the end of its row. It will be displayed in the \"Allocated\" section when selected.</p> <p></p> </li> <li> <p>Choose the desired instance size by clicking the up arrow at the end of its row. It will be displayed in the \"Allocated\" section when selected.</p> <p></p> </li> <li> <p>Choose a network in the same way as an image or instance size. Note that all nodes in a cluster must be on the same network.</p> <p></p> </li> <li> <p>Choose a security group in the same way as an image or instance size. Note that all nodes in a cluster must be in the same security group.</p> <p></p> </li> <li> <p>Choose the keypair in the same way as an image or instance size.</p> <p></p> </li> <li> <p>In the \"Configuration\" section, there is a \"Customisation Script\" section with a text box. This will be used to set your user data</p> <p></p> </li> <li> <p>When all options have been selected, press the \"Launch Instance\" button to launch. If the button is greyed out, then a mandatory setting has not been configured.</p> <p></p> </li> <li> <p>Go to the \"Instances\" page in the \"Compute\" section. The created node should be there and be finishing or have finished creation.</p> <p></p> </li> <li> <p>Click on the down arrow at the end of the instance row. This will bring up a drop-down menu.</p> </li> <li> <p>Select \"Associate Floating IP\", this will make the ip management window pop up.</p> <p></p> </li> <li> <p>Associate a floating IP, either by using an existing one or allocating a new one.</p> <ol> <li> <p>To use an existing floating IP:</p> <ol> <li> <p>Open the IP Address drop-down menu.</p> <p></p> </li> <li> <p>Select one of the IP Addresses.</p> </li> <li> <p>Click \"Associate\" to finish associating an IP.</p> </li> </ol> </li> <li> <p>To allocate a new floating IP:</p> <ol> <li> <p>Click the \"+\" next to the drop-down arrow to open the allocation menu.</p> <p></p> </li> <li> <p>Click \"Allocate IP\".</p> </li> </ol> </li> </ol> </li> <li> <p>Click \"Associate\" to finish associating an IP.</p> </li> </ol>"},{"location":"docs/flight-solo/cluster-build-methods/slurm-multinode-openstack/#launch-compute-nodes","title":"Launch Compute Nodes","text":""},{"location":"docs/flight-solo/cluster-build-methods/slurm-multinode-openstack/#prepare-user-data_1","title":"Prepare User Data","text":"<p>Setting up compute nodes is done slightly differently than a login node. The basic steps are the same except subnets, networks and security groups need to match the ones used for the login node.</p> <p>This is the smallest amount of cloud init data necessary. It allows the login node to find the compute nodes as long as they are on the same network, and ssh into them from the root user (which is necessary for setup). <pre><code>#cloud-config\nusers:\n  - default\n  - name: root\n    ssh_authorized_keys:\n      - &lt;Content of ~/.ssh/id_alcescluster.pub from root user on login node&gt;\n</code></pre></p> <p>Tip</p> <p>The above is not required if the <code>SHAREPUBKEY</code> option was provided to the login node. If this was the case then the <code>SERVER</code> option provided to the compute node will be enough to enable root access from the login node.</p> <p>There are several options that can be added to change how a compute node will contact nodes on startup.</p> <ul> <li>Sending to a specific server:<ul> <li>Instead of broadcasting across a range, add the line <code>SERVER=&lt;private server IP&gt;</code> to send to specifically that node, which would be your login node.</li> </ul> </li> <li>Add an auth key:<ul> <li>Add the line <code>AUTH_KEY=&lt;string&gt;</code>. This means that the compute node will send it's flight hunter packet with this key. This must match the auth key provided to your login node</li> </ul> </li> </ul> An example of all mentioned lines in a single cloud init script.<pre><code>#cloud-config\nwrite_files:\n  - content: |\n      SERVER=10.10.0.1\n      AUTH_KEY=banana\n    path: /opt/flight/cloudinit.in\n    permissions: '0600'\n    owner: root:root\nusers:\n  - default\n  - name: root\n    ssh_authorized_keys:\n      - &lt;Content of ~/.ssh/id_alcescluster.pub from root user on login node&gt;\n</code></pre> <p>Info</p> <p>More information on available user data options for Flight Solo via the user data documentation</p>"},{"location":"docs/flight-solo/cluster-build-methods/slurm-multinode-openstack/#deploy_1","title":"Deploy","text":"<ol> <li> <p>Go to the Openstack instances page.</p> <p></p> </li> <li> <p>Click \"Launch Instance\", and the instance creation window will pop up.</p> </li> <li> <p>Fill in the instance name, and set the number of instances to create, then click next.</p> <p></p> </li> <li> <p>Choose the desired image to use by clicking the up arrow at the end of its row. It will be displayed in the \"Allocated\" section when selected.</p> <p></p> </li> <li> <p>Choose the desired instance size by clicking the up arrow at the end of its row. It will be displayed in the \"Allocated\" section when selected.</p> <p></p> </li> <li> <p>Choose a network in the same way as an image or instance size. Note that this should be the same network as the login node.</p> <p></p> </li> <li> <p>Choose a security group in the same way as an image or instance size. Note that this should be the same security group as the login node.</p> <p></p> </li> <li> <p>Choose the keypair in the same way as an image or instance size.</p> <p></p> </li> <li> <p>In the \"Configuration\" section, there is a \"Customisation Script\" section with a text box. This will be used to set user data script prepared earlier</p> <p></p> </li> <li> <p>When all options have been selected, press the \"Launch Instance\" button to launch. If the button is greyed out, then a mandatory setting has not been configured.</p> <p></p> </li> </ol>"},{"location":"docs/flight-solo/cluster-build-methods/slurm-multinode-openstack/#general-configuration","title":"General Configuration","text":""},{"location":"docs/flight-solo/cluster-build-methods/slurm-multinode-openstack/#create-node-inventory","title":"Create Node Inventory","text":"<ol> <li> <p>Parse your node(s) with the command <code>flight hunter parse</code>.</p> <ol> <li> <p>This will display a list of hunted nodes, for example     <pre><code>[flight@login-node.novalocal ~]$ flight hunter parse\nSelect nodes: (Scroll for more nodes)\n\u2023 \u2b21 login-node.novalocal - 10.10.0.1\n  \u2b21 compute-node-1.novalocal - 10.10.101.1\n</code></pre></p> </li> <li> <p>Select the desired node to be parsed with Space, and you will be taken to the label editor     <pre><code>Choose label: login-node.novalocal\n</code></pre></p> </li> <li> <p>Here, you can edit the label like plain text     <pre><code>Choose label: login1\n</code></pre></p> <p>Tip</p> <p>You can clear the current node name by pressing Down in the label editor.</p> </li> <li> <p>When done editing, press Enter to save. The modified node label will appear next to the ip address and original node label.     <pre><code>Select nodes: login-node.novalocal - 10.10.0.1 (login1) (Scroll for more nodes)\n\u2023 \u2b22 login-node.novalocal - 10.10.0.1 (login1)\n  \u2b21 compute-node-1.novalocal - 10.10.101.1\n</code></pre></p> </li> <li> <p>From this point, you can either hit Enter to finish parsing and process the selected nodes, or continue changing nodes. Either way, you can return to this list by running <code>flight hunter parse</code>.</p> </li> <li> <p>Save the node inventory before moving on to the next step.</p> <p>Tip</p> <p>See <code>flight hunter parse -h</code> for more ways to parse nodes.</p> </li> </ol> </li> </ol>"},{"location":"docs/flight-solo/cluster-build-methods/slurm-multinode-openstack/#add-genders","title":"Add genders","text":"<ol> <li>Optionally, you may add genders to the newly parsed node. For example, in the case that the node should have the gender <code>cluster</code> and <code>all</code> then run the command:     <pre><code>flight hunter modify-groups --add cluster,all login1\n</code></pre></li> </ol>"},{"location":"docs/flight-solo/cluster-build-methods/slurm-multinode-openstack/#slurm-multinode-configuration","title":"SLURM Multinode Configuration","text":"<ol> <li> <p>Configure profile</p> <pre><code>flight profile configure\n</code></pre> <ol> <li>This brings up a UI, where several options need to be set. Use up and down arrow keys to scroll through options and enter to move to the next option. Options in brackets coloured yellow are the default options that will be applied if nothing is entered.<ul> <li>Cluster type: The type of cluster setup needed, in this case <code>Slurm Multinode</code>.</li> <li>Cluster name: The name of the cluster.</li> <li>Setup Multi User Environment with IPA?: Boolean value to determine whether to configure a multi-user environment with IPA. If set to true then the following will need to be filled in<ul> <li>IPA domain: The domain for the IPA server to use.</li> <li>IPA secure admin password: The password to be used by the <code>admin</code> user of the IPA installation to manage the server.</li> </ul> </li> <li>Default user: The user that you log in with.</li> <li>Set user password: Set a password to be used for the chosen default user.</li> <li>IP or FQDN for Web Access: As described here, this could be the public IP or public hostname.</li> <li>IP range of compute nodes: The IP range of the compute nodes used, remember to add the netmask. E.g. <code>172.31.16.0/20</code></li> </ul> </li> </ol> </li> <li> <p>Apply identities by running the command <code>flight profile apply</code></p> <ol> <li> <p>First apply an identity to the login node     <pre><code>flight profile apply login1 login\n</code></pre></p> </li> <li> <p>Wait for the login node identity to finish applying. You can check the status of all nodes with <code>flight profile list</code>.</p> <p>Tip</p> <p>You can watch the progress of the application with <code>flight profile view login1 --watch</code></p> </li> <li> <p>Apply an identity to the each of the compute nodes (in this example, genders-style syntax is used to apply to <code>node01</code> and <code>node02</code>)      <pre><code>flight profile apply node[01-02] compute\n</code></pre></p> <p>Tip</p> <p>You can check all available identities for the current profile with <code>flight profile identities</code></p> </li> </ol> </li> </ol> <p>Success</p> <p>Congratulations, you've now created a SLURM Multinode environment! Learn more about SLURM in the HPC Environment docs.</p>"},{"location":"docs/flight-solo/cluster-build-methods/slurm-multinode-openstack/#verifying-functionality","title":"Verifying Functionality","text":"<ol> <li> <p>Create a file called <code>simplejobscript.sh</code>, and copy this into it:     <pre><code>#!/bin/bash -l\necho \"Starting running on host $HOSTNAME\"\nsleep 30\necho \"Finished running - goodbye from $HOSTNAME\"\n</code></pre></p> </li> <li> <p>Run the script with <code>sbatch simplejobscript.sh</code>, and to test all your nodes try queuing up enough jobs that all nodes will have to run.</p> </li> <li> <p>In the directory that the job was submitted from there should be a <code>slurm-X.out</code> where <code>X</code> is the Job ID returned from the <code>sbatch</code> command. This will contain the echo messages from the script created in step 1 </p> </li> </ol>"},{"location":"docs/flight-solo/cluster-build-methods/slurm-standalone-alces-cloud/","title":"SLURM Standalone on Alces Cloud","text":""},{"location":"docs/flight-solo/cluster-build-methods/slurm-standalone-alces-cloud/#launch-login-node","title":"Launch Login Node","text":"CLIGUI <p>To set up a cluster, you will need to import a Flight Solo image.</p> <p>Before setting up a cluster on Alces Cloud, there are several required prerequisites:</p> <ul> <li>Your own keypair</li> <li>A network with a subnet and a router bridging the subnet to the external network</li> <li>A security group that allows traffic is given below (if creating the security group through the web interface then the \"Any\" protocol will need to be an \"Other Protocol\" rule with \"IP Protocol\" of <code>-1</code>)</li> </ul> Protocol Direction CIDR Port Range Any egress 0.0.0.0/0 any Any ingress Virtual Network CIDR any ICMP ingress 0.0.0.0/0 any SSH ingress 0.0.0.0/0 22 TCP ingress 0.0.0.0/0 80 TCP ingress 0.0.0.0/0 443 TCP ingress 0.0.0.0/0 5900-5903 <p>Note</p> <p>The \"Virtual Network CIDR\" is the subnet and netmask for the network that the nodes are using. For example, a node on the 11.11.11.0 network with a netmask of 255.255.255.0 would have a network CIDR of 11.11.11.0/24.</p> <ol> <li> <p>Launch a login node with a command similar to the following: </p> <pre><code>$ openstack server create --flavor p1.small \\\n                          --image \"Flight Solo VERSION\" \\\n                          --boot-from-volume 16 \\\n                          --network \"mycluster1-network\" \\\n                          --key-name \"MyKey\" \\\n                          --security-group \"mycluster1-sg\" \\\n                          --user-data myuserdata.yml \\\n                          login1\n</code></pre> <ul> <li> <p>Where:</p> <ul> <li><code>flavor</code> - Is the desired size of the instance</li> <li><code>image</code> - Is the Flight Solo image imported to Alces Cloud</li> <li><code>boot-from-volume</code> - Is the size of the system disk in GB</li> <li><code>network</code> - Is the name or ID of the network created for the cluster</li> <li><code>key-name</code> - Is the name of the SSH key to use</li> <li><code>security-group</code> - Is the name or ID of the security group created previously</li> <li><code>user-data</code> - Is the file containing cloud-init user-data (this is optional in standalone scenarios)</li> <li><code>login1</code> - Is the name of the system</li> </ul> </li> </ul> </li> <li> <p>Associate a floating IP, either by using an existing one or creating a new one</p> <ol> <li> <p>To use an existing floating IP</p> <ol> <li> <p>Identify the IP address of an available floating IP (<code>Port</code> will be <code>None</code>)</p> <pre><code>$ openstack floating ip list\n+---------------------+---------------------+------------------+---------------------+---------------------+-----------------------+\n| ID                  | Floating IP Address | Fixed IP Address | Port                | Floating Network    | Project               |\n+---------------------+---------------------+------------------+---------------------+---------------------+-----------------------+\n| 726318f4-4dbb-4d51- | 10.199.31.6         | None             | None                | c681d94b-e2ec-4b73- | dcd92da7538a4f64a42b0 |\n| b119-d9e53c47a9f5   |                     |                  |                     | 89bf-9943bcce3255   | d4d9ce8845f           |\n</code></pre> </li> <li> <p>Associate the floating IP with the instance</p> <pre><code>$ openstack server add floating ip login1 10.199.31.6\n</code></pre> </li> </ol> </li> <li> <p>To create a new floating IP </p> <ol> <li> <p>Create new floating IP and note the <code>floating_ip_address</code> </p> <pre><code>$ openstack floating ip create external1\n</code></pre> </li> <li> <p>Associate the floating IP with the instance (using the <code>floating_ip_address</code> from the previous output)</p> <pre><code>$ openstack server add floating ip login1 10.199.31.212\n</code></pre> </li> </ol> </li> </ol> </li> </ol> <p>Further detail on collecting the information from the above can be found in the Alces Cloud documentation.</p> <p>To set up a cluster, you will need to import a Flight Solo image.</p> <p>Before setting up a cluster on Alces Cloud, there are several required prerequisites:</p> <ul> <li>Your own keypair</li> <li>A network with a subnet and a router bridging the subnet to the external network</li> <li>A security group that allows traffic is given below (if creating the security group through the web interface then the \"Any\" protocol will need to be an \"Other Protocol\" rule with \"IP Protocol\" of <code>-1</code>)</li> </ul> Protocol Direction CIDR Port Range Any egress 0.0.0.0/0 any Any ingress Virtual Network CIDR any ICMP ingress 0.0.0.0/0 any SSH ingress 0.0.0.0/0 22 TCP ingress 0.0.0.0/0 80 TCP ingress 0.0.0.0/0 443 TCP ingress 0.0.0.0/0 5900-5903 <p>Note</p> <p>The \"Virtual Network CIDR\" is the subnet and netmask for the network that the nodes are using. For example, a node on the 11.11.11.0 network with a netmask of 255.255.255.0 would have a network CIDR of 11.11.11.0/24.</p> <p>To set up a cluster:</p> <ol> <li> <p>Go to the Alces Cloud instances page.</p> <p></p> </li> <li> <p>Click \"Launch Instance\", and the instance creation window will pop up.</p> </li> <li> <p>Fill in the instance name, and leave the number of instances as 1, then click next.</p> <p></p> </li> <li> <p>Choose the desired image to use by clicking the up arrow at the end of its row. It will be displayed in the \"Allocated\" section when selected.</p> <p></p> </li> <li> <p>Choose the desired instance size by clicking the up arrow at the end of its row. It will be displayed in the \"Allocated\" section when selected.</p> <p></p> </li> <li> <p>Choose a network in the same way as an image or instance size. Note that all nodes in a cluster must be on the same network.</p> <p></p> </li> <li> <p>Choose a security group in the same way as an image or instance size. Note that all nodes in a cluster must be in the same security group.</p> <p></p> </li> <li> <p>Choose the keypair in the same way as an image or instance size.</p> <p></p> </li> <li> <p>In the \"Configuration\" section, there is a \"Customisation Script\" section with a text box. This will be used to set your user data</p> <p></p> </li> <li> <p>When all options have been selected, press the \"Launch Instance\" button to launch. If the button is greyed out, then a mandatory setting has not been configured.</p> <p></p> </li> <li> <p>Go to the \"Instances\" page in the \"Compute\" section. The created node should be there and be finishing or have finished creation.</p> <p></p> </li> <li> <p>Click on the down arrow at the end of the instance row. This will bring up a drop-down menu.</p> </li> <li> <p>Select \"Associate Floating IP\", this will make the ip management window pop up.</p> <p></p> </li> <li> <p>Associate a floating IP, either by using an existing one or allocating a new one.</p> <ol> <li> <p>To use an existing floating IP:</p> <ol> <li> <p>Open the IP Address drop-down menu.</p> <p></p> </li> <li> <p>Select one of the IP Addresses.</p> </li> <li> <p>Click \"Associate\" to finish associating an IP.</p> </li> </ol> </li> <li> <p>To allocate a new floating IP:</p> <ol> <li> <p>Click the \"+\" next to the drop-down arrow to open the allocation menu.</p> <p></p> </li> <li> <p>Click \"Allocate IP\".</p> </li> </ol> </li> </ol> </li> <li> <p>Click \"Associate\" to finish associating an IP.</p> </li> </ol> <p>Further detail on collecting the information from the above can be found in the Alces Cloud documentation.</p>"},{"location":"docs/flight-solo/cluster-build-methods/slurm-standalone-alces-cloud/#general-configuration","title":"General Configuration","text":""},{"location":"docs/flight-solo/cluster-build-methods/slurm-standalone-alces-cloud/#create-node-inventory","title":"Create Node Inventory","text":"<ol> <li> <p>Parse your node(s) with the command <code>flight hunter parse</code>.</p> <ol> <li> <p>This will display a list of hunted nodes, for example     <pre><code>[flight@login-node.novalocal ~]$ flight hunter parse\nSelect nodes: (Scroll for more nodes)\n\u2023 \u2b21 login-node.novalocal - 10.10.0.1\n  \u2b21 compute-node-1.novalocal - 10.10.101.1\n</code></pre></p> </li> <li> <p>Select the desired node to be parsed with Space, and you will be taken to the label editor     <pre><code>Choose label: login-node.novalocal\n</code></pre></p> </li> <li> <p>Here, you can edit the label like plain text     <pre><code>Choose label: login1\n</code></pre></p> <p>Tip</p> <p>You can clear the current node name by pressing Down in the label editor.</p> </li> <li> <p>When done editing, press Enter to save. The modified node label will appear next to the ip address and original node label.     <pre><code>Select nodes: login-node.novalocal - 10.10.0.1 (login1) (Scroll for more nodes)\n\u2023 \u2b22 login-node.novalocal - 10.10.0.1 (login1)\n  \u2b21 compute-node-1.novalocal - 10.10.101.1\n</code></pre></p> </li> <li> <p>From this point, you can either hit Enter to finish parsing and process the selected nodes, or continue changing nodes. Either way, you can return to this list by running <code>flight hunter parse</code>.</p> </li> <li> <p>Save the node inventory before moving on to the next step.</p> <p>Tip</p> <p>See <code>flight hunter parse -h</code> for more ways to parse nodes.</p> </li> </ol> </li> </ol>"},{"location":"docs/flight-solo/cluster-build-methods/slurm-standalone-alces-cloud/#add-genders","title":"Add genders","text":"<ol> <li>Optionally, you may add genders to the newly parsed node. For example, in the case that the node should have the gender <code>cluster</code> and <code>all</code> then run the command:     <pre><code>flight hunter modify-groups --add cluster,all login1\n</code></pre></li> </ol>"},{"location":"docs/flight-solo/cluster-build-methods/slurm-standalone-alces-cloud/#slurm-standalone-configuration","title":"SLURM Standalone Configuration","text":"<ol> <li> <p>Configure profile</p> <pre><code>flight profile configure\n</code></pre> <ol> <li>This brings up a UI, where several options need to be set. Use up and down arrow keys to scroll through options and enter to move to the next option. Options in brackets coloured yellow are the default options that will be applied if nothing is entered.<ul> <li>Cluster type: The type of cluster setup needed, in this case select <code>Slurm Standalone</code>.</li> <li>Cluster name: The name of the cluster.</li> <li>Default user: The user that you log in with.</li> <li>Set user password: Set a password to be used for the chosen default user.</li> <li>IP or FQDN for Web Access: As described here, this could be the public IP or public hostname.</li> </ul> </li> </ol> </li> <li> <p>Apply an identity by running the command <code>flight profile apply</code>, E.g.     <pre><code>flight profile apply login1 all-in-one\n</code></pre></p> <p>Tip</p> <p>You can check all available identities for the current profile with <code>flight profile identities</code></p> </li> <li> <p>Wait for the identity to finish applying. You can check the status of all nodes with <code>flight profile list</code>.</p> <p>Tip</p> <p>You can watch the progress of the application with <code>flight profile view login1 --watch</code></p> </li> </ol> <p>Success</p> <p>Congratulations, you've now created a SLURM Standalone environment! Learn more about SLURM in the HPC Environment docs.</p>"},{"location":"docs/flight-solo/cluster-build-methods/slurm-standalone-alces-cloud/#verifying-functionality","title":"Verifying Functionality","text":"<ol> <li> <p>Create a file called <code>simplejobscript.sh</code>, and copy this into it:     <pre><code>#!/bin/bash -l\necho \"Starting running on host $HOSTNAME\"\nsleep 30\necho \"Finished running - goodbye from $HOSTNAME\"\n</code></pre></p> </li> <li> <p>Run the script with <code>sbatch simplejobscript.sh</code>, and to test all your nodes try queuing up enough jobs that all nodes will have to run.</p> </li> <li> <p>In the directory that the job was submitted from there should be a <code>slurm-X.out</code> where <code>X</code> is the Job ID returned from the <code>sbatch</code> command. This will contain the echo messages from the script created in step 1 </p> </li> </ol>"},{"location":"docs/flight-solo/cluster-build-methods/slurm-standalone-aws/","title":"SLURM Standalone on AWS","text":""},{"location":"docs/flight-solo/cluster-build-methods/slurm-standalone-aws/#launch-login-node","title":"Launch Login Node","text":"AWS MarketplaceAWS Imported <p>To set up a cluster, you will need to import a Flight Solo image.</p> <ol> <li> <p>Go the EC2 instance console</p> <p></p> </li> <li> <p>Click \"Launch\" to go to the EC2 instance setup page.</p> <p></p> </li> <li> <p>Set the number of instances to 1, and name of instance to something descriptive.</p> <p></p> </li> <li> <p>Confirm that the region(top right, next to username) is correct.</p> <p></p> </li> <li> <p>In the \"Application and OS Images\" section choose the \"My AMIs\" tab and select your imported solo AMI.</p> <p></p> </li> <li> <p>In the \"Instance type\" section, choose the required instance size.</p> <p></p> </li> <li> <p>In the \"Keypair\" section, select a keypair to use. It is good practice to use the same keypair for the login and compute nodes.</p> <p></p> </li> <li> <p>In the \"Network settings\" sections, click the \"Edit\" button to set the network and subnet. Remember what these are, as they should be the same for any associated compute nodes.</p> <p></p> <p></p> </li> <li> <p>Another thing needed is a security group to associate with all nodes on the cluster. It is recommended to use a security group with rules limiting traffic through:</p> <ul> <li>HTTP</li> <li>HTTPS</li> <li>SSH</li> <li>Port 8888</li> <li>Ports 5900 - 5903</li> <li>All traffic from within the security group should be allowed. (This rule can only be added after creation)</li> </ul> <p>Note</p> <p>If you already have a security group which does this, use it here and make sure to use it again for the compute nodes. Otherwise, a security group can be made from the launch page, or through the security groups page</p> <p>Describing exactly how to create a security group is out of scope for this documentation, but covered by the AWS documentation.</p> <p>However, here is an example security group that might be used for a Flight Solo cluster:</p> <p></p> </li> <li> <p>After a security group has been made, click \"Choose Existing\" select it from the drop down menu.</p> </li> <li> <p>In the \"Configure Storage\" section, allocate as much memory as needed. 8GB is the minimum required for Flight Solo, so it is likely the compute nodes will not need much more than that, as the login node hosts most data.</p> <p></p> </li> <li> <p>Finally, click \"Launch Instance\".</p> </li> </ol> <ol> <li> <p>Find the Flight Solo image here or by searching the marketplace for \"Flight Solo\".</p> </li> <li> <p>Click \"Continue to Subscribe\"</p> <p></p> </li> <li> <p>Read the terms and conditions, then click \"Continue to Configuration\"</p> <p></p> </li> <li> <p>Configure region, software version (if unsure use the latest), and fulfillment option (if unsure use the default). Then click \"Continue to Launch\". Make sure the region is the same for all nodes to be used in a cluster.</p> <p></p> </li> <li> <p>Click on \"Usage Instructions\" to see some instructions on how to get started, and a link to this documentation.</p> <p></p> </li> <li> <p>Select the \"Launch from Website\" action.</p> <p></p> </li> <li> <p>Choose an instance type to use.</p> <p></p> </li> <li> <p>Choose VPC settings. Remember what VPC was used to create this instance, as it should also be used for any associated compute nodes.</p> <p></p> </li> <li> <p>Choose a subnet. Remember what subnet was used to create this instance, as it should also be used for any associated compute nodes.</p> <p></p> </li> <li> <p>A security group is needed to associate with all nodes on the cluster. It is recommended to use a security group with rules limiting traffic through:</p> <ul> <li>HTTP</li> <li>HTTPS</li> <li>SSH</li> <li>Port 8888</li> <li>Ports 5900 - 5903</li> <li>All traffic from within the security group should be allowed. (This rule can only be added after creation)</li> </ul> <p>Note</p> <p>If you already have a security group which does this, use it here and make sure to use it again for the compute nodes. Otherwise, a security group can be made from the launch page, or through the security groups page</p> <p>Describing exactly how to create a security group is out of scope for this documentation, but covered by the AWS documentation.</p> <p>However, here is an example security group that might be used for a Flight Solo cluster:</p> <p></p> <p>Tip</p> <p>The seller's settings (shown below) can be used as a reference for creating a security group.</p> <p></p> </li> <li> <p>After a security group has been made, click \"Select existing security group\" select it from the drop down menu.</p> <p></p> </li> <li> <p>Choose what key pair to use. It is good practice for this to be the same on all nodes in a cluster.</p> <p></p> </li> <li> <p>Click Launch</p> <p></p> </li> </ol>"},{"location":"docs/flight-solo/cluster-build-methods/slurm-standalone-aws/#general-configuration","title":"General Configuration","text":""},{"location":"docs/flight-solo/cluster-build-methods/slurm-standalone-aws/#create-node-inventory","title":"Create Node Inventory","text":"<ol> <li> <p>Parse your node(s) with the command <code>flight hunter parse</code>.</p> <ol> <li> <p>This will display a list of hunted nodes, for example     <pre><code>[flight@login-node.novalocal ~]$ flight hunter parse\nSelect nodes: (Scroll for more nodes)\n\u2023 \u2b21 login-node.novalocal - 10.10.0.1\n  \u2b21 compute-node-1.novalocal - 10.10.101.1\n</code></pre></p> </li> <li> <p>Select the desired node to be parsed with Space, and you will be taken to the label editor     <pre><code>Choose label: login-node.novalocal\n</code></pre></p> </li> <li> <p>Here, you can edit the label like plain text     <pre><code>Choose label: login1\n</code></pre></p> <p>Tip</p> <p>You can clear the current node name by pressing Down in the label editor.</p> </li> <li> <p>When done editing, press Enter to save. The modified node label will appear next to the ip address and original node label.     <pre><code>Select nodes: login-node.novalocal - 10.10.0.1 (login1) (Scroll for more nodes)\n\u2023 \u2b22 login-node.novalocal - 10.10.0.1 (login1)\n  \u2b21 compute-node-1.novalocal - 10.10.101.1\n</code></pre></p> </li> <li> <p>From this point, you can either hit Enter to finish parsing and process the selected nodes, or continue changing nodes. Either way, you can return to this list by running <code>flight hunter parse</code>.</p> </li> <li> <p>Save the node inventory before moving on to the next step.</p> <p>Tip</p> <p>See <code>flight hunter parse -h</code> for more ways to parse nodes.</p> </li> </ol> </li> </ol>"},{"location":"docs/flight-solo/cluster-build-methods/slurm-standalone-aws/#add-genders","title":"Add genders","text":"<ol> <li>Optionally, you may add genders to the newly parsed node. For example, in the case that the node should have the gender <code>cluster</code> and <code>all</code> then run the command:     <pre><code>flight hunter modify-groups --add cluster,all login1\n</code></pre></li> </ol>"},{"location":"docs/flight-solo/cluster-build-methods/slurm-standalone-aws/#slurm-standalone-configuration","title":"SLURM Standalone Configuration","text":"<ol> <li> <p>Configure profile</p> <pre><code>flight profile configure\n</code></pre> <ol> <li>This brings up a UI, where several options need to be set. Use up and down arrow keys to scroll through options and enter to move to the next option. Options in brackets coloured yellow are the default options that will be applied if nothing is entered.<ul> <li>Cluster type: The type of cluster setup needed, in this case select <code>Slurm Standalone</code>.</li> <li>Cluster name: The name of the cluster.</li> <li>Default user: The user that you log in with.</li> <li>Set user password: Set a password to be used for the chosen default user.</li> <li>IP or FQDN for Web Access: As described here, this could be the public IP or public hostname.</li> </ul> </li> </ol> </li> <li> <p>Apply an identity by running the command <code>flight profile apply</code>, E.g.     <pre><code>flight profile apply login1 all-in-one\n</code></pre></p> <p>Tip</p> <p>You can check all available identities for the current profile with <code>flight profile identities</code></p> </li> <li> <p>Wait for the identity to finish applying. You can check the status of all nodes with <code>flight profile list</code>.</p> <p>Tip</p> <p>You can watch the progress of the application with <code>flight profile view login1 --watch</code></p> </li> </ol> <p>Success</p> <p>Congratulations, you've now created a SLURM Standalone environment! Learn more about SLURM in the HPC Environment docs.</p>"},{"location":"docs/flight-solo/cluster-build-methods/slurm-standalone-aws/#verifying-functionality","title":"Verifying Functionality","text":"<ol> <li> <p>Create a file called <code>simplejobscript.sh</code>, and copy this into it:     <pre><code>#!/bin/bash -l\necho \"Starting running on host $HOSTNAME\"\nsleep 30\necho \"Finished running - goodbye from $HOSTNAME\"\n</code></pre></p> </li> <li> <p>Run the script with <code>sbatch simplejobscript.sh</code>, and to test all your nodes try queuing up enough jobs that all nodes will have to run.</p> </li> <li> <p>In the directory that the job was submitted from there should be a <code>slurm-X.out</code> where <code>X</code> is the Job ID returned from the <code>sbatch</code> command. This will contain the echo messages from the script created in step 1 </p> </li> </ol>"},{"location":"docs/flight-solo/cluster-build-methods/slurm-standalone-azure/","title":"SLURM Standalone on Azure","text":""},{"location":"docs/flight-solo/cluster-build-methods/slurm-standalone-azure/#launch-login-node","title":"Launch Login Node","text":"<p>To set up a cluster, you will need to import a Flight Solo image.</p> <ol> <li> <p>Go to the Microsoft Azure portal.</p> <p></p> </li> <li> <p>Go to Virtual Machines, and click \"Create\".</p> <p></p> </li> <li> <p>Select \"Azure virtual machine\", which will load this page:</p> <p></p> </li> <li> <p>On the Basics page:</p> <ol> <li>Set Subscription to your subscription type.</li> <li>Set Resource Group to your desired resource group (where the vm will be kept after creation).</li> <li>Set Virtual machine name to any suitable name. (<code>-</code> does not work in a name)</li> <li>Set Image to the imported Flight Solo Image.<ol> <li>It may be necessary to open the drop-down and/or see all images in order to find the imported image.     </li> <li>Scroll down to see more options     </li> </ol> </li> <li>Set Size to your choice of size.</li> <li>Set Authentication type to <code>SSH public key</code></li> <li>Set Username to any suitable username.</li> <li>Set SSH public key source to the most suitable option, but remember what key was used if creating compute nodes later.</li> <li>Fill in the Key pair name/Stored key/Use existing key as appropriate to the chosen public key source.</li> <li>Allow traffic to selected ports, and select <code>SSH(22)</code>, <code>HTTP(80)</code> and <code>HTTPS(443)</code> as the allowed ports.</li> <li>Set the most appropriate license type.</li> </ol> </li> <li> <p>Continuing on to the next page, Disks, all necessary details should already be filled out, so this page can be skipped (unless you know what you want to change). However, it is recommended to select Delete with VM.</p> <p></p> </li> <li> <p>Go on to the networking tab and fill out the necessary options.</p> <p></p> <ol> <li>Set Virtual Network or create a new one by pressing \"Create new\" and setting a name. Remember what this is for if you create compute nodes.</li> <li>Set Subnet to one of the options in the drop-down menu, if it isn't already set. Remember what this is for if you create compute nodes.</li> <li>Set Public IP to an existing public IP or create a new one by pressing \"Create new\" and setting a name.</li> <li>Set NIC network security group to \"Advanced\", and press \"Create new\" to create a new security group.     <ol> <li>Click on \"Add an inbound rule\" to open the inbound rule creator     </li> <li>Create rules to allow <code>HTTP</code>, <code>HTTPS</code> and <code>SSH</code> traffic from your IP address to the security group.</li> <li>When complete, press \"OK\" at the bottom left of the screen to return to image creation.</li> </ol> </li> </ol> </li> <li> <p>The Management, Monitoring and Tags tabs have more options that aren't necessary for setup. Skip to the tab Advanced</p> </li> <li> <p>In the Custom data and cloud init section, there is a text box. This is where your user data can be specified</p> <p></p> </li> <li> <p>Azure will take some time to review your settings. If there are no issues click \"Create\" to finish creation.</p> </li> </ol>"},{"location":"docs/flight-solo/cluster-build-methods/slurm-standalone-azure/#general-configuration","title":"General Configuration","text":""},{"location":"docs/flight-solo/cluster-build-methods/slurm-standalone-azure/#create-node-inventory","title":"Create Node Inventory","text":"<ol> <li> <p>Parse your node(s) with the command <code>flight hunter parse</code>.</p> <ol> <li> <p>This will display a list of hunted nodes, for example     <pre><code>[flight@login-node.novalocal ~]$ flight hunter parse\nSelect nodes: (Scroll for more nodes)\n\u2023 \u2b21 login-node.novalocal - 10.10.0.1\n  \u2b21 compute-node-1.novalocal - 10.10.101.1\n</code></pre></p> </li> <li> <p>Select the desired node to be parsed with Space, and you will be taken to the label editor     <pre><code>Choose label: login-node.novalocal\n</code></pre></p> </li> <li> <p>Here, you can edit the label like plain text     <pre><code>Choose label: login1\n</code></pre></p> <p>Tip</p> <p>You can clear the current node name by pressing Down in the label editor.</p> </li> <li> <p>When done editing, press Enter to save. The modified node label will appear next to the ip address and original node label.     <pre><code>Select nodes: login-node.novalocal - 10.10.0.1 (login1) (Scroll for more nodes)\n\u2023 \u2b22 login-node.novalocal - 10.10.0.1 (login1)\n  \u2b21 compute-node-1.novalocal - 10.10.101.1\n</code></pre></p> </li> <li> <p>From this point, you can either hit Enter to finish parsing and process the selected nodes, or continue changing nodes. Either way, you can return to this list by running <code>flight hunter parse</code>.</p> </li> <li> <p>Save the node inventory before moving on to the next step.</p> <p>Tip</p> <p>See <code>flight hunter parse -h</code> for more ways to parse nodes.</p> </li> </ol> </li> </ol>"},{"location":"docs/flight-solo/cluster-build-methods/slurm-standalone-azure/#add-genders","title":"Add genders","text":"<ol> <li>Optionally, you may add genders to the newly parsed node. For example, in the case that the node should have the gender <code>cluster</code> and <code>all</code> then run the command:     <pre><code>flight hunter modify-groups --add cluster,all login1\n</code></pre></li> </ol>"},{"location":"docs/flight-solo/cluster-build-methods/slurm-standalone-azure/#slurm-standalone-configuration","title":"SLURM Standalone Configuration","text":"<ol> <li> <p>Configure profile</p> <pre><code>flight profile configure\n</code></pre> <ol> <li>This brings up a UI, where several options need to be set. Use up and down arrow keys to scroll through options and enter to move to the next option. Options in brackets coloured yellow are the default options that will be applied if nothing is entered.<ul> <li>Cluster type: The type of cluster setup needed, in this case select <code>Slurm Standalone</code>.</li> <li>Cluster name: The name of the cluster.</li> <li>Default user: The user that you log in with.</li> <li>Set user password: Set a password to be used for the chosen default user.</li> <li>IP or FQDN for Web Access: As described here, this could be the public IP or public hostname.</li> </ul> </li> </ol> </li> <li> <p>Apply an identity by running the command <code>flight profile apply</code>, E.g.     <pre><code>flight profile apply login1 all-in-one\n</code></pre></p> <p>Tip</p> <p>You can check all available identities for the current profile with <code>flight profile identities</code></p> </li> <li> <p>Wait for the identity to finish applying. You can check the status of all nodes with <code>flight profile list</code>.</p> <p>Tip</p> <p>You can watch the progress of the application with <code>flight profile view login1 --watch</code></p> </li> </ol> <p>Success</p> <p>Congratulations, you've now created a SLURM Standalone environment! Learn more about SLURM in the HPC Environment docs.</p>"},{"location":"docs/flight-solo/cluster-build-methods/slurm-standalone-azure/#verifying-functionality","title":"Verifying Functionality","text":"<ol> <li> <p>Create a file called <code>simplejobscript.sh</code>, and copy this into it:     <pre><code>#!/bin/bash -l\necho \"Starting running on host $HOSTNAME\"\nsleep 30\necho \"Finished running - goodbye from $HOSTNAME\"\n</code></pre></p> </li> <li> <p>Run the script with <code>sbatch simplejobscript.sh</code>, and to test all your nodes try queuing up enough jobs that all nodes will have to run.</p> </li> <li> <p>In the directory that the job was submitted from there should be a <code>slurm-X.out</code> where <code>X</code> is the Job ID returned from the <code>sbatch</code> command. This will contain the echo messages from the script created in step 1 </p> </li> </ol>"},{"location":"docs/flight-solo/cluster-build-methods/slurm-standalone-openstack/","title":"SLURM Standalone on OpenStack","text":""},{"location":"docs/flight-solo/cluster-build-methods/slurm-standalone-openstack/#launch-login-node","title":"Launch Login Node","text":"<p>To set up a cluster, you will need to import a Flight Solo image.</p> <p>Before setting up a cluster on Openstack, there are several required prerequisites:</p> <ul> <li>Your own keypair</li> <li>A network</li> <li>A router<ul> <li>With an interface both on the External Gateway network and an Internal Interface on the previously created network</li> </ul> </li> <li>A security group that allows traffic is given below (if creating the security group through the web interface then the \"Any\" protocol will need to be an \"Other Protocol\" rule with \"IP Protocol\" of <code>-1</code>)</li> </ul> Protocol Direction CIDR Port Range Any egress 0.0.0.0/0 any Any ingress Virtual Network CIDR any ICMP ingress 0.0.0.0/0 any SSH ingress 0.0.0.0/0 22 TCP ingress 0.0.0.0/0 80 TCP ingress 0.0.0.0/0 443 TCP ingress 0.0.0.0/0 5900-5903 <p>Note</p> <p>The \"Virtual Network CIDR\" is the subnet and netmask for the network that the nodes are using. For example, a node on the 11.11.11.0 network with a netmask of 255.255.255.0 would have a network CIDR of 11.11.11.0/24.</p> <p>The documentation includes instructions for importing an image to Openstack, and guides for setting up the other prerequisites can be found in the Openstack documentation</p> <p>To set up a cluster:</p> <ol> <li> <p>Go to the Openstack instances page.</p> <p></p> </li> <li> <p>Click \"Launch Instance\", and the instance creation window will pop up.</p> </li> <li> <p>Fill in the instance name, and leave the number of instances as 1, then click next.</p> <p></p> </li> <li> <p>Choose the desired image to use by clicking the up arrow at the end of its row. It will be displayed in the \"Allocated\" section when selected.</p> <p></p> </li> <li> <p>Choose the desired instance size by clicking the up arrow at the end of its row. It will be displayed in the \"Allocated\" section when selected.</p> <p></p> </li> <li> <p>Choose a network in the same way as an image or instance size. Note that all nodes in a cluster must be on the same network.</p> <p></p> </li> <li> <p>Choose a security group in the same way as an image or instance size. Note that all nodes in a cluster must be in the same security group.</p> <p></p> </li> <li> <p>Choose the keypair in the same way as an image or instance size.</p> <p></p> </li> <li> <p>In the \"Configuration\" section, there is a \"Customisation Script\" section with a text box. This will be used to set your user data</p> <p></p> </li> <li> <p>When all options have been selected, press the \"Launch Instance\" button to launch. If the button is greyed out, then a mandatory setting has not been configured.</p> <p></p> </li> <li> <p>Go to the \"Instances\" page in the \"Compute\" section. The created node should be there and be finishing or have finished creation.</p> <p></p> </li> <li> <p>Click on the down arrow at the end of the instance row. This will bring up a drop-down menu.</p> </li> <li> <p>Select \"Associate Floating IP\", this will make the ip management window pop up.</p> <p></p> </li> <li> <p>Associate a floating IP, either by using an existing one or allocating a new one.</p> <ol> <li> <p>To use an existing floating IP:</p> <ol> <li> <p>Open the IP Address drop-down menu.</p> <p></p> </li> <li> <p>Select one of the IP Addresses.</p> </li> <li> <p>Click \"Associate\" to finish associating an IP.</p> </li> </ol> </li> <li> <p>To allocate a new floating IP:</p> <ol> <li> <p>Click the \"+\" next to the drop-down arrow to open the allocation menu.</p> <p></p> </li> <li> <p>Click \"Allocate IP\".</p> </li> </ol> </li> </ol> </li> <li> <p>Click \"Associate\" to finish associating an IP.</p> </li> </ol>"},{"location":"docs/flight-solo/cluster-build-methods/slurm-standalone-openstack/#general-configuration","title":"General Configuration","text":""},{"location":"docs/flight-solo/cluster-build-methods/slurm-standalone-openstack/#create-node-inventory","title":"Create Node Inventory","text":"<ol> <li> <p>Parse your node(s) with the command <code>flight hunter parse</code>.</p> <ol> <li> <p>This will display a list of hunted nodes, for example     <pre><code>[flight@login-node.novalocal ~]$ flight hunter parse\nSelect nodes: (Scroll for more nodes)\n\u2023 \u2b21 login-node.novalocal - 10.10.0.1\n  \u2b21 compute-node-1.novalocal - 10.10.101.1\n</code></pre></p> </li> <li> <p>Select the desired node to be parsed with Space, and you will be taken to the label editor     <pre><code>Choose label: login-node.novalocal\n</code></pre></p> </li> <li> <p>Here, you can edit the label like plain text     <pre><code>Choose label: login1\n</code></pre></p> <p>Tip</p> <p>You can clear the current node name by pressing Down in the label editor.</p> </li> <li> <p>When done editing, press Enter to save. The modified node label will appear next to the ip address and original node label.     <pre><code>Select nodes: login-node.novalocal - 10.10.0.1 (login1) (Scroll for more nodes)\n\u2023 \u2b22 login-node.novalocal - 10.10.0.1 (login1)\n  \u2b21 compute-node-1.novalocal - 10.10.101.1\n</code></pre></p> </li> <li> <p>From this point, you can either hit Enter to finish parsing and process the selected nodes, or continue changing nodes. Either way, you can return to this list by running <code>flight hunter parse</code>.</p> </li> <li> <p>Save the node inventory before moving on to the next step.</p> <p>Tip</p> <p>See <code>flight hunter parse -h</code> for more ways to parse nodes.</p> </li> </ol> </li> </ol>"},{"location":"docs/flight-solo/cluster-build-methods/slurm-standalone-openstack/#add-genders","title":"Add genders","text":"<ol> <li>Optionally, you may add genders to the newly parsed node. For example, in the case that the node should have the gender <code>cluster</code> and <code>all</code> then run the command:     <pre><code>flight hunter modify-groups --add cluster,all login1\n</code></pre></li> </ol>"},{"location":"docs/flight-solo/cluster-build-methods/slurm-standalone-openstack/#slurm-standalone-configuration","title":"SLURM Standalone Configuration","text":"<ol> <li> <p>Configure profile</p> <pre><code>flight profile configure\n</code></pre> <ol> <li>This brings up a UI, where several options need to be set. Use up and down arrow keys to scroll through options and enter to move to the next option. Options in brackets coloured yellow are the default options that will be applied if nothing is entered.<ul> <li>Cluster type: The type of cluster setup needed, in this case select <code>Slurm Standalone</code>.</li> <li>Cluster name: The name of the cluster.</li> <li>Default user: The user that you log in with.</li> <li>Set user password: Set a password to be used for the chosen default user.</li> <li>IP or FQDN for Web Access: As described here, this could be the public IP or public hostname.</li> </ul> </li> </ol> </li> <li> <p>Apply an identity by running the command <code>flight profile apply</code>, E.g.     <pre><code>flight profile apply login1 all-in-one\n</code></pre></p> <p>Tip</p> <p>You can check all available identities for the current profile with <code>flight profile identities</code></p> </li> <li> <p>Wait for the identity to finish applying. You can check the status of all nodes with <code>flight profile list</code>.</p> <p>Tip</p> <p>You can watch the progress of the application with <code>flight profile view login1 --watch</code></p> </li> </ol> <p>Success</p> <p>Congratulations, you've now created a SLURM Standalone environment! Learn more about SLURM in the HPC Environment docs.</p>"},{"location":"docs/flight-solo/cluster-build-methods/slurm-standalone-openstack/#verifying-functionality","title":"Verifying Functionality","text":"<ol> <li> <p>Create a file called <code>simplejobscript.sh</code>, and copy this into it:     <pre><code>#!/bin/bash -l\necho \"Starting running on host $HOSTNAME\"\nsleep 30\necho \"Finished running - goodbye from $HOSTNAME\"\n</code></pre></p> </li> <li> <p>Run the script with <code>sbatch simplejobscript.sh</code>, and to test all your nodes try queuing up enough jobs that all nodes will have to run.</p> </li> <li> <p>In the directory that the job was submitted from there should be a <code>slurm-X.out</code> where <code>X</code> is the Job ID returned from the <code>sbatch</code> command. This will contain the echo messages from the script created in step 1 </p> </li> </ol>"},{"location":"docs/flight-solo/cluster-build-methods/workflows/","title":"Cluster Build Workflows","text":"<p>This section details specific Cluster Build Workflows. Whereas the General Methods provide generalised instructions for deployment, the workflows detailed here are exact, reproducible examples of deployments that can be followed closely. </p> <p>The General Methods provide detailed, step-by-step guidance to assist with utilising various platforms and tools. The workflows here are written for those who know what they are doing. Any further assistance that is required with understanding steps here is likely covered in the General Methods of cluster building.</p>"},{"location":"docs/flight-solo/cluster-build-methods/workflows/kubernetes-aws-auto-apply/","title":"Kubernetes on AWS with Auto-Applying","text":""},{"location":"docs/flight-solo/cluster-build-methods/workflows/kubernetes-aws-auto-apply/#overview","title":"Overview","text":"<p>This workflow demonstrates the creation of a multinode Kubernetes cluster on AWS which utilises Flight Solo to automatically configure nodes as workers in the Kubernetes cluster upon boot.</p>"},{"location":"docs/flight-solo/cluster-build-methods/workflows/kubernetes-aws-auto-apply/#prepare-network","title":"Prepare Network","text":"<ol> <li>Create a VPC with<ol> <li>Name: <code>kubecluster1</code></li> </ol> </li> <li>In \"VPC and More\"<ol> <li>IPv4 Subnet Block: 10.10.0.0/16</li> <li>Number of Availability Zones: 1</li> <li>Public Subnets: 1</li> <li>Private Subnets: 0</li> <li>VPC Endpoints: None</li> </ol> </li> <li>Create a Security Group with<ol> <li>Name: <code>kubecluster1-sg</code></li> <li>Description: \"Security group for <code>kubecluster1</code> cluster\"</li> <li>VPC: <code>kubecluster1</code></li> </ol> </li> <li>Inbound Rules<ol> <li>\"SSH\" from \"Anywhere-IPv4\"</li> <li>\"HTTP\" from \"Anywhere-IPv4\"</li> <li>\"HTTPS\" from \"Anywhere-IPv4\"</li> <li>\"All Traffic\" from 10.10.0.0/16</li> </ol> </li> </ol>"},{"location":"docs/flight-solo/cluster-build-methods/workflows/kubernetes-aws-auto-apply/#build-login-node","title":"Build Login Node","text":"<p>Launch the Flight Solo image in AWS marketplace</p> <ol> <li>Select \"Launch from EC2\"</li> <li>Instance Type: t3.2xlarge</li> <li>Click \"Edit\" on Network Settings and</li> <li>VPC: <code>kubecluster1</code></li> <li>Auto-assign public IP: Enable</li> <li>Select existing security group: <code>kubecluster1-sg</code></li> <li>Set root volume size to at least 20GB</li> <li>Under Advanced Details -&gt; User Data, add this:     <pre><code>#cloud-config\nwrite_files:\n  - content: |\n      SHAREPUBKEY=\"true\"\n      AUTOPARSEMATCH=\".*\"\n      AUTH_KEY=kubecluster1\n    path: /opt/flight/cloudinit.in\n    permissions: '0600'\n    owner: root:root\n  - content: |\n      /opt/flight/bin/flight profile prepare openflight-kubernetes-multinode\n    path: /var/lib/firstrun/scripts/00-prepare-profile.bash\n    permissions: '0600'\n    owner: root:root\n</code></pre></li> </ol> <p>Note</p> <p>The above data will enable sharing the public key to clients, automatically add any nodes that connect to hunter with a correct auth key, and secure node hunting with the authorisation key of kubecluster1. For more information see the user data documentation</p>"},{"location":"docs/flight-solo/cluster-build-methods/workflows/kubernetes-aws-auto-apply/#configure-login-node","title":"Configure Login Node","text":"<ol> <li>Parse &amp; Label Login Node<ol> <li>Run the command <code>flight hunter parse</code>.</li> <li>Select the node with Space</li> <li>Press Down to erase the field.</li> <li>Type in <code>login1</code> as the label and press Enter.</li> </ol> </li> <li>Run flight profile configure and Select <code>OpenFlight Kubernetes Multinode</code><ol> <li>Cluster Name: kubecluster1</li> <li>Default user: flight</li> <li>Set user password to: Some secure password</li> <li>NFS server (hostname or flight-hunter label): login1</li> <li>IP or FQDN for Web Access: Public IPv4 DNS from AWS EC2 Console for Node</li> <li>IP Range of Compute Nodes: 10.10.0.0/16</li> <li>IP Range of Kubernetes Pods (must not overlap with Compute Node IP Range): 192.168.0.0/16</li> </ol> </li> <li>Apply the master profile to login1 with <code>flight profile apply login1 master</code></li> <li>Wait for flight profile list to show the status of login1 as completed (<code>flight profile view login1 --watch</code>) </li> <li> <p>Setup Automatic Application of Hunter Nodes</p> <ol> <li> <p>Add the following lines to <code>/opt/flight/opt/hunter/etc/config.yml</code> to automatically apply the worker profile to hunter nodes with labels containing <code>node</code>.     <pre><code>auto_apply:\n  node: worker\n</code></pre></p> <p>Note</p> <p>You will need to use sudo to have permissions to edit this config file.</p> </li> <li> <p>Restart hunter service     <pre><code>flight service restart hunter\n</code></pre></p> </li> </ol> </li> </ol>"},{"location":"docs/flight-solo/cluster-build-methods/workflows/kubernetes-aws-auto-apply/#launch-a-compute-node","title":"Launch a Compute Node","text":"<p>Launch the Flight Solo image in AWS marketplace</p> <ol> <li>Select \"Launch from EC2\"<ol> <li>Instance Type: t3.2xlarge</li> <li>Click \"Edit\" on Network Settings and select:<ol> <li>VPC: kubecluster1</li> <li>Auto-assign public IP: Disable</li> <li>Select existing security group: kubecluster1-sg</li> </ol> </li> <li>Set root volume size to at least 20GB</li> <li>Under Advanced Details -&gt; User Data (replace <code>LOGIN_SERVER_IPV4_PRIVATE_ADDRESS</code> with the IP of the login node, this can be found with <code>ip addr</code> on that system)     <pre><code>#cloud-config\nwrite_files:\n  - content: |\n      SERVER=&lt;LOGIN_SERVER_IPV4_PRIVATE_ADDRESS&gt;\n      LABEL=node01\n      AUTH_KEY=kubecluster1\n    path: /opt/flight/cloudinit.in\n    permissions: '0600'\n    owner: root:root\n  - content: |\n      /opt/flight/bin/flight profile prepare openflight-kubernetes-multinode\n    path: /var/lib/firstrun/scripts/00-prepare-profile.bash\n    permissions: '0600'\n    owner: root:root\n</code></pre></li> </ol> </li> <li>Repeat the above to create more nodes, changing the <code>LABEL=</code> field in the cloud-init data to be a unique label.</li> </ol>"},{"location":"docs/flight-solo/cluster-build-methods/workflows/kubernetes-aws-auto-apply/#checking-it-works","title":"Checking it Works","text":"<p>Congratulations! You now have an automatically expanding Kubernetes cluster!</p> <p>Once the compute node has come up it will be automatically added to the accepted list of hosts for the cluster (see <code>flight hunter list</code> ) and will have the Kubernetes worker profile applied to it automatically (see <code>flight profile list</code> ).</p> <p>What you can do next is:</p> <ul> <li>Seamlessly access the cluster via the Flight Web Suite in a web browser by visiting the Public IPv4 DNS from AWS EC2 Console for the login node</li> <li>Run some kubernetes pods</li> <li>Remove unused nodes from the cluster before deleting them from AWS</li> </ul>"},{"location":"docs/flight-solo/cluster-build-methods/workflows/slurm-openstack-autoconfig/","title":"SLURM Standalone on OpenStack with Auto-Configure","text":""},{"location":"docs/flight-solo/cluster-build-methods/workflows/slurm-openstack-autoconfig/#overview","title":"Overview","text":"<p>This workflow demonstrates the creation of a standalone slurm cluster on Openstack which utilises Flight Solo to automatically configure the node upon boot. It is assumed that you have a flight solo image on openstack, if not see the documentation here.</p>"},{"location":"docs/flight-solo/cluster-build-methods/workflows/slurm-openstack-autoconfig/#prepare-network","title":"Prepare Network","text":"<p>Preparing the network, router and subnet is out of scope for this documentation, so it is assumed that this is already done. See the Openstack documentation for more information.</p> <ol> <li>Create a Security Group with<ol> <li>Name: <code>autostandalone1-sg</code></li> <li>Description: \"Security group for automatic standalone cluster\"</li> <li>Ingress Rules<ol> <li>\"SSH\",  remote: \"CIDR\", CIDR: \"<code>0.0.0.0/0</code>\"</li> <li>\"HTTP\",  remote: \"CIDR\", CIDR: \"<code>0.0.0.0/0</code>\"</li> <li>\"HTTPS\",  remote: \"CIDR\", CIDR: \"<code>0.0.0.0/0</code>\"</li> <li>\"All TCP\", remote: \"Security Group\",  from this security group</li> <li>\"All UDP\", remote: \"Security Group\",  from this security group</li> </ol> </li> </ol> </li> <li>Create a keypair with<ol> <li>Key Pair Name: <code>autostandalone-key</code></li> <li>Key Type: SSH Key</li> </ol> </li> </ol>"},{"location":"docs/flight-solo/cluster-build-methods/workflows/slurm-openstack-autoconfig/#launch-the-instance","title":"Launch the instance","text":"<ol> <li>Click Launch Instance.<ol> <li>Details:<ol> <li>Instance Name: <code>auto-slurm</code></li> <li>Count: <code>1</code></li> </ol> </li> <li>Source:<ol> <li>Volume Size: <code>20</code></li> <li>Image: imported Flight Solo image</li> </ol> </li> <li>Flavour: <code>m1.medium</code></li> <li>Networks: your network</li> <li>Configuration<ol> <li>Customisation script     <pre><code>#cloud-config\nwrite_files:\n- content: |\n    LABEL=\"standalone1\"\n    AUTOPARSEMATCH=\"auto\"\n    PROFILE_ANSWERS='{\"cluster_type\": \"openflight-slurm-standalone\",  \"cluster_name\": \"my-cluster\",  \"default_username\": \"flight\",  \"default_password\": \"0penfl1ght\"}'\n    AUTOAPPLY=\"standalone: all-in-one\"\n  path: /opt/flight/cloudinit.in\n  permissions: '0600'\n  owner: root:root\nusers:\n  - default\n</code></pre></li> </ol> </li> </ol> </li> <li>Press Launch Instance</li> <li>Wait for the node to finish building, and associate it with a floating ip from the node's drop down menu.</li> </ol> <p>Tip</p> <p>The progress of the auto-configuration can be checked with <code>flight profile view standalone1 --watch</code></p>"},{"location":"docs/flight-solo/cluster-build-methods/workflows/slurm-openstack-autoconfig/#checking-it-works","title":"Checking it works","text":"<p>Congratulations! You now have a Slurm Standalone Cluster!</p> <p>Once the node has come up it will automatically configure and set up the slurm profile on itself (see <code>flight profile list</code> ), with no more human interaction required before it is ready to run jobs.</p> <p>What you can do next is: - Seamlessly access the cluster via the Flight Web Suite in a web browser by visiting the Floating IP you assigned it. - Run some slurm jobs.</p>"},{"location":"docs/flight-solo/get-solo/","title":"Getting Flight Solo","text":"<p>Flight Solo is platform-agnostic so is likely to be available for the cloud provider you use. Currently the Flight Solo image is built &amp; tested on AWS, Azure, OpenStack and Alces Cloud. Further to the image being made available, it can also be launched through the AWS Marketplace.</p> <p>This section details how to obtain and import the Flight Solo image into the supported platforms. </p>"},{"location":"docs/flight-solo/get-solo/alces-cloud/","title":"Import Flight Solo Image to Alces Cloud","text":""},{"location":"docs/flight-solo/get-solo/alces-cloud/#prepare-for-image-import","title":"Prepare for Image Import","text":""},{"location":"docs/flight-solo/get-solo/alces-cloud/#prerequisites","title":"Prerequisites","text":"<ol> <li>To set this up, you will need access to the Alces Cloud platform</li> </ol>"},{"location":"docs/flight-solo/get-solo/alces-cloud/#upload-image","title":"Upload Image","text":"<ol> <li>Login to your Alces Cloud account</li> <li>Download the Flight Solo OpenStack image here (example command below, replace <code>VERSION</code> with the desired version of Flight Solo)     <pre><code>wget https://repo.openflighthpc.org/images/FlightSolo/VERSION/Flight_Solo_VERSION_generic-cloudinit.raw\n</code></pre></li> <li>Source your account OpenStack settings file     <pre><code>source ~/openrc\n</code></pre></li> <li>Upload the image (example command below, replace <code>VERSION</code> with the desired version of Flight Solo)     <pre><code>openstack image create --disk-format raw --min-disk 10 --min-ram 2048 --file Flight_Solo_VERSION_generic-cloudinit.raw \"Flight Solo VERSION\"\n</code></pre></li> </ol>"},{"location":"docs/flight-solo/get-solo/aws/","title":"Import Flight Solo Image to AWS","text":"<p>Tip</p> <p>If you are looking to evaluate the latest release of Flight Solo on AWS then it is recommended to utilise the AWS Marketplace Image - this is identical to the latest release and provides a streamlined launch process</p>"},{"location":"docs/flight-solo/get-solo/aws/#prepare-aws-account-for-image-import","title":"Prepare AWS Account for Image Import","text":""},{"location":"docs/flight-solo/get-solo/aws/#cli-prerequisites","title":"CLI Prerequisites","text":"<ol> <li> <p>To set this up, you will need to install the AWS Command Line Interface(CLI). Confirm that you have the prerequisites for the AWS CLI.</p> </li> <li> <p>Install the AWS CLI by following the AWS guide.</p> </li> <li> <p>Configure basic the basic AWS CLI by following this guide. There is more information about configuration in other parts of the AWS documentation.</p> </li> </ol>"},{"location":"docs/flight-solo/get-solo/aws/#setup-a-bucket","title":"Setup a Bucket","text":"<ol> <li>Create a bucket as described in the AWS Documentation</li> <li>Create a directory within the bucket called <code>images</code></li> </ol>"},{"location":"docs/flight-solo/get-solo/aws/#create-vm-import-policy","title":"Create VM Import Policy","text":"<ol> <li> <p>Create a <code>vmimport</code> policy file to enable vm import operations. Make a file called <code>trust-policy.json</code> with these contents:</p> <pre><code>{\n\"Version\": \"2022-11-03\",\n\"Statement\": [\n  {\n     \"Effect\": \"Allow\",\n     \"Principal\": { \"Service\": \"vmie.amazonaws.com\" },\n     \"Action\": \"sts:AssumeRole\",\n     \"Condition\": {\n        \"StringEquals\":{\n           \"sts:Externalid\": \"vmimport\"\n        }\n     }\n  }\n]\n}\n</code></pre> </li> <li> <p>Create a role from the <code>vmimport</code> policy file.     <pre><code>aws iam create-role --role-name vmimport --assume-role-policy-document \"file://trust-policy.json\"\n</code></pre></p> </li> <li> <p>Create a bucket association with the <code>vmimport</code> role in a file called <code>role-policy.json</code>, replacing <code>&lt;bucketname&gt;</code> with the name of your S3 bucket.     <pre><code>{\n\"Version\": \"2012-10-17\",\n\"Statement\": [\n    {\n        \"Effect\": \"Allow\",\n        \"Action\": [\n            \"s3:GetBucketLocation\",\n            \"s3:GetObject\",\n            \"s3:ListBucket\",\n            \"s3:PutObject\",\n            \"s3:GetBucketAcl\"\n        ],\n        \"Resource\": [\n            \"arn:aws:s3:::&lt;bucketname&gt;\",\n            \"arn:aws:s3:::&lt;bucketname&gt;/*\"\n        ]\n    },\n    {\n        \"Effect\": \"Allow\",\n        \"Action\": [\n            \"ec2:ModifySnapshotAttribute\",\n            \"ec2:CopySnapshot\",\n            \"ec2:RegisterImage\",\n            \"ec2:Describe*\"\n        ],\n        \"Resource\": \"*\"\n    }\n]\n}\n</code></pre></p> </li> <li> <p>Apply the role policy.     <pre><code>aws iam put-role-policy --role-name vmimport --policy-name vmimport --policy-document \"file://role-policy.json\"\n</code></pre></p> </li> </ol>"},{"location":"docs/flight-solo/get-solo/aws/#upload-image","title":"Upload Image","text":"<ol> <li> <p>Download the Flight Solo AWS image here</p> </li> <li> <p>Upload the downloaded Flight Solo image to a directory called <code>images</code> in the S3 bucket with this command:     <pre><code>aws s3 cp Flight_Solo_VERSION_aws.raw s3://&lt;bucketname&gt;/images/\n</code></pre></p> </li> <li> <p>Wait until it has finished uploaded before proceeding.</p> </li> </ol>"},{"location":"docs/flight-solo/get-solo/aws/#import-image-as-snapshot","title":"Import Image As Snapshot","text":"<ol> <li> <p>Create a file called <code>containers.json</code> with raw disk image information. These are the contents (replace <code>&lt;bucketname&gt;</code> with your bucket name):</p> <pre><code>{\n\"Description\": \"Flight_Solo_VERSION_aws.raw\",\n\"Format\": \"raw\",\n\"UserBucket\": {\n    \"S3Bucket\": \"&lt;bucketname&gt;\",\n    \"S3Key\": \"images/Flight_Solo_VERSION_aws.raw\"\n    }\n}\n</code></pre> </li> <li> <p>Import the raw image as a disk snapshot.     <pre><code>aws ec2 import-snapshot --description \"Flight_Solo_VERSION_aws.raw\" --disk-container \"file://containers.json\"\n</code></pre></p> </li> <li> <p>Wait until the import is complete. You can check the progress with this command: (replace the import task ID with the ID output of the previous command)     <pre><code>aws ec2 describe-import-snapshot-tasks --import-task-ids import-snap-00000000000000000\n</code></pre></p> </li> </ol>"},{"location":"docs/flight-solo/get-solo/aws/#create-ami-from-snapshot","title":"Create AMI from Snapshot","text":"<ol> <li>Once imported, the snapshot can be registered as an AMI (replacing <code>VERSION</code> with the version of Flight Solo and <code>SNAPSHOT_ID</code> with the ID of the snapshot created in the previous section)      <pre><code>aws ec2 register-image --name \"Flight Solo VERSION\" --description \"Flight Solo VERSION from snapshot\" --block-device-mappings \"[{\\\"DeviceName\\\": \\\"/dev/sda1\\\",\\\"Ebs\\\":{\\\"VolumeSize\\\":10, \\\"SnapshotId\\\":\\\"SNAPSHOT_ID\\\"}}]\" --root-device-name \"/dev/sda1\" --architecture x86_64 --ena-support\n</code></pre></li> </ol>"},{"location":"docs/flight-solo/get-solo/azure/","title":"Importing Flight Solo Image to Azure","text":""},{"location":"docs/flight-solo/get-solo/azure/#prepare-azure-account","title":"Prepare Azure Account","text":""},{"location":"docs/flight-solo/get-solo/azure/#prerequisites","title":"Prerequisites","text":"<ol> <li>Start by installing the Azure Command Line Interface(CLI), as using it is the simplest way to import a raw image. Alternatively you can follow these instructions using the Azure Cloud Shell.</li> </ol>"},{"location":"docs/flight-solo/get-solo/azure/#create-storage-account","title":"Create Storage Account","text":"<ol> <li> <p>Create a resource group for the storage account     <pre><code>az resource group create MY_RESOURCE_GROUP_NAME\n</code></pre></p> </li> <li> <p>Create a storage account     <pre><code>az storage account create --name \"MY_STORAGE_ACCOUNT_NAME\" --resource-group \"MY_RESOURCE_GROUP_NAME\"\n</code></pre></p> </li> <li> <p>Create a storage container      <pre><code>az storage container create --name \"MY_CONTAINER_NAME\" --account-name \"MY_STORAGE_ACCOUNT_NAME\" --resource-group \"MY_RESOURCE_GROUP_NAME\"\n</code></pre></p> </li> </ol>"},{"location":"docs/flight-solo/get-solo/azure/#import-image","title":"Import Image","text":"<ol> <li>Download the Flight Solo Azure image here</li> <li> <p>Upload the raw Flight Solo image as a storage blob to the container     <pre><code>az storage blob upload --account-name \"MY_STORAGE_ACCOUNT_NAME\" \\\n                       --container-name \"MY_CONTAINER_NAME\" \\\n                       --type page \\\n                       --file Flight_Solo_VERSION_azure.raw \\\n                       --name Flight-Solo_VERSION_azure.vhd\n</code></pre></p> </li> <li> <p>Finally, create an Azure image from the storage blob (Make sure to get the correct source from the uploaded storage blob)     <pre><code>az image create --resource-group \"MY_RESOURCE_GROUP_NAME\" \\\n    --name Flight_Solo_VERSION_azure \\\n    --os-type Linux \\\n    --hyper-v-generation V2 \\\n    --source  https://MY_STORAGE_ACCOUNT_NAME.blob.core.windows.net/MY_CONTAINER_NAME/Flight_Solo_VERSION_azure.vhd\n</code></pre></p> </li> </ol>"},{"location":"docs/flight-solo/get-solo/azure/#changing-regions","title":"Changing regions","text":"<p>The storage blob will be placed in the region of the storage account and container it is created in, and an image made from it must go into a resource group with the same region.</p> <p>In case this region is the wrong one, it can be changed after the image is created:</p> <ol> <li> <p>Install the Azure CLI image copy extension.     <pre><code>az extension add --name image-copy-extension\n</code></pre></p> </li> <li> <p>Set the options for source and target resource group, regions and source image.     <pre><code>az image copy --source-resource-group \"MY_RESOURCE_GROUP_NAME\"\n              --source-object-name \"Flight_Solo_VERSION_azure\" \\\n              --target-location \"uksouth\" \"westeurope\" \\\n              --target-resource-group \"MY_RESOURCE_GROUP_NAME\" \\\n              --cleanup\n</code></pre></p> </li> <li> <p>After a short wait, the source image will have the name <code>SOURCE_OBJECT_NAME-region</code> and be in the target resource group</p> </li> </ol>"},{"location":"docs/flight-solo/get-solo/openstack/","title":"Import Flight Solo Image to OpenStack","text":""},{"location":"docs/flight-solo/get-solo/openstack/#prepare-for-image-import","title":"Prepare for Image Import","text":""},{"location":"docs/flight-solo/get-solo/openstack/#cli-prerequisites","title":"CLI Prerequisites","text":"<ol> <li>To set this up, you will need to install the OpenStack CLI</li> <li>As well as the CLI, you will need your OpenStack RC file</li> </ol>"},{"location":"docs/flight-solo/get-solo/openstack/#upload-image","title":"Upload Image","text":"<ol> <li>Download the Flight Solo OpenStack image here</li> <li>Upload the image     <pre><code>openstack image create --disk-format raw --min-disk 10 --min-ram 2048 --file /path/to/Flight_Solo_VERSION_generic-cloudinit.raw Flight_Solo_VERSION_generic-cloudinit\n</code></pre></li> </ol>"},{"location":"docs/flight-solo/understand-solo/","title":"Understanding Flight Solo","text":"<p>Flight Solo is a combination of tools, process and knowledge with the aim of providing a streamlined solution to launching HPC systems. It broadly consists of:</p> <ul> <li>HPC-Optimised Rocky 9 OS</li> <li>Cloud-Init Integration</li> <li>Flight Environment</li> </ul>"},{"location":"docs/flight-solo/understand-solo/#hpc-optimised-rocky-9-os","title":"HPC Optimised Rocky 9 OS","text":"<p>Flight Solo is built of a clean Rocky EL9 image. This image has been tweaked to be ready to do HPC through:</p> <ul> <li>Kernel drivers set to support most virtualisation platforms</li> <li>Suitable security settings</li> </ul>"},{"location":"docs/flight-solo/understand-solo/#cloud-init-integration","title":"Cloud-Init Integration","text":"<p>Cloud-Init provides the ability to inject runtime information and configuration into the OS of cloud systems. It can be used to perform many different setup tasks, detailing this setup is outside the scope of this documentation. Information </p> <p>More information on using Cloud-Init in the context of Flight Solo can be found in the User Data documentation.</p>"},{"location":"docs/flight-solo/understand-solo/#flight-environment","title":"Flight Environment","text":"<p>Flight Solo comes with the complete Flight Environment installed by default along with suitable configuration modifications to get the most out of the tools from the get-go. </p>"},{"location":"docs/flight-solo/understand-solo/user-data/","title":"Cloud-Init User Data with Flight Solo","text":"<p>User data (otherwise known as \"cloud-init data\" or \"cloud data\") is a way of giving data to an instance for it to run as part of the startup process. This page will describe how to provide user data specific to Flight Solo to customise and configure your cluster.</p> <p>When present, the file <code>/opt/flight/cloudinit.in</code> will be read and used to configure the Flight Solo image on first boot. The best way of creating this file on boot is by creating it with user data and providing the desired configuration keys to be written to the file. </p> <p>The cloud-init <code>write_files</code> directive is used to create the file which is read in. A basic user data script that creates this file would look like: Minimal Cloud Config Script<pre><code>#cloud-config\nwrite_files:\n  - content: |\n      OPTION=\"value\" # (1)!\n      OPTION2=\"value2\"\n    path: /opt/flight/cloudinit.in\n    permissions: '0600'\n    owner: root:root\n</code></pre></p> <ol> <li>Replace <code>OPTION</code> with a config option described below</li> </ol>"},{"location":"docs/flight-solo/understand-solo/user-data/#flight-solo-config-options","title":"Flight Solo Config Options","text":"<p>The available options for the Flight Solo cloud-init file are as follows.</p>"},{"location":"docs/flight-solo/understand-solo/user-data/#server","title":"<code>SERVER</code>","text":"<p>Sets the target address that Flight Hunter on the node will send to at startup. It is recommended that this is the local IP of whichever server is going to be used to manage the inventory of hosts. </p> Example Usage<pre><code>SERVER=10.10.0.1\n</code></pre>"},{"location":"docs/flight-solo/understand-solo/user-data/#broadcast_address","title":"<code>BROADCAST_ADDRESS</code>","text":"<p>Set the broadcast address which Flight Hunter will send to. </p> Example Usage<pre><code>BROADCAST_ADDRESS=10.10.255.255\n</code></pre>"},{"location":"docs/flight-solo/understand-solo/user-data/#auth_key","title":"<code>AUTH_KEY</code>","text":"<p>Sets the auth key that is used by Flight Hunter, this is used by both the <code>hunt</code> process and <code>send</code> command. Use this to secure your hunter inventory by only allowing hosts to push their information to it with a matching key.</p> Example Usage<pre><code>AUTH_KEY=banana\n</code></pre>"},{"location":"docs/flight-solo/understand-solo/user-data/#label","title":"<code>LABEL</code>","text":"<p>Sets label to be used for the node in Flight Hunter.</p> Example Usage<pre><code>LABEL=node01\n</code></pre>"},{"location":"docs/flight-solo/understand-solo/user-data/#prefix","title":"<code>PREFIX</code>","text":"<p>Sets the prefix to be used for the node in Flight Hunter.</p> Example Usage<pre><code>PREFIX=node\n</code></pre> <p>Note</p> <p>This option is superseded by <code>LABEL</code> if both are provided</p>"},{"location":"docs/flight-solo/understand-solo/user-data/#autoparsematch","title":"<code>AUTOPARSEMATCH</code>","text":"<p>Sets the regex for auto-parse rules in the Flight Hunter server. </p> Example Usage 1 - Match any incoming hostnames including 'example-domain'<pre><code>AUTOPARSEMATCH=example-domain\n</code></pre> Example Usage 2 - Parse all hosts that make it into the buffer<pre><code>AUTOPARSEMATCH=.*\n</code></pre>"},{"location":"docs/flight-solo/understand-solo/user-data/#sharepubkey","title":"<code>SHAREPUBKEY</code>","text":"<p>If set then this node will share the root user's pub ssh key over the local network on port 1234. This means that any solo images with <code>SERVER</code> set to this node will attempt to grab its public key to allow root SSH between your cluster.</p> Example Usage<pre><code>SHAREPUBKEY=true\n</code></pre>"},{"location":"docs/flight-solo/understand-solo/user-data/#profile_answers","title":"<code>PROFILE_ANSWERS</code>","text":"<p>Set to json text which is used to answer profile configure questions, the format should be the same as for the profile configure sub-option <code>--answers</code>. Any answers not supplied will be set to their default values.</p> Example Usage<pre><code>PROFILE_ANSWERS='{ \"cluster_type\": \"openflight-slurm-standalone\", \"cluster_name\": \"mycluster1\" }'\n</code></pre>"},{"location":"docs/flight-solo/understand-solo/user-data/#autoapply","title":"<code>AUTOAPPLY</code>","text":"<p>Set automatic application of identities to nodes. When a node connects with hunter, if it matches one of the regular expressions then the corresponding identity will be applied.  </p> Example Usage - Match hunter labels containing 'node' and apply the 'compute' identity to them, match hunter labels with 'gateway' in them and apply 'login' identity<pre><code>AUTOAPPLY=\"node: compute, gateway: login\"\n</code></pre> <p>Note</p> <p><code>AUTOAPPLY</code> can only start if a cluster type has already been configured on this node. The configuration of the Flight Profile can be done before launching client nodes or set with <code>PROFILE_ANSWERS</code>. </p>"},{"location":"docs/flight-solo/understand-solo/user-data/#prefix_starts","title":"<code>PREFIX_STARTS</code>","text":"<p>Set prefix start numbers based on node name. When a node connects with hunter, if it has a prefix which matches one of the regular expressions then it will be given a number, starting with the start number and incrementing until there is an unused number.</p> Example Usage - Increment counts for 'node' from '001' and 'gpu' from '01'<pre><code>PREFIX_STARTS=\"node: '001', gpu: '01'\"\n</code></pre> <p>Note</p> <p>Many of these relate to command line options that are explained in more detail in the Flight Hunter documentation.</p>"},{"location":"docs/flight-solo/understand-solo/user-data/#autoremove","title":"<code>AUTOREMOVE</code>","text":"<p>If set then this will set automatic removal of nodes from <code>hunter</code> and <code>profile</code> upon shutdown. This is especially beneficial for auto-scaling scenarios where a node shutting down usually means the resource is being destroyed and therefore tidying it from the cluster environment is required.</p> Example Usage<pre><code>AUTOREMOVE=true\n</code></pre>"},{"location":"docs/hpc-concepts/","title":"HPC Concepts","text":"<p>The purpose of this documentation is to provide a list of considerations and guidelines for the development of a HPC environment. This documentation should be followed through in order to properly understand the structure of the environment and that certain considerations are not missed out along the way.</p> <p>To generalise the entire process, it goes as follows:</p> <pre><code>graph LR\n    A[Hardware Architecture Design] --&gt; B[Hardware Build]\n    B --&gt; C[Software Build]\n    C --&gt; D[Platform Delivery]</code></pre> <p>Ensuring that a suitable hardware and network architecture is designed before the build process begins will allow you to create a stable base for your HPC platform.</p> <p>Performing the hardware build before doing any software configuration guarantees that the network and hardware is properly setup. A partially built network during software setup can lead to unforeseen issues with communication and configuration.</p> <p>Once the infrastructure has been physically built the software build can proceed. Usually the central servers will be configured first before client and compute nodes are configured.</p> <p>Finally, platform delivery includes a range of connectivity, performance and quality tests which ensure that the completed environment is stable, manageable and consistent.</p> <p>Tip</p> <p>It is recommended to read through all of the documentation before starting to design the HPC platform to understand the scope and considerations.</p>"},{"location":"docs/hpc-concepts/base-system/","title":"Considerations for Software and Application Deployment","text":"<p>Before considering how the OS and applications will be deployed it is worth making a few decisions regarding the OS that will be used:</p> <ul> <li>Will the same OS be used on all systems? (it's strong recommended to do so)</li> <li>What software will be used? (and therefore will need to be supported by the OS)</li> <li>How stable is the OS? (bleeding edge OSes may have bugs and instabilities that could negatively impact the HPC environment)</li> <li>If you are using bare-metal hardware, is the OS supported by your hardware vendor? (running an unsupported OS can lead to issues when attempting to obtain hardware support)</li> </ul>"},{"location":"docs/hpc-concepts/base-system/#deployment","title":"Deployment","text":"<p>The deployment of the HPC platform can be summarised in two main sections, these being:</p> <ul> <li>Operating System Deployment</li> <li>Software Package Repository Management</li> </ul>"},{"location":"docs/hpc-concepts/base-system/#operating-system-deployment","title":"Operating System Deployment","text":"<p>When it comes to performing many operating installations across nodes in the network it can be tricky to find a flexible, manageable, automated solution. Performing manual installations of operating systems may be the ideal solution if there are only a few compute nodes, however, there are many other ways of improving the speed of OS deployment:</p> <ul> <li>Disk Cloning - A somewhat inelegant solution, disk cloning involves building the operating system once and creating a compressed copy on a hard-drive that can be restored to blank hard-drives.</li> <li>Kickstart - A kickstart file is a template for automating OS installations, the configuration file can be served over the network such that clients can PXE boot the installation. This can allow for easy, distributed deployment over a local network.</li> <li>Image Deployment - Cloud service providers usually deploy systems from template images that set hostnames and other unique system information at boot time. Customised templates can also be created for streamlining the deployment and customisation procedure.</li> </ul> <p>It is worth considering manual, cloning and kickstart solutions for your OS deployment, any one of them could be the ideal solution depending on the number of machines that are being deployed.</p>"},{"location":"docs/hpc-concepts/base-system/#repository-management","title":"Repository Management","text":"<p>It is worth considering how packages, libraries and applications will be installed onto individual nodes and the network as a whole. Operating systems usually have their own package management system installed that uses public repositories for pulling down packages for installation. It is likely that all of the packages required for a system are not in the public repositories so it's worth considering where additional packages will come from (e.g. a 3rd party repository, downloaded directly from the package maintainer or manually compiled).</p> <p>Further to managing packages on the local system, the entire network may require applications to be installed; there are a couple of options for achieving this:</p> <ul> <li>Server Management Tools - Management tools such as ansible, puppet, chef or pdsh can execute commands across multiple systems in parallel. This saves time instead of having to individually login and run commands on each node in the system.</li> <li>Network Package Managers - Software such as Conda and EasyBuild can install an application in a centralised storage location, allowing users simply to load the module in order to start using the application.</li> </ul> <p>For more information regarding network package managers and application deployment, see application deployment.</p>"},{"location":"docs/hpc-concepts/base-system/#additional-considerations-and-questions","title":"Additional Considerations and Questions","text":"<ul> <li>How will applications outside of the repositories be installed?</li> <li>Will the application need to be usable by all nodes in the HPC network? (e.g. an NFS export for apps may solve the issue of multiple installations)</li> <li>How will new package versions be installed when the HPC environment being maintained in the future?</li> <li>How will you create and maintain a consistent software environment on all nodes over time?</li> </ul>"},{"location":"docs/hpc-concepts/hpc-environment/","title":"Considerations for HPC Environment Design","text":""},{"location":"docs/hpc-concepts/hpc-environment/#job-scheduling","title":"Job Scheduling","text":"<p>In a HPC environment there are large, distributed, multiple processor jobs that the users wish to run. While these jobs can be run manually by simply executing job scripts along with a hostfile containing the compute nodes to execute on, you will soon run into problems with multiple users, job queuing and priorities. These features are provided by a job scheduler, delivering a centralised server that manages the distribution, prioritisation and execution of job scripts from multiple users in the HPC network.</p> <p>Popular job schedulers for HPC clusters include:</p> <ul> <li>Open Grid Scheduler (SGE)</li> <li>PBS / Torque-Maui / Torque-Moab / PBSPro</li> <li>SLURM</li> <li>LSF / OpenLava</li> </ul> <p>All job schedulers provide a similar level of functionality and customisations so it is worth investigating the features of the available solutions to find the one best suited for your environment.</p> <p>Info</p> <p>Traditional job schedulers are not the only solution, in some modern HPC environments resources can be managed using container frameworks, such as, Kubernetes</p>"},{"location":"docs/hpc-concepts/hpc-environment/#application-deployment","title":"Application Deployment","text":"<p>General management of applications and libraries is mentioned in the repository management docs however this section focuses on installing applications into the entire HPC environment instead of individually to each node system.</p> <p>A few things to consider when designing/implementing an application deployment system are:</p> <ul> <li>How will applications be stored? (central network storage location?)</li> <li>What parts of the application need to be seen by the nodes? (application data? program files? libraries?)</li> <li>How will multiple versions of the same application be installed, and how will users choose between them?</li> <li>How will dependencies be managed? (more on this below)</li> </ul> <p>An application deployment system can be created yourself or but it's recommended to use an existing solution that provides tools and an index of HPC applications for HPC platform installations (such as Conda, EasyBuild or Spack).</p>"},{"location":"docs/hpc-concepts/hpc-environment/#dependencies","title":"Dependencies","text":"<p>When it comes to managing dependencies for applications it can either be done with local installations of libraries/packages or by storing these in a centralised location (as suggested with the applications themselves). Dependency control is one of the main reasons that using the same OS for all systems is recommended as it eliminates the risk of applications only working on some systems within the HPC environment.</p> <p>Dependencies must be managed across all nodes of the cluster, and over time as the system is managed. For example, an application that requires a particular C++ library that is available from your Linux distribution may not work properly after you install distribution updates on your compute nodes. Dependencies for applications that utilise dynamic libraries (i.e. loaded at runtime, rather than compile-time) must be particularly carefully managed over time.</p>"},{"location":"docs/hpc-concepts/hpc-environment/#reproducibility","title":"Reproducibility","text":"<p>It is important that your users receive a consistent, long-term service from your HPC cluster to allow them to rely on results from applications run at different points in your clusters' life cycle. Consider the following questions when designing your application management system:</p> <ul> <li>How can I install new applications quickly and easily for users?</li> <li>What test plans have I created to ensure that applications run in the same way across all cluster nodes?</li> <li>How can I ensure that applications run normally as nodes are re-installed, or new nodes are added to the cluster?</li> <li>How can I test that applications are working properly after an operating system upgrade or update?</li> <li>How will I prepare for moving to a new HPC cluster created on fresh hardware, or using cloud resources?</li> <li>What are the disaster recovery plans for my software applications?</li> </ul>"},{"location":"docs/hpc-concepts/infrastructure/","title":"Considerations for Infrastructure Design","text":"<p>Infrastructure design largely relates to the considerations made for the cluster architecture. Depending on the design being used, some of the infrastructure decisions may have already been made.</p>"},{"location":"docs/hpc-concepts/infrastructure/#infrastructure-service-availability","title":"Infrastructure Service Availability","text":"<p>There are typically 3 possible service availability options to choose from, these are:</p> <ul> <li>All-in-one</li> <li>VM Platform</li> <li>High Availability VM Platform</li> </ul> <p>These are covered in more detail below.</p> <p>Note</p> <p>If using a Cloud Platform then the service availability will be handled by the cloud provider. The only additional considerations are how services will be provided in terms of native running services or containerised.</p>"},{"location":"docs/hpc-concepts/infrastructure/#all-in-one","title":"All-in-one","text":"<p>This is the most common solution, an all-in-one approach loads services onto a single machine which serves the network. It is the simplest solution as a single OS install is required and no additional configuration of virtual machine services is needed.</p> <p>This solution, while quick and relatively easy to implement, is not a recommended approach. Due to the lack of redundancy options and the lack of service isolation there is a higher risk of an issue effecting one service (or the machine) to have an effect on other services.</p>"},{"location":"docs/hpc-concepts/infrastructure/#vm-platform","title":"VM Platform","text":"<p>A VM platform provides an additional layer of isolation between services. This can allow for services to be configured, migrated and modified without potentially effecting other services.</p> <p>There are a number of solutions for hosting virtual machines, including:</p> <ul> <li>Open-source solutions (e.g. VirtualBox, KVM, Xen)</li> <li>Commercial solutions (e.g. VMware)</li> </ul> <p>The above software solutions provide similar functionality and can all be used as a valid virtualisation platform. Further investigation into the ease of use, flexibility and features of the software is recommended to identify the ideal solution for your HPC platform.</p>"},{"location":"docs/hpc-concepts/infrastructure/#high-availability-vm-platform","title":"High Availability VM Platform","text":"<p>For further redundancy, the virtualisation platform can utilise a resource pool. The service will be spread across multiple machines which allows for VMs to migrate between the hosts whilst still active. This live migration can allow for one of the hosts to be taken off of the network for maintenance without impacting the availability of the service VMs.</p>"},{"location":"docs/hpc-concepts/infrastructure/#node-network-configuration","title":"Node Network Configuration","text":"<p>In addition to the availability of services, the network configuration on the node can provide better performance and redundancy. Some of the network configuration options that can improve the infrastructure are:</p> <ul> <li>Channel Bonding - Bonding interfaces allows for traffic to be shared between 2 network interfaces. If the bonded interfaces are connected to separate network switches then this solution</li> <li>Interface Bridging - Network bridges are used by interfaces on virtual machines to connect to the rest of the network. A bridge can sit on top of a channel bond such that the VM service network connection is constantly available.</li> <li>VLAN Interface Tagging - VLAN management can be performed both on a managed switch and on the node. The node is able to create subdivisions of network interfaces to add VLAN tags to packets. This will create separate interfaces that can be seen by the operating system (e.g. eth0.1 and eth0.2) which can individually have IP addresses set.</li> </ul>"},{"location":"docs/hpc-concepts/infrastructure/#additional-considerations-and-questions","title":"Additional Considerations and Questions","text":"<ul> <li>Could containerised solutions be used instead of VMs?<ul> <li>Docker or Singularity containerised services can provide similar levels of isolation between services as VMs without the additional performance overhead.</li> </ul> </li> </ul>"},{"location":"docs/hpc-concepts/infrastructure/#recommendations-for-infrastructure-design","title":"Recommendations for Infrastructure Design","text":"<p>The example configurations here combine elements of the network and hardware guide as well as the different infrastructure solutions from :ref:<code>infrastructure-considerations</code>. These focus on the internal configuration of the master node but these examples can be extrapolated for configuring login, storage, compute or any other nodes that are part of the HPC environment.</p>"},{"location":"docs/hpc-concepts/infrastructure/#simple-infrastructure","title":"Simple Infrastructure","text":"<p>The simplest infrastructure configuration uses the all-in-one approach where services are configured on the master node's operating system.</p>"},{"location":"docs/hpc-concepts/infrastructure/#virtual-machine-infrastructure","title":"Virtual Machine Infrastructure","text":"<p>This solution separates the services into VMs running on the master node. In order for these VMs to be able to connect to the primary network a network bridge is created that allows the VM interfaces to send traffic over the eth0 interface.</p>"},{"location":"docs/hpc-concepts/infrastructure/#channel-bonded-infrastructure","title":"Channel Bonded Infrastructure","text":"<p>This example adds a layer of redundancy over the VM Infrastructure design by bonding the eth0 and eth3 interfaces. These interfaces are connected to separate network switches (the switches will be bridged together as well) which provides redundancy should a switch or network interface fail. Bonding of the two interfaces creates a new bond interface that the bridge for the virtual machines connects to.</p>"},{"location":"docs/hpc-concepts/infrastructure/#vlan-infrastructure","title":"VLAN Infrastructure","text":"<p>The above solution implements the channel bonded infrastructure in a network with VLANs. The VLANs have bond and bridge interfaces created for them. This allows some additional flexibility for VM bridging as virtual interfaces can be bridged onto specific VLANs whilst maintaining the redundancy provided by the bond. This adds additional security to the network as the master node can be left without an IP on certain VLAN bond interfaces which prevents that network from accessing the master node whilst VMs on the master node are able to reach that VLAN.</p>"},{"location":"docs/hpc-concepts/monitoring/","title":"Considerations for Monitoring the HPC Platform","text":""},{"location":"docs/hpc-concepts/monitoring/#types-of-monitoring","title":"Types of Monitoring","text":"<p>There are 2 types of monitoring that can be implemented into a network; these are:</p> <ul> <li>Passive - Passive monitoring tools collect data and store from systems. Usually this data will be displayed in graphs and is accessible either through command-line or web interfaces. This sort of monitoring is useful for historical metrics and live monitoring of systems.</li> <li>Active - Active monitoring collects and checks metrics; it will then send out notifications if certain thresholds or conditions are met. This form of monitoring is beneficial for ensuring the health of systems; for example, email notifications can be sent out when systems start overheating or if a system is no longer responsive.</li> </ul> <p>Both forms of monitoring are usually necessary in order to ensure that your HPC cluster is running properly, and in full working order.</p>"},{"location":"docs/hpc-concepts/monitoring/#metrics","title":"Metrics","text":"<p>It is worth considering what metrics for the system will be monitored; a few common ones are listed here:</p> <ul> <li>CPU<ul> <li>Load average</li> <li>Idle percentage</li> <li>Temperature</li> </ul> </li> <li>Memory<ul> <li>Used</li> <li>Free</li> <li>Cached</li> </ul> </li> <li>Disk<ul> <li>Free space</li> <li>Used space</li> <li>Swap (free/used/total)</li> <li>Quotas (if configured)</li> </ul> </li> <li>Network<ul> <li>Packets in</li> <li>Packets out</li> </ul> </li> </ul> <p>Note</p> <p>Cloud service providers usually have both passive and active monitoring services available through their cloud management front-end.</p>"},{"location":"docs/hpc-concepts/monitoring/#additional-considerations-and-questions","title":"Additional Considerations and Questions","text":"<ul> <li>What metrics should be monitored?</li> <li>How frequently should metrics be checked?</li> <li>What level of notification is required?<ul> <li>Escalation upon repeated errors?</li> <li>Acknowledgement of long-running outages?</li> <li>How do we avoid over-saturation of notifications during major outages?</li> <li>What tests will we run to ensure that notifications are working properly?</li> </ul> </li> </ul>"},{"location":"docs/hpc-concepts/network-hardware/","title":"Considerations for Network and Hardware Design","text":"<p>In general, the things to consider when designing the hardware and network solution for a HPC platform are:</p> <ul> <li>The hardware environment</li> <li>The types of nodes required in the network</li> <li>The different networks to be used by the network</li> <li>The level of resilience desired</li> <li>The hostname and domain naming convention</li> </ul> <p>These topics are covered in more detail below.</p>"},{"location":"docs/hpc-concepts/network-hardware/#hardware-environment","title":"Hardware Environment","text":"<p>The hardware environment will generally be one of two setups, metal or cloud.</p> <ul> <li>Metal - Metal environments are those which are composed of on-site systems in a data center which are usually running 24/7.</li> <li>Cloud - Cloud environments are systems hosted in a third-party data center (or on-premise private cloud) and are usually ephemeral systems that are being created and destroyed on demand.</li> <li>Metal/Cloud Hybrid - A hybrid environment usually consists of a core metal configuration that uses cloud as an overflow for additional capacity at times of high utilisation.</li> </ul> <p>A hardware environment is mainly focussed on the location, capacity and permanence of the HPC platform and does not directly determine the hardware that will be used in the various systems.</p>"},{"location":"docs/hpc-concepts/network-hardware/#node-types","title":"Node Types","text":"<p>A complete HPC platform will be comprised of systems that serve different purposes within the network. Ideas of node types along with the services and purpose of those nodes can be seen below.</p> <ul> <li>Login Node - A login node will usually provide access to the HPC platform and will be the central system that users access to run applications. How users will access the system should be considered, usually this will be SSH and some graphical login service, such as, VNC.</li> <li>Master Node - A master node will usually run services for the HPC platform. Such as, the master process for a job scheduler, monitoring software and user management services.</li> <li>Compute Node - Compute nodes are usually used for running HPC applications that are queued through a job scheduler. Additionally, these can be used for VM deployments (via software like OpenStack) or other computational uses. Compute nodes usually have large amounts of cores and memory as well as high bandwidth interconnect (like Infiniband).</li> <li>Special-purpose Node - Some compute nodes may feature a particular specification to be used for a particular job, or stage in your workflow. Examples may include nodes with more memory, larger amounts of local scratch storage, or GPU/FPGA devices installed.</li> <li>Storage Node - The storage node will serve network storage solutions to systems on the network. It would have some sort of storage array connected to it which would provide large and resilient storage.</li> </ul> <p>The above types are not strict. Services can be mixed, matched and moved around to create the desired balance and distribution of services and functions for the platform.</p>"},{"location":"docs/hpc-concepts/network-hardware/#different-networks","title":"Different Networks","text":"<p>The network in the system will most likely be broken up (physically or virtually with VLANs) into separate networks to serve different usages and isolate traffic. Potential networks that may be in the HPC platform are:</p> <ul> <li>Primary Network - The main network that all systems are connected to.</li> <li>Out-of-Band Network - A separate network for management traffic. This could contain on-board BMCs, switch management ports and disk array management ports. Typically this network would only be accessible by system administrators from within the HPC network.</li> <li>High Performance Network - Usually built on an Infiniband fabric, the high performance network would be used by the compute nodes for running large parallel jobs over MPI. This network can also be used for storage servers to provide performance improvements to data access.</li> <li>External Networks - The network outside of the HPC environment that nodes may need to access. For example, the Master Node could be connected to an Active Directory server on the external network and behave as a slave to relay user information to the rest of the HPC environment.</li> <li>Build Network - This network can host a DHCP server for deploying operating systems via PXE boot kickstart installations. It allows for systems that require a new build or rebuild to be flipped over and provisioned without disturbing the rest of the network.</li> <li>DMZ - A demilitarised zone would contain any externally-facing services, this could be setup in conjunction with the external networks access depending on the services and traffic passing through.</li> </ul> <p>The above networks could be physically or virtually separated from one another. In a physical separation scenario there will be a separate network switch for each one, preventing any sort of cross-communication. In a virtually separated network there will be multiple bridged switches that separate traffic by dedicating ports (or tagging traffic) to different VLANs. The benefit of the VLAN solution is that the bridged switches (along with bonded network interfaces) provides additional network redundancy.</p> <p>Note</p> <p>If a cloud environment is being used then it is most likely that all systems will reside on the primary network and no others. This is due to the network configuration from the cloud providers.</p>"},{"location":"docs/hpc-concepts/network-hardware/#resilience","title":"Resilience","text":"<p>How well a system can cope with failures is crucial when delivering a HPC platform. Adequate resilience can allow for maximum system availability with a minimal chance of failures disrupting the user. System resilience can be improved with many hardware and software solutions, such as:</p> <ul> <li>RAID Arrays - A RAID array is a collection of disks configured in such a way that they become a single storage device. There are different RAID levels which improve data redundancy or storage performance (and maybe even both). Depending on the RAID level used, a disk in the array can fail without disrupting the access to data and can be hot swapped to rebuild the array back to full functionality. <sup>1</sup></li> <li>Service Redundancy - Many software services have the option to configure a failover server that can take over the service management should the master process be unreachable. Having a secondary server that mirrors critical network services would provide suitable resilience to master node failure.</li> <li>Failover Hardware - For many types of hardware there is the possibility of setting up failover devices. For example, in the event of a power failure (either on the circuit or in a power supply itself) a redundant power supply will continue to provide power to the server without any downtime occurring.</li> </ul> <p>There are many more options than the examples above for improving the resilience of the HPC platform, it is worth exploring and considering available solutions during design.</p> <p>Info</p> <p>Cloud providers are most likely to implement all of the above resilience procedures and more to ensure that their service is available 99.99% of the time.</p>"},{"location":"docs/hpc-concepts/network-hardware/#hostname-and-domain-names","title":"Hostname and Domain Names","text":"<p>Using proper domain naming conventions during design of the HPC platform is best practice for ensuring a clear, logical and manageable network. Take the below fully qualified domain name::</p> <pre><code>node01.pri.cluster1.compute.estate\n</code></pre> <p>Which can be broken down as follows:</p> <ul> <li><code>node01</code> - The hostname of the system</li> <li><code>pri</code> - The network that the interface of the system is sat on (in this case, <code>pri</code> = primary)</li> <li><code>cluster1</code> - The cluster that <code>node01</code> is a part of</li> <li><code>compute</code> - The subdomain of the greater network that <code>cluster1</code> is a part of</li> <li><code>estate</code> - The top level domain</li> </ul>"},{"location":"docs/hpc-concepts/network-hardware/#security","title":"Security","text":"<p>Network security is key for both the internal and external connections of the HPC environment. Without proper security control the system configuration and data is at risk to attack or destruction from user error. Some tips for improving network security are below:</p> <ul> <li>Restrict external access points where possible. This will reduce the quantity of points of entry, minimising the attack surface from external sources.</li> <li>Limit areas that users have access to. In general, there are certain systems that users would never (and should never) have access to so preventing them from reaching these places will circumvent any potential user error risks.</li> <li>Implement firewalls to limit the types of traffic allowed in/out of systems.</li> </ul> <p>It is also worth considering the performance and usability impacts of security measures.</p> <p>Much like with resilience, a Cloud provider will most likely implement the above security features - it is worth knowing what security features and limitations are in place when selecting a cloud environment.</p> <p>Note</p> <p>Non-Ethernet networks usually cannot usually be secured to the same level as Ethernet so be aware of what the security drawbacks are for the chosen network technology.</p>"},{"location":"docs/hpc-concepts/network-hardware/#additional-considerations-and-questions","title":"Additional Considerations and Questions","text":"<p>The below questions should be considered when designing the network and hardware solution for the HPC platform.</p> <ul> <li>How much power will the systems draw?<ul> <li>Think about the power draw of the selected hardware, it may be drawing a large amount of amps so sufficient power sources must be available.</li> </ul> </li> <li>How many users are going to be accessing the system?<ul> <li>A complex, distributed service network would most likely be overkill and a centralised login/master node would be more appropriate.</li> </ul> </li> <li>What network interconnect will be used?<ul> <li>It's most likely that different network technologies will be used for different-networks. For example, the high performance network could benefit from using Infiniband as the interconnect.</li> </ul> </li> <li>How could the hardware be optimised?<ul> <li>BIOS settings could be tweaked on the motherboard to give additional performance and stability improvements.</li> <li>Network switch configurations could be optimised for different types of traffic</li> </ul> </li> <li>What types of nodes will be in the system?</li> <li>What applications are going to be run on the system?<ul> <li>Are they memory intensive?</li> <li>Is interconnect heavily relied upon for computations?</li> </ul> </li> </ul>"},{"location":"docs/hpc-concepts/network-hardware/#recommendations","title":"Recommendations","text":"<p>Below are some different recommendations for hardware and network design. These can vary depending on the number of users and quantity of systems within the HPC platform.</p>"},{"location":"docs/hpc-concepts/network-hardware/#hardware-recommendations","title":"Hardware Recommendations","text":"<p>Useful recommendations for blades, network switches and storage technologies can be found in the Alces Software knowledgebase</p> <p>With the above in mind, diagrams of different architectures are below. They increase in complexity and redundancy as the list goes on.</p>"},{"location":"docs/hpc-concepts/network-hardware/#example-1-standalone","title":"Example 1 - Standalone","text":"<p>The above architecture consists of master, login and compute nodes. The services provided by the master &amp; login nodes can be seen to the right of each node type. This architecture only separates the services for users and admins.</p>"},{"location":"docs/hpc-concepts/network-hardware/#example-2-high-availability","title":"Example 2 - High Availability","text":"<p>This architecture provides additional redundancy to the services running on the master node. For example, the disk array is connected to both master nodes which use multipath to ensure the higher availability of the storage device.</p>"},{"location":"docs/hpc-concepts/network-hardware/#example-3-ha-vms","title":"Example 3 - HA VMs","text":"<p>This architecture puts services inside of VMs to improve the ability to migrate and modify services with little impact to the other services and systems on the architecture. Virtual machines can be moved between VM hosts live without service disruption allowing for hardware replacements to take place on servers.</p>"},{"location":"docs/hpc-concepts/network-hardware/#network-designs","title":"Network Designs","text":"<p>The above architectures can be implemented with any of the below network designs.</p>"},{"location":"docs/hpc-concepts/network-hardware/#example-1-simple","title":"Example 1- Simple","text":"<p>The above design contains the minimum recommended internal networks. A primary network (for general logins and navigating the system), a management network (for BMC management of nodes and switches) and a high performance Infiniband network (connected to the nodes). The master and login nodes have access to the external network for user and admin access to the HPC network.</p> <p>Tip</p> <p>The master node could additionally be connected to the high performance network so that compute nodes have a faster network connection to storage.</p>"},{"location":"docs/hpc-concepts/network-hardware/#example-2-vlans","title":"Example 2 - VLANs","text":"<p>The above network design has a few additions to the first example. The main change is the inclusion of VLANs for the primary, management and build networks (with the build network being a new addition to this design). The build network allows for systems to be toggled over to a DHCP system that uses PXE booting to kickstart an OS installation.</p>"},{"location":"docs/hpc-concepts/network-hardware/#other-recommendations","title":"Other Recommendations","text":""},{"location":"docs/hpc-concepts/network-hardware/#bios-settings","title":"BIOS Settings","text":"<p>It's recommended to ensure that the BIOS settings are reset to default and the latest BIOS version is installed before optimising the settings. This can ensure that any issues that may be present in the configuration before proceeding have been removed.</p> <p>When it comes to optimising the BIOS settings on a system in the network, the following changes are recommended:</p> <ul> <li>Setting the power management to maximum performance</li> <li>Disabling CPU CStates</li> <li>Disabling Hyperthreading</li> <li>Enabling turbo mode</li> <li>Disabling quiet boot</li> <li>Setting BMC to use the dedicated port for BMC traffic</li> <li>Setting the node to stay off when power is restored after AC power loss</li> </ul> <p>Note</p> <p>The wordings for settings above may differ depending on the hardware that is being used. Look for similar settings that can be configured to achieve the same result.</p> <p>For hardware-specific BIOS configuration settings see the Alces Software knowledgebase which has many recommendations based on their experience with integrating HPC environments.</p> <ol> <li> <p>For more information on RAID arrays see the Wikipedia page \u21a9</p> </li> </ol>"},{"location":"docs/hpc-concepts/storage/","title":"Considerations for Storage Solution","text":""},{"location":"docs/hpc-concepts/storage/#storage-hardware","title":"Storage Hardware","text":"<p>When selecting the storage solution it is worth considering the size, performance and resilience of the desired storage solution. Usually some sort of storage array will be used; e.g. a collection of disks (otherwise known as JBOD) in the form of an internal or external RAID array.</p>"},{"location":"docs/hpc-concepts/storage/#type-of-filesystem","title":"Type of filesystem","text":"<p>For many requirements, a simple NFS solution can provide sufficient performance and resiliency for data. Application, library and source-code files are often small enough to be stored on an appropriately sized NFS solution; such storage systems can even be grown over time using technologies like LVM and XFS as requirements increase.</p> <p>For data-sets that require high capacity (&gt;100TB) or high-performance (&gt;1GB/sec) access, a parallel filesystem may be suitable to store data. Particularly well suited to larger files (e.g. at least 4MB per storage server), a parallel filesystem can provide additional features such as byte-range locking (the ability for multiple nodes to update sections of a large file simultaneously), and MPI-IO (the ability to control data read/written using MPI calls). Parallel filesystems work by aggregating performance and capacity from multiple servers and allowing clients (your cluster compute and login nodes) to mount the filesystem as a single mount-point. Common examples include:</p> <ul> <li>Lustre - an open-source kernel-based parallel filesystem for Linux</li> <li>GPFS - a proprietary kernel-based parallel filesystem for Linux</li> <li>BeeGFS - an open-source user-space parallel filesystem for Linux</li> </ul> <p>Your choice of filesystem will depend on the features you require, and your general familiarity with the technologies involved. As parallel filesystems do not perform well for smaller files, it is very common to deploy a parallel filesystem alongside a simple NFS-based storage solution.</p> <p>If your data-set contains a large number of files which need to be kept and searchable for a long time (&gt; 12-months) then an object storage system can also be considered. Accessed using client-agnostic protocols such as HTTPS, an object storage system is ideal for creating data archives which can include extended metadata to assist users to locate and organise their data. Most object storage systems include data redundancy options (e.g. multiple copies, object versioning and tiering), making them an excellent choice for long-term data storage. Examples of object-storage systems include:</p> <ul> <li>AWS Simple Storage Service (S3) - a cloud-hosted service with a range of data persistence options</li> <li>Swift-stack - available as both on-premise and cloud-hosted services, the SWIFT protocol is compatible with a wide range of software and services</li> <li>Ceph - an on-premise object storage system with a range of interfaces (block, object, S3, POSIX file)</li> </ul>"},{"location":"docs/hpc-concepts/storage/#network-storage-solutions","title":"Network Storage Solutions","text":""},{"location":"docs/hpc-concepts/storage/#single-server-with-nfs","title":"Single server with NFS","text":"<p>In this example, a single server is connected to a RAID 6 storage array which it is serving over NFS to the systems on the network. While simple in design and implementation, this design only provides redundancy at the RAID level.</p>"},{"location":"docs/hpc-concepts/storage/#multiple-servers-with-nfs","title":"Multiple Servers with NFS","text":"<p>In addition to the previous example, this setup features multiple storage servers which balance the load of serving the disk over NFS.</p>"},{"location":"docs/hpc-concepts/storage/#multiple-servers-with-parallel-filesystem","title":"Multiple Servers with Parallel filesystem","text":"<p>This setup features multiple RAID sets which are installed externally to the storage servers and are connected to both of them using multipath - this allows for multiple paths to the storage devices to be utilised. Using this storage, a Lustre volume has been configured which consists of a combination of all the external disks. Authorisation of access to the storage volume is managed by the metadata node, which also has dedicated storage.</p>"},{"location":"docs/hpc-concepts/storage/#additional-considerations-and-questions","title":"Additional Considerations and Questions","text":"<ul> <li>What data will need to be centrally stored?</li> <li>Where will data be coming from?<ul> <li>Are source files created within the HPC network or do they exist in the external network?</li> <li>Will compute nodes be writing out logs/results from running jobs?</li> <li>Where else might data be coming from?</li> </ul> </li> <li>Is scratch space needed?</li> <li>What level of redundancy/stability is required for the data?</li> <li>How will the data be backed up?<ul> <li>Will there be off-site backups?</li> <li>Should a separate storage medium be used?</li> <li>Does all the data need backing up or only certain files?</li> <li>For how long will we keep the data, and any backups created?</li> </ul> </li> <li>What are my disaster recovery and business continuity plans?<ul> <li>If the storage service fails, how will my users continue working?</li> <li>How can I recreate the storage service if it fails?</li> <li>How long will it take to restore any data from backups?</li> </ul> </li> </ul>"},{"location":"docs/hpc-concepts/user-management/","title":"Considerations for User Management","text":""},{"location":"docs/hpc-concepts/user-management/#user-authentication","title":"User Authentication","text":"<p>User authentication is usually performed in a server/client setup inside the HPC environment due to the unnecessary overhead of manually maintaining <code>/etc/passwd</code> on a network of nodes. A few options for network user management are:</p> <ul> <li>NIS - NIS is a directory service that enables the sharing of user and host information across a network.</li> <li>FreeIPA - FreeIPA provides all the information that NIS does as well as providing application and service information to the network. Additionally, FreeIPA uses directory structure such that information can be logically stored in a tree-like structure. It also comes with a web interface for managing the solution.</li> <li>Connecting to an externally-managed user-authentication service (e.g. LDAP, active-directory). This option is not recommended, as a large HPC cluster can put considerable load on external services. Using external user-authentication also creates a dependency for your cluster on another service, complicating troubleshooting and potentially impacting service availability.</li> </ul> <p>Note</p> <p>If the user accounts need to be consistent with accounts on the external network then the master node should have a replica service to the external networks account management system. This will allow the account information to be forwarded to the HPC network, without creating a hard-dependency on an external authentication service.</p>"},{"location":"docs/hpc-concepts/user-management/#user-access","title":"User Access","text":"<p>It is also worth considering how users will be accessing the system. A few ways that users can be accessing and interacting with the HPC environment are:</p> <ul> <li>SSH - This is the most common form of access for both users and admins. SSH will provide terminal-based access and X forwarding capabilities to the user.</li> <li>VNC - The VNC service creates a desktop session that can be remotely connected to by a user, allowing them to run graphical applications inside the HPC network.</li> <li>VPN - A VPN will provide remote network access to the HPC environment. This can be especially useful when access to the network is required from outside of the external network. Once connected to the VPN service, SSH or VNC can be used as it usually would be.</li> </ul> <p>Tip</p> <p>If running firewall services within the environment (recommended) then be sure to allow access from the ports used by the selected user access protocols.</p>"},{"location":"docs/hpc-concepts/user-management/#additional-considerations-and-questions","title":"Additional Considerations and Questions","text":"<ul> <li>What information will need to be shared between the systems?</li> <li>How will users want to access the system?</li> </ul>"},{"location":"docs/hpc-concepts/verification/","title":"Considerations for HPC Platform Verification","text":"<p>Before putting the system into a production environment it is worth verifying that the hardware and software is functioning as expected. The 2 key types of verification are:</p> <ul> <li>Configuration - Verifying that the system is functioning properly as per the server setup.</li> <li>Performance - Verifying that the performance of the system is as expected for the hardware, network and applications.</li> </ul>"},{"location":"docs/hpc-concepts/verification/#verifying-the-configuration","title":"Verifying the configuration","text":"<p>Simple configuration tests for the previous stages of the HPC platform creation will need to be performed to verify that it will perform to user expectations. For example, the following could be tested:</p> <ul> <li>Passwordless SSH between nodes performs as expected</li> <li>Running applications on different nodes within the network</li> <li>Pinging and logging into systems on separate networks</li> </ul> <p>Best practice would be to test the configuration whilst it is being setup at regular intervals to confirm functionality is still as expected. In combination with written documentation, a well practiced preventative maintenance schedule is essential to ensuring a high-quality, long-term stable platform for your users.</p>"},{"location":"docs/hpc-concepts/verification/#testing-system","title":"Testing System","text":"<p>There are multiple parts of the hardware configuration that can be tested on the systems. The main few areas are CPU, memory and interconnect (but may also include GPUs, disk drives and any other hardware that will be heavily utilised). Many applications are available for testing, including:</p> <ul> <li>Memtester</li> <li>HPL/Linpack/HPC-Challenge (HPCC)</li> <li>IOZone</li> <li>IMB</li> <li>GPUBurn</li> </ul> <p>Additionally, benchmarking can be performed using whichever applications the HPC platform is being designed to run to give more representable results for the use case.</p>"},{"location":"docs/hpc-concepts/verification/#additional-considerations-and-questions","title":"Additional Considerations and Questions","text":"<ul> <li>How will you know that a compute node is performing at the expected level?</li> <li>Gflops theoretical vs actual performance efficiency</li> <li>Network performance (bandwidth, latency, ping interval)</li> <li>How can you test nodes regularly to ensure that performance has not changed / degraded?</li> </ul>"},{"location":"docs/hpc-environment-basics/","title":"HPC Environment Basics","text":"<p>The sections here cover the basics about using Linux within the context of a HPC environment along with additional guidance on common HPC cluster resource managers.</p>"},{"location":"docs/hpc-environment-basics/hpc-usage/","title":"HPC Environment Usage","text":"<p>This section seeks to expand and explain common HPC environment use cases and software. </p>"},{"location":"docs/hpc-environment-basics/hpc-usage/job-schedulers/","title":"Job Schedulers","text":"<p>This section provides more information about jobs, including what they do, what different types are available and how they might be used.</p>"},{"location":"docs/hpc-environment-basics/hpc-usage/job-schedulers/#what-is-a-job","title":"What is a Job?","text":"<p>A job can be loosely defined as an automated research task, for example, a bash script that runs various stages in an OpenFoam simulation on a model.</p> <p>Jobs vary in size, resource usage and run time. A job could utilise multiple cores through parallel libraries or simply run on a single core.</p>"},{"location":"docs/hpc-environment-basics/hpc-usage/job-schedulers/#what-is-a-scheduler","title":"What is a Scheduler?","text":"<p>Most existing HPC environments are managed by a job scheduler; also known as the batch scheduler, workload manager, queuing system, or load-balancer. The scheduler allows multiple users to fairly share compute nodes, allowing system administrators to control how resources are made available to different groups of users. All schedulers are designed to perform the following functions:</p> <ul> <li>Allow users to submit new jobs to the research environment</li> <li>Allow users to monitor the state of their queued and running jobs</li> <li>Allow users and system administrators to control running jobs</li> <li>Monitor the status of managed resources including system load, memory available, etc.</li> </ul> <p>When a new job is submitted by a user, the research environment scheduler software assigns compute cores and memory to satisfy the job requirements. If suitable resources are not available to run the job, the scheduler adds the job to a queue until enough resources are available for the job to run. You can configure the scheduler to control how jobs are selected from the queue and executed on research environment nodes, including automatically preparing nodes to run parallel MPI jobs. Once a job has finished running, the scheduler returns the resources used by the job to the pool of free resources, ready to run another user job.</p>"},{"location":"docs/hpc-environment-basics/hpc-usage/job-schedulers/#why-use-a-scheduler","title":"Why use a Scheduler?","text":"<p>Good question. A job-scheduler is often used as a control mechanism to make sure that users don't unfairly monopolise the valuable compute resources. In extreme cases, the scheduler may be wielded by system administrators to force \u201cgood behaviour\u201d in a shared environment, and can feel like an imposition to research environment users.</p> <p>However, a job-scheduler can still be a useful tool for a research environment rather than just a control mechanism:</p> <ol> <li>It can help you organise multi-stage work flows, with batch jobs launching subsequent jobs in a defined process.</li> <li>It can automate launching of MPI jobs, finding available nodes to run applications on.</li> <li>It can help prevent accidentally over-allocating CPUs or memory, which could lead to nodes failing.</li> <li>It can help bring discipline to the environment, providing a consistent method to replicate the running of jobs in different environments.</li> </ol> <p>Your research environment comes with a job-scheduler installed, ready for you to start using. The scheduler uses very few resources when idle, so you can choose to use it if you find it useful, or run jobs manually across your research environment if you prefer.</p>"},{"location":"docs/hpc-environment-basics/hpc-usage/job-schedulers/jobs/","title":"Types of Compute Job","text":""},{"location":"docs/hpc-environment-basics/hpc-usage/job-schedulers/jobs/#local-job","title":"Local Job","text":""},{"location":"docs/hpc-environment-basics/hpc-usage/job-schedulers/jobs/#why-run-a-local-job","title":"Why Run a Local Job?","text":"<p>When using a personal research environment there isn't a need to monitor resource usage as closely as multi-user systems where miscommunication and overloaded resources can negatively impact research progress. With all the resources available to one user it is quicker and easier to run jobs simply through a terminal than using a queue system.</p> <p>Local jobs also have the benefit over schedulers by launching immediately and providing all output through a single terminal.</p>"},{"location":"docs/hpc-environment-basics/hpc-usage/job-schedulers/jobs/#running-a-local-job","title":"Running a Local Job","text":"<p>Local job scripts can be written in any language supported within the research environment. In most cases this is likely to be bash as it's a flexible and functional shell scripting language which enables users to intuitively navigate the filesystem, launch applications and manage data output.</p> <p>In the event that a job script is executable, contains a shebang(#!) specifying the launch language, and is in the current directory, it's as simple to run as:</p> <pre><code>[flight@chead1 ~]$ ./myjob.sh\n</code></pre>"},{"location":"docs/hpc-environment-basics/hpc-usage/job-schedulers/jobs/#scheduler-jobs","title":"Scheduler Jobs","text":"<p>Users can run a number of different types of job via the research environment scheduler, including:</p> <ul> <li>Batch jobs; single-threaded applications that run only on one compute core</li> <li>Array jobs; two or more similar batch jobs which are submitted together for convenience</li> <li>SMP or multi-threaded jobs; multi-threaded applications that run on two or more compute cores on the same compute node</li> <li>Parallel jobs; multi-threaded applications making use of an MPI library to run on multiple cores spread over one or more compute nodes</li> </ul> <p>The research environment job-scheduler is responsible for finding compute nodes in your research environment to run all these different types of jobs on. It keeps track of the available resources and allocates jobs to individual groups of nodes, making sure not to over-commit CPU and memory. The example below shows how a job-scheduler might allocate jobs of different types to a group of 8-CPU-core compute nodes:</p> <p></p>"},{"location":"docs/hpc-environment-basics/hpc-usage/job-schedulers/jobs/#interactive-and-batch-jobs","title":"Interactive and Batch Jobs","text":"<p>Users typically interact with research environments by running either interactive or batch (also known as non-interactive) jobs.</p> <ul> <li>An interactive job is one that the user directly controls, either via a graphical interface or by typing at the command-prompt.</li> <li>A batch job is run by writing a list of instructions that are passed to compute nodes to run at some point in the future.</li> </ul> <p>Both methods of running jobs can be equally as efficient, particularly on a personal, ephemeral research environment. Both classes of job can be of any type - for example, it's possible to run interactive parallel jobs and batch multi-threaded jobs across your research environment. The choice of which class of job-type you want to use will depend on the application you're running, and which method is more convenient for you to use.</p>"},{"location":"docs/hpc-environment-basics/hpc-usage/slurm/","title":"SLURM Job Scheduler","text":"<p>The SLURM research environment job-scheduler is an open-source project used by many high performance computing systems around the world - including many of the TOP 500 supercomputers. This section describes how to use the slurm scheduler.</p>"},{"location":"docs/hpc-environment-basics/hpc-usage/slurm/further-docs/","title":"Further Documentation","text":"<p>This guide is a quick overview of some of the many available options of the SLURM research environment scheduler. For more information on the available options, you may wish to reference some of the following available documentation for the demonstrated SLURM commands:</p> <ul> <li>Use the <code>man squeue</code> command to see a full list of scheduler queue instructions</li> <li>Use the <code>man sbatch/srun</code> command to see a full list of scheduler submission instructions</li> <li>Online documentation for the SLURM scheduler is available here</li> </ul>"},{"location":"docs/hpc-environment-basics/hpc-usage/slurm/running-jobs/array/","title":"Submitting an Array Job","text":"<p>A common workload is having a large number of jobs to run which basically do the same thing, aside perhaps from having different input data. You could generate a job-script for each of them and submit it, but that's not very convenient - especially if you have many hundreds or thousands of tasks to complete. Such jobs are known as task arrays - an embarrassingly parallel job will often fit into this category.</p> <p>A convenient way to run such jobs on a research environment is to use a task array, using the <code>-a [array_spec] | --array=[array_spec]</code> directive. Your job-script can then use the pseudo environment variables created by the scheduler to refer to data used by each task in the job. The following job-script uses the <code>$SLURM_ARRAY_TASK_ID</code>/<code>%a</code> variable to echo its current task ID to an output file:</p> <pre><code>#!/bin/bash -l\n#SBATCH --job-name=array\n#SBATCH --output=output.array.%A.%a\n#SBATCH --array=1-1000\necho \"I am $SLURM_ARRAY_TASK_ID from job $SLURM_ARRAY_JOB_ID\"\n</code></pre> <pre><code>[flight@chead1 (mycluster1) ~]$ sbatch arrayjob.sh\nSubmitted batch job 77\n[flight@chead1 (mycluster1) ~]$ squeue\n           JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n    77_[85-1000]       all    array    centos PD       0:00      1 (Resources)\n           77_71       all    array    centos  R       0:00      1 node03\n           77_72       all    array    centos  R       0:00      1 node06\n           77_73       all    array    centos  R       0:00      1 node03\n           77_74       all    array    centos  R       0:00      1 node06\n           77_75       all    array    centos  R       0:00      1 node07\n           77_76       all    array    centos  R       0:00      1 node07\n           77_77       all    array    centos  R       0:00      1 node05\n           77_78       all    array    centos  R       0:00      1 node05\n           77_79       all    array    centos  R       0:00      1 node02\n           77_80       all    array    centos  R       0:00      1 node04\n           77_81       all    array    centos  R       0:00      1 node01\n           77_82       all    array    centos  R       0:00      1 node01\n           77_83       all    array    centos  R       0:00      1 node02\n           77_84       all    array    centos  R       0:00      1 node04\n</code></pre> <p>All tasks in an array job are given a job ID with the format <code>[job_ID]_[task_number]</code> e.g. <code>77_81</code> would be job number 77, array task 81.</p> <p>Array jobs can easily be cancelled using the <code>scancel</code> command - the following examples show various levels of control over an array job:</p> <code>scancel 77</code> <p>Cancels all array tasks under the job ID <code>77</code></p> <code>scancel 77_[100-200]</code> <p>Cancels array tasks <code>100-200</code> under the job ID <code>77</code></p> <code>scancel 77_5</code> <p>Cancels array task <code>5</code> under the job ID <code>77</code></p>"},{"location":"docs/hpc-environment-basics/hpc-usage/slurm/running-jobs/batch/","title":"Submitting a Batch Job","text":"<p>Batch (or non-interactive) jobs allow users to leverage one of the main benefits of having a research environment scheduler; jobs can be queued up with instructions on how to run them and then executed across the research environment while the user does something else. Users submit jobs as scripts, which include instructions on how to run the job - the output of the job (stdout and stderr in Linux terminology) is written to a file on disk for review later on. You can write a batch job that does anything that can be typed on the command-line.</p> <p>We'll start with a basic example - the following script is written in bash (the default Linux command-line interpreter). You can create the script yourself using the Nano command-line editor - use the command <code>nano simplejobscript.sh</code> to create a new file, then type in the contents below. The script does nothing more than print some messages to the screen (the echo lines), and sleeps for 120 seconds. We've saved the script to a file called <code>simplejobscript.sh</code> - the <code>.sh</code> extension helps to remind us that this is a shell script, but adding a filename extension isn't strictly necessary for Linux.</p> <pre><code>#!/bin/bash -l\necho \"Starting running on host $HOSTNAME\"\nsleep 120\necho \"Finished running - goodbye from $HOSTNAME\"\n</code></pre> <p>Info</p> <p>We use the <code>-l</code> option to bash on the first line of the script to request a login session. This ensures that environment modules can be loaded as required as part of your script.</p> <p>We can execute that script directly on the login node by using the command <code>bash simplejobscript.sh</code> - after a couple of minutes, we get the following output:</p> <pre><code>Started running on host chead1\nFinished running - goodbye from chead1\n</code></pre> <p>To submit your job script to the research environment job scheduler, use the command <code>sbatch simplejobscript.sh</code>. The job scheduler should immediately report the job-ID for your job; your job-ID is unique for your current research environment - it will never be repeated once used.</p> <pre><code>[flight@chead1 (mycluster1) ~]$ sbatch simplejobscript.sh\nSubmitted batch job 21\n\n[flight@chead1 (mycluster1) ~]$ ls\nsimplejobscript.sh  slurm-21.out\n\n[flight@chead1 (mycluster1) ~]$ cat slurm-21.out\nStarting running on host node01\nFinished running - goodbye from node01\n</code></pre>"},{"location":"docs/hpc-environment-basics/hpc-usage/slurm/running-jobs/dynamic-vars/","title":"Dynamic Scheduler Variables","text":"<p>Your research environment job scheduler automatically creates a number of pseudo environment variables which are available to your job-scripts when they are running on research environment compute nodes, along with standard Linux variables. Useful values include the following:</p> <ul> <li><code>$HOME</code> The location of your home-directory</li> <li><code>%u</code> / <code>$USER</code> The Linux username of the submitting user. The <code>%u</code> substitution should only be used in your job scheduler filename directives.</li> <li><code>$HOSTNAME</code> The Linux hostname of the compute node running the job</li> <li><code>%a</code> / <code>$SLURM_ARRAY_TASK_ID</code> Job array ID (index) number. The <code>%a</code> substitution should only be used in your job scheduler filename directives<sup>1</sup></li> <li><code>%A</code> / <code>$SLURM_ARRAY_JOB_ID</code> Job allocation number for an array job. The <code>%A</code> substitution should only be used in your job scheduler filename directives<sup>1</sup></li> <li><code>%j</code> / <code>$SLURM_JOBID</code> Job allocation number. The <code>%j</code> substitution should only be used in your job scheduler filename directives</li> </ul> <p>More information on this.</p> <ol> <li> <p>These relate to task array jobs, which are covered in a later section.\u00a0\u21a9\u21a9</p> </li> </ol>"},{"location":"docs/hpc-environment-basics/hpc-usage/slurm/running-jobs/interactive/","title":"Submitting an Interactive Job","text":"<p>You can start a new interactive job on your research environment by using the <code>srun</code> command; the scheduler will search for an available compute node, and provide you with an interactive login shell on the node if one is available.</p> <pre><code>[flight@chead1 (mycluster1) ~]$ srun --pty /bin/bash\n[flight@node01 (mycluster1) ~]$\n[flight@node01 (mycluster1) ~]$ squeue\n           JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n               3       all     bash    centos R       0:39      1 node01\n</code></pre> <p>In the above example, the <code>srun</code> command is used together with two options: <code>--pty</code> and <code>/bin/bash</code>. The <code>--pty</code> option executes the task in pseudo terminal mode, allowing the session to act like a standard terminal session. The <code>/bin/bash</code> option is the command that you wish to run - here the default Linux shell, BASH.</p> <p>Alternatively, the <code>srun</code> command can also be executed from an interactive desktop session; the job-scheduler will automatically find an available compute node to launch the job on. Applications launched from within the <code>srun</code> session are executed on the assigned research environment compute node.</p> <p></p> <p>Tip</p> <p>The Slurm scheduler does not automatically set up your session to allow you to run graphical applications inside an interactive session. Once your interactive session has started, you must run the following command before running a graphical application: <code>export DISPLAY=chead1$DISPLAY</code></p> <p>Warning</p> <p>Running X applications from a compute node may not work due to missing X libraries on the compute node, these can be installed from an SSH session into a compute node with <code>sudo yum groupinstall \"X Window System\"</code></p> <p>When you've finished running your application in your interactive session, simply type <code>logout</code>, <code>exit</code>, or press Ctrl+D to exit the interactive job.</p> <p>If the job-scheduler could not satisfy the resource you've requested for your interactive job (e.g. all your available compute nodes are busy running other jobs), it will report back after a few seconds with an error:</p> <pre><code>[flight@chead1 (mycluster1) ~]$ srun --pty /bin/bash\nsrun: job 20 queued and waiting for resources\n</code></pre>"},{"location":"docs/hpc-environment-basics/hpc-usage/slurm/running-jobs/job-configuration/","title":"Providing Job Configuration","text":"<p>In order to promote efficient usage of the research environment - the job-scheduler is automatically configured with default run-time limits for jobs. These defaults can be overridden by users to help the scheduler understand how you want it to run your job. </p> <p>Job instructions can be provided in two ways; they are:</p> <ol> <li>On the command line, as parameters to your <code>sbatch</code> or <code>srun</code> command. For example, you can set the name of your job using the <code>--job-name=[name] | -J [name]</code> option:     <pre><code>[flight@chead1 (mycluster1) ~]$ sbatch --job-name=mytestjob simplejobscript.sh\nSubmitted batch job 51\n\n[flight@chead1 (mycluster1) ~]$ squeue\n             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n                51       all mytestjo    centos  R       0:02      1 node01\n</code></pre></li> <li>In your job script, by including scheduler directives at the top of your job script - you can achieve the same effect as providing options with the <code>sbatch</code> or <code>srun</code> commands. Create an example job script or modify your existing script to include a scheduler directive to use a specified job name:     <pre><code>#!/bin/bash -l\n#SBATCH --job-name=mytestjob\necho \"Starting running on host $HOSTNAME\"\nsleep 120\necho \"Finished running - goodbye from $HOSTNAME\"\n</code></pre></li> </ol> <p>Including job scheduler instructions in your job-scripts is often the most convenient method of working for batch jobs - follow the guidelines below for the best experience:</p> <ul> <li>Lines in your script that include job-scheduler directives must start with <code>#SBATCH</code> at the beginning of the line.</li> <li>You can put multiple instructions separated by a space on a single line starting with <code>#SBATCH</code></li> <li>The scheduler will parse the script from top to bottom and set instructions in order; if you set the same parameter twice, the second value will be used.</li> <li>Instructions are parsed at job submission time, before the job itself has actually run. This means you can't, for example, tell the scheduler to put your job output in a directory that you create in the job-script itself - the directory will not exist when the job starts running, and your job will fail with an error.</li> <li>You can use dynamic variables in your instructions (see next)</li> </ul> <p>Warning</p> <p>After <code>#!/bin/bash -l</code> write all of your <code>#SBATCH</code> lines. As soon as the interpreter reads a normal script line it will stop looking for <code>#SBATCH</code> lines.</p> <p>More information on this</p>"},{"location":"docs/hpc-environment-basics/hpc-usage/slurm/running-jobs/job-configuration/#common-job-configuration-examples","title":"Common Job Configuration Examples","text":""},{"location":"docs/hpc-environment-basics/hpc-usage/slurm/running-jobs/job-configuration/#setting-output-file-location","title":"Setting Output File Location","text":"<p>To set the output file location for your job, use the <code>-o [file_name] | --output=[file_name]</code> option - both standard-out and standard-error from your job-script, including any output generated by applications launched by your job-script will be saved in the filename you specify.</p> <p>By default, the scheduler stores data relative to your home-directory - but to avoid confusion, we recommend specifying a full path to the filename to be used. Although Linux can support several jobs writing to the same output file, the result is likely to be garbled - it's common practice to include something unique about the job (e.g. its job-ID) in the output filename to make sure your job's output is clear and easy to read.</p> <p>Note</p> <p>The directory used to store your job output file must exist and be writable by your user before you submit your job to the scheduler. Your job may fail to run if the scheduler cannot create the output file in the directory requested.</p> <p>The following example uses the <code>--output=[file_name]</code> instruction to set the output file location:</p> <pre><code>#!/bin/bash -l\n#SBATCH --job-name=myjob --output=output.%j\n\necho \"Starting running on host $HOSTNAME\"\nsleep 120\necho \"Finished running - goodbye from $HOSTNAME\"\n</code></pre> <p>In the above example, assuming the job was submitted as the <code>centos</code> user and was given the job-ID number <code>24</code>, the scheduler will save the output data from the job in the filename <code>/home/centos/output.24</code>.</p>"},{"location":"docs/hpc-environment-basics/hpc-usage/slurm/running-jobs/job-configuration/#setting-working-directory","title":"Setting Working Directory","text":"<p>By default, jobs are executed from your home-directory on the research environment (i.e. <code>/home/&lt;your-user-name&gt;</code>, <code>$HOME</code> or <code>~</code>). You can include <code>cd</code> commands in your job-script to change to different directories; alternatively, you can provide an instruction to the scheduler to change to a different directory to run your job. The available options are:</p> <p><code>-D | --workdir=[dir_name]</code> - instruct the job scheduler to move into the directory specified before starting to run the job on a compute node</p> <p>Note</p> <p>The directory specified must exist and be accessible by the compute node in order for the job you submitted to run.</p>"},{"location":"docs/hpc-environment-basics/hpc-usage/slurm/running-jobs/job-configuration/#waiting-for-a-previous-job-before-running","title":"Waiting for a Previous Job Before Running","text":"<p>You can instruct the scheduler to wait for an existing job to finish before starting to run the job you are submitting with the <code>-d [state:job_id] | --depend=[state:job_id]</code> option. For example, to wait until the job with ID 75 has finished before starting the job, you could use the following syntax:</p> <pre><code>[flight@chead1 (mycluster1) ~]$ squeue\n             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n                75       all    myjob    centos  R       0:01      1 node01\n\n[flight@chead1 (mycluster1) ~]$ sbatch --dependency=afterok:75 mytestjob.sh\nSubmitted batch job 76\n\n[flight@chead1 (mycluster1) ~]$ squeue\n             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n                76       all    myjob    centos PD       0:00      1 (Dependency)\n                75       all    myjob    centos  R       0:15      1 node01\n</code></pre>"},{"location":"docs/hpc-environment-basics/hpc-usage/slurm/running-jobs/requesting-resources/","title":"Requesting More Resources","text":"<p>By default, jobs are constrained to the default set of resources - users can use scheduler instructions to request more resources for their jobs. The following documentation shows how these requests can be made.</p>"},{"location":"docs/hpc-environment-basics/hpc-usage/slurm/running-jobs/requesting-resources/#running-multi-threaded-jobs","title":"Running Multi-Threaded Jobs","text":"<p>If users want to use multiple cores on a compute node to run a multi-threaded application, they need to inform the scheduler - this allows jobs to use multiple cores without needing to rely on any interconnect. Using multiple CPU cores is achieved by specifying the <code>-n</code> or <code>--ntasks=&lt;number&gt;</code> option in either your submission command or the scheduler directives in your job script. The <code>--ntasks</code> option informs the scheduler of the number of cores you wish to reserve for use. If the parameter is omitted, the default <code>--ntasks=1</code> is assumed. You could specify the option <code>-n 4</code> to request 4 CPU cores for your job. Besides the number of tasks, you will need to add <code>--nodes=1</code> to your scheduler command or at the top of your job script with <code>#SBATCH --nodes=1</code>, this will set the maximum number of nodes to be used to 1 and prevent the job selecting cores from multiple nodes.</p> <pre><code>#!/bin/bash -l\n#SBATCH --nodes=2\n#SBATCH --ntasks=2\n#SBATCH --mem=200\necho \"Example asking for 2 CPU cores and 2 nodes\"\n</code></pre> <p>Warning</p> <p>If you request more cores than are available on a node in your research environment, the job will not run until a node capable of fulfilling your request becomes available. The scheduler will display the error in the output of the <code>squeue</code> command</p>"},{"location":"docs/hpc-environment-basics/hpc-usage/slurm/running-jobs/requesting-resources/#running-parallel-mpi-jobs","title":"Running Parallel (MPI) Jobs","text":"<p>If users want to run parallel jobs via a messaging passing interface (MPI), they need to inform the scheduler - this allows jobs to be efficiently spread over compute nodes to get the best possible performance. Using multiple CPU cores across multiple nodes is achieved by specifying the <code>-N</code> or <code>--nodes=&lt;minnodes[-maxnodes]&gt;</code> option - which requests a minimum (and optional maximum) number of nodes to allocate to the submitted job. If only the <code>minnodes</code> count is specified - then this is used for both the minimum and maximum node count for the job.</p> <code>--nodes=1-4</code> <p>Example of how to request a minimum of 1 node and maximum of 4.</p> <p>You can request multiple cores over multiple nodes using a combination of scheduler directives either in your job submission command or within your job script. Some of the following examples demonstrate how you can obtain cores across different resources;</p> <code>--nodes=2 --ntasks=16</code> <p>Requests 16 cores across 2 compute nodes</p> <code>--nodes=2</code> <p>Requests all available cores of 2 compute nodes</p> <code>--ntasks=16</code> <p>Requests 16 cores across any available compute nodes</p> <p>Warning</p> <p>If you request more CPU cores than your research environment can accommodate, your job will wait in the queue.</p>"},{"location":"docs/hpc-environment-basics/hpc-usage/slurm/running-jobs/requesting-resources/#requesting-more-memory","title":"Requesting More Memory","text":"<p>In order to promote best use of the research environment scheduler - particularly in a shared environment, it is recommended to inform the scheduler the maximum required memory per submitted job. This helps the scheduler appropriately place jobs on the available nodes in the research environment.</p> <p>You can specify the maximum amount of memory required per submitted job with the <code>--mem=&lt;MB&gt;</code> option. This informs the scheduler of the memory required for the submitted job. Optionally - you can also request an amount of memory per CPU core rather than a total amount of memory required per job. To specify an amount of memory to allocate per core, use the <code>--mem-per-cpu=&lt;MB&gt;</code> option.</p> <p>Examples:</p> <ul> <li><code>--mem=200</code> - Requesting 200MB of memory.</li> <li><code>--mem-per-cpu=10</code> - Requesting 10MB of memory per CPU.</li> </ul> <p>Note</p> <p>When running a job across multiple compute hosts, the <code>--mem=&lt;MB&gt;</code> option informs the scheduler of the required memory per node</p>"},{"location":"docs/hpc-environment-basics/hpc-usage/slurm/running-jobs/requesting-resources/#requesting-a-longer-runtime","title":"Requesting a Longer Runtime","text":"<p>In order to promote best-use of the research environment scheduler, particularly in a shared environment, it is recommend to inform the scheduler the amount of time the submitted job is expected to take. You can inform the research environment scheduler of the expected runtime using the <code>-t, --time=&lt;time&gt;</code> option. For example - to submit a job that runs for 2 hours, the following example job script could be used:</p> <pre><code>#!/bin/bash -l\n#SBATCH --job-name=sleep\n#SBATCH -D $HOME/\n#SBATCH --time=0-2:00\nsleep 7200\n</code></pre> <p>You can then see any time limits assigned to running jobs using the command <code>squeue --long</code>:</p> <pre><code>[flight@chead1 (mycluster1) ~]$ squeue --long\nTue Aug 30 10:55:55 2016\n             JOBID PARTITION     NAME     USER    STATE       TIME TIME_LIMI  NODES NODELIST(REASON)\n              1163       all    sleep    centos  RUNNING       0:07   2:00:00      1 ip-10-75-1-42\n</code></pre>"},{"location":"docs/hpc-environment-basics/hpc-usage/slurm/using-scheduler/default_resources/","title":"Viewing Default Resources","text":"<p>In order to promote efficient usage of your research environment, the job-scheduler automatically sets a number of default resources for your jobs when you submit them. These defaults must be overridden by users to help the scheduler understand how you want it to run your job - if we don't include any instructions to the scheduler, then our job will take the defaults shown below:</p> <ul> <li>Number of CPU cores for your job: 1</li> <li>Number of nodes for your job: the default behavior is to allocate enough nodes to satisfy the requirements of the number of CPUs requested</li> </ul> <p>You can view all default resource limits by running the following command:</p> <pre><code>[root@chead1(mycluster1) ~]# scontrol show config | grep Def\nCpuFreqDef              = Unknown\nDefMemPerNode           = UNLIMITED\nMpiDefault              = none\nSallocDefaultCommand    = (null)\n</code></pre> <p>This documentation will explain how to change these limits to suit the jobs that you want to run. You can also disable these limits if you prefer to control resource allocation manually by yourself.</p>"},{"location":"docs/hpc-environment-basics/hpc-usage/slurm/using-scheduler/host-status/","title":"Viewing Compute Host Status","text":"<p>Users can use the <code>sinfo -Nl</code> command to view the status of compute node hosts in your research environment.</p> <pre><code>[flight@chead1 (mycluster1) ~]$ sinfo -Nl\nFri Aug 26 14:46:34 2016\nNODELIST        NODES PARTITION       STATE CPUS    S:C:T MEMORY TMP_DISK WEIGHT AVAIL_FE REASON\nnode01       1      all*        idle    2    2:1:1   3602    20462      1   (null) none\nnode02      1      all*        idle    2    2:1:1   3602    20462      1   (null) none\nnode03      1      all*        idle    2    2:1:1   3602    20462      1   (null) none\nnode04      1      all*        idle    2    2:1:1   3602    20462      1   (null) none\nnode05      1      all*        idle    2    2:1:1   3602    20462      1   (null) none\nnode06      1      all*        idle    2    2:1:1   3602    20462      1   (null) none\nnode07      1      all*        idle    2    2:1:1   3602    20462      1   (null) none\n</code></pre> <p>The <code>sinfo -Nl</code> output will show (from left-to-right):</p> <ul> <li>The hostname of your compute nodes</li> <li>The number of nodes in the list</li> <li>The node partition the node belongs to</li> <li>Current usage of the node - if no jobs are running, the state will be listed as <code>idle</code>. If a job is running, the state will be listed as <code>allocated</code></li> <li>The detected number of CPUs (including hyper-threaded cores)</li> <li>The number of sockets, cores and threads per node</li> <li>The amount of memory in MB per node</li> <li>The amount of disk space in MB available to the <code>/tmp</code> partition per node</li> <li>The scheduler weighting</li> </ul> <p>Using the command <code>sinfo</code> without <code>-Nl</code> will display only some information and in a different order.</p> <pre><code>[flight@chead1 (mycluster1) ~]$ sinfo\nPARTITION AVAIL  TIMELIMIT  NODES  STATE NODELIST\nall*         up   infinite      2   idle cnode[01-02]\n</code></pre>"},{"location":"docs/hpc-environment-basics/hpc-usage/slurm/using-scheduler/view-control/","title":"Viewing and Controlling Queued Jobs","text":"<p>Once your job has been submitted, use the <code>squeue</code> command to view the status of the job queue. If you have available compute nodes, your job should be shown in the <code>R</code> (running) state; if your compute nodes are busy your job may be shown in the <code>PD</code> (pending) state until compute nodes are available to run it. If a job is in <code>PD</code> state - the reason for being unable to run will be displayed in the <code>NODELIST(REASON)</code> column of the <code>squeue</code> output.</p> <pre><code>[flight@chead1 (mycluster1) ~]$ squeue\n         JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n            41       all simplejo    centos  R       0:03      1 node01\n            42       all simplejo    centos  R       0:00      1 node01\n</code></pre> <p>You can keep running the <code>squeue</code> command until your job finishes running and disappears from the queue. The output of your batch job will be stored in a file for you to look at. The default location to store the output file is your home directory. You can use the Linux <code>more</code> command to view your output file:</p> <pre><code>[flight@chead1 (mycluster1) ~]$ more slurm-42.out\nStarting running on host node01\nFinished running - goodbye from node01\n</code></pre> <p>Your job runs on whatever node the scheduler can find which is available for use - you can try submitting a bunch of jobs at the same time, and using the <code>squeue</code> command to see where they run. The scheduler is likely to spread them around over different nodes (if you have multiple nodes). The login node is not included in your research environment for scheduling purposes - jobs submitted to the scheduler will only be run on your research environment compute nodes. You can use the <code>scancel &lt;job-ID&gt;</code> command to delete a job you've submitted, whether it's running or still in the queued state.</p> <pre><code>[flight@chead1 (mycluster1) ~]$ sbatch simplejobscript.sh\nSubmitted batch job 46\n[flight@chead1 (mycluster1) ~]$ sbatch simplejobscript.sh\nSubmitted batch job 47\n[flight@chead1 (mycluster1) ~]$ sbatch simplejobscript.sh\nSubmitted batch job 48\n[flight@chead1 (mycluster1) ~]$ squeue\n             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n                43       all simplejo    centos  R       0:04      1 node01\n                44       all simplejo    centos  R       0:04      1 node01\n                45       all simplejo    centos  R       0:04      1 node02\n                46       all simplejo    centos  R       0:04      1 node02\n                47       all simplejo    centos  R       0:04      1 node03\n                48       all simplejo    centos  R       0:04      1 node03\n\n[flight@chead1 (mycluster1) ~]$ scancel 47\n[flight@chead1 (mycluster1) ~]$ squeue\n             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n                43       all simplejo    centos  R       0:11      1 node01\n                44       all simplejo    centos  R       0:11      1 node01\n                45       all simplejo    centos  R       0:11      1 node02\n                46       all simplejo    centos  R       0:11      1 node02\n                48       all simplejo    centos  R       0:11      1 node03\n</code></pre>"},{"location":"docs/hpc-environment-basics/linux-usage/","title":"Linux Environment Usage","text":"<p>This section covers the basic access and interaction with a Linux environment on a HPC environment. Providing guidance to users on any client system (Windows, Mac, Linux) with guidance to perform common tasks. </p>"},{"location":"docs/hpc-environment-basics/linux-usage/genders-pdsh/","title":"Genders and PDSH","text":""},{"location":"docs/hpc-environment-basics/linux-usage/genders-pdsh/#overview","title":"Overview","text":"<p>Genders provides a simple method for categorising a node inventory. To learn more about genders, read the official tutorial.</p> <p>PDSH provides a CLI tool for performing commands on multiple nodes at once, utilising the data stored in genders. To learn more about using PDSH, read the manual page for the command <code>man pdsh</code>.</p> <p>Info</p> <p>OpenFlight has it's own build of PDSH which can be installed here and is explained further on in the documentation</p>"},{"location":"docs/hpc-environment-basics/linux-usage/genders-pdsh/#installing-genders-and-pdsh","title":"Installing Genders and PDSH","text":"<p>The packages should be available in the package manager for your distribution. To install on CentOS/RHEL/Rocky:</p> <pre><code>dnf install pdsh genders\n</code></pre> <p>Note</p> <p>You will need <code>sudo</code> permissions to install packages, see becoming the root user for more information</p>"},{"location":"docs/hpc-environment-basics/linux-usage/genders-pdsh/#creating-a-genders-file","title":"Creating a Genders File","text":"<p>Open the genders file (<code>/etc/genders</code>) with your preferred text editor and add a gender in with the following format:</p> <pre><code>node01,node02,node03,node04,node05 gendername\n</code></pre> <p>There are alternate formats that make writing a gender easier:</p> <pre><code>node01,node02,node03,node04,node05:    node[01-05]\nnode3,node7,node9,node10,node11:       node[3,7,9-11]\nnodei,nodej,node0,node1,node2:         nodei,nodej,node[0-2]\n</code></pre> <p>For example:</p> <pre><code>node[01-05] mygroup1\nnodeA,nodeB,node[1,2,3,10-20],nodeC mygroup2\n</code></pre> <p>After adding the desired gender(s), save and close the file.</p>"},{"location":"docs/hpc-environment-basics/linux-usage/genders-pdsh/#finding-the-names-of-your-compute-nodes","title":"Finding the Names of Your Compute Nodes","text":"<p>In best practice, the hostnames of compute nodes usually follow a sequential order (e.g. node01, node02, node03... node10).</p> <p>Users can find the names of their compute nodes by using the <code>nodeattr</code> command with a group; e.g.</p> Show a space-separated list of hosts in the group 'nodes'<pre><code>nodeattr -s nodes\n</code></pre> Show a comma-separated list of hosts in the group 'group'<pre><code>nodeattr -c group\n</code></pre> Show a newline-separate list of hosts in the group 'groups'<pre><code>nodeattr -n groups\n</code></pre>"},{"location":"docs/hpc-environment-basics/linux-usage/genders-pdsh/#using-pdsh","title":"Using PDSH","text":"<p>Users can run a command across many hosts at once using the pdsh command. This can be useful if users want to make the same change to multiple systems in the research environment - for example, installing a new software package. The <code>pdsh</code> command can take a number of parameters that control how commands are processed; for example:</p> <p>Run `uptime` across hosts in the 'all' group<pre><code>pdsh -g all uptime\n</code></pre> Install the package `screen` with `yum` on all hosts in the 'nodes' group<pre><code>pdsh -g nodes 'sudo yum -y install screen'\n</code></pre></p> Check usage of `/tmp` on all hosts in the 'nodes' group one at a time<pre><code>pdsh -g nodes -f 1 df -h /tmp\n</code></pre> Run `which ldconfig` on two specified hosts<pre><code>pdsh -w node01,node03 which ldconfig\n</code></pre>"},{"location":"docs/hpc-environment-basics/linux-usage/cli-basics/","title":"CLI Basics","text":"<p>Using the Linux CLI is necessary as HPC environments typically use Linux due to it being freely available and therefore not requiring license fees for many nodes in the system. This section covers a few of the basics of using the CLI, for use as an introduction or refresher. If you would like to know more about using linux, there are a large number of tutorials available online.</p>"},{"location":"docs/hpc-environment-basics/linux-usage/cli-basics/becoming-root/","title":"Becoming the Root User","text":"<p>Most research environment operations, including starting applications and running jobs, should be performed by a regular user. However - for some privileged operations, users may need to change to being the root user. Users can prefix any command they want to run as the root user with the <code>sudo</code> command; e.g.</p> <p><code>sudo yum install screen</code></p> <p>To get a Linux shell with root privileges, please login as your standard user then execute the command <code>sudo -s</code>.</p> <p>Danger</p> <p>Users must exercise caution when running commands as root, as they have the potential to disrupt research environment operations.</p> <p>Note</p> <p>Some users may not have root privileges. This is at the discretion of your admin or system configuration.</p> <p>You can log out of the root user by running the command <code>exit</code>, <code>logout</code>, or by pressing Ctrl+D</p>"},{"location":"docs/hpc-environment-basics/linux-usage/cli-basics/logging-in/","title":"Logging In","text":"<p>Usually a HPC environment will have a \"login\" or \"gateway\" node which is the entrypoint to the rest of the nodes.</p> <p>You can access the login node for your research environment using SSH to connect to the externally facing IP address of the login node. You will need to use the SSH keypair configured for the research environment in order to access it.</p> <p>When you login to the research environment via SSH, you are automatically placed in your home-directory. This area is shared across all compute nodes in the research environment, and is mounted in the same place on every compute node. Data copied to the research environment or created in your home-directory on the login node is also accessible from all compute nodes.</p>  Linux /  Mac Windows <p>To access the research environment login node from a Linux or Mac client, use the following command: <pre><code>ssh -i mypublickey.pem flight@52.50.141.144\n</code></pre></p> <p>Where:</p> <ul> <li><code>mypublickey.pem</code> is the name of your private key associated with the public SSH key of your user </li> <li><code>flight</code> is the username of the user on the research environment</li> <li><code>52.50.141.144</code> is the Access-IP address for the gateway node of the research environment</li> </ul> <p>If you are accessing from a Windows client using the Putty utility, the private key associated with the account will need to be converted to ppk format from pem to be compatible with Putty. This can be done as follows:</p> <ul> <li> <p>Open PuTTYgen (this will already be installed on your system if Putty was installed using <code>.msi</code> and not launched from the <code>.exe</code> - if you do not think you have this, download putty-installer from here</p> </li> <li> <p>Select Conversions -&gt; Import Key</p> </li> <li> <p>Locate .pem file and click open</p> </li> <li> <p>Click Save Private Key</p> </li> <li> <p>Answer Yes to saving without a passphrase</p> </li> <li> <p>Input the name for the newly generated ppk to be saved as</p> </li> </ul> <p>To load the key in Putty, select Connection -&gt; SSH -&gt; Auth, click Browse and select the ppk that was generated from the above steps.</p> <p></p> <p>Next, enter the username and IP address of the research environment login node in the \u201cHost Name\u201d box provided (in the Session section):</p> <p></p> <p>The first time you connect to your research environment, you will be prompted to accept a new server SSH host-key. This happens because you've never logged in to your research environment before - it should only happen the first time you login; click OK to accept the warning. Once connected to the research environment, you should be logged in to the research environment login node as your user.</p> <p></p>"},{"location":"docs/hpc-environment-basics/linux-usage/cli-basics/moving-between-hosts/","title":"Moving Between Hosts","text":"<p>Usually SSH is used to navigate between systems in a HPC environment. This provides a secure shell connection to another host in order to access the CLI on that system. </p> <p>The first connection between two hosts will create a trust relationship. Depending on the user management solution for the HPC environment it may be possible to navigate between hosts without being prompted for a password or needing to specify a private key which would allow moving quickly and seamlessly between hosts and simplifying the running of large-scale jobs which involve multiple nodes. </p> <p>From the command line, a user can simply use the <code>ssh &lt;node-name&gt;</code> command to login to one of the compute nodes from the login node. For example, to login to a compute node named <code>node01</code> from the current host, use the command: <pre><code>ssh node01\n</code></pre></p> <p>Use the command <code>logout</code> or <code>exit</code> (or press Ctrl+D) to exit the compute node and return to the previous host.</p>"},{"location":"docs/hpc-environment-basics/linux-usage/cli-basics/understanding-the-cli/","title":"Understanding the CLI","text":"<p>Usually a lot of HPC environment usage takes place via the Linux command line. For unfamiliar users here is a brief description of a typical CLI prompt.</p> <pre><code>[flight@chead1 (mycluster1) folder]$ echo Hello World!\nHello World!\n</code></pre> <ul> <li><code>flight</code> is the name of the user.</li> <li><code>chead1</code> is the node the user is currently on.</li> <li><code>(mycluster1)</code> is the name of the cluster being used. (This is specific to the Flight Environment)</li> <li><code>folder</code> is the directory the user is currently in. If this is a <code>~</code> then that means you are in your home directory.</li> <li><code>$</code> indicates whether the user is a root user or normal user (root users see a <code>#</code>).</li> <li><code>echo Hello World!</code> is a command that prints out \"Hello World!\". This is where commands you type will go.</li> </ul>"},{"location":"docs/hpc-environment-basics/linux-usage/working-with-data/","title":"Working with Data and Files","text":"<p>This section details accessing data, common ways that data can be shared and some options for archival in a HPC environment. </p>"},{"location":"docs/hpc-environment-basics/linux-usage/working-with-data/copy-between-hosts/","title":"Copying Data Between Hosts","text":"<p>In a HPC environment there is likely to be a shared filesystem, so it is not normally necessary to copy data directly between nodes in the research environment. Users simply need to place the data to be shared in their home-directory on the login node, and it will be available on all compute nodes in the same location.</p> <p>If necessary, users can use the <code>scp</code> command to copy files between nodes like:</p> <pre><code>scp node01:/tmp/myfile.txt . # (1)!\n</code></pre> <ol> <li>Copy the file <code>myfile.txt</code> stored under <code>/tmp/</code> on <code>node01</code> to the <code>cwd</code> (<code>.</code>) of this host</li> </ol> <p>Alternatively, users could login to a host that has unshared data on and copy the data back to the shared filesystem.</p>"},{"location":"docs/hpc-environment-basics/linux-usage/working-with-data/storage-types/","title":"Storage Types","text":""},{"location":"docs/hpc-environment-basics/linux-usage/working-with-data/storage-types/#shared-storage","title":"Shared Storage","text":"<p>With HPC environments typically combining multiple hosts there comes the requirement to be able to share user data quickly between the available resources. Due to this, HPC environments usually have some sort of shared storage solution.</p> <p>The shared storage solution can share user home directories between all the nodes, create shared mount-points for a project or department and even provide redundancy by using resilient storage back-ends. </p>"},{"location":"docs/hpc-environment-basics/linux-usage/working-with-data/storage-types/#local-scratch","title":"Local Scratch","text":"<p>Your compute nodes may have an amount of disk space available to store temporary data under the <code>/tmp</code> mount-point. This area is intended for temporary data created during compute jobs, and shouldn't be used for long-term data storage. Compute nodes are configured to clear up temporary space automatically, removing orphan data left behind by jobs.</p> <p>Users must make sure that they copy data they want to keep back to the shared filesystem after compute jobs have been completed.</p> <p>Utilising local scratch can dramatically increase the execution speed of HPC workflows that generate a lot of data during runtime where a lot of it isn't required for analysing results. By utilising the local scratch, this data can typically be written to, and read from, quicker than network shared storage.</p>"},{"location":"docs/hpc-environment-basics/linux-usage/working-with-data/your-home-directory/","title":"Your Home Directory","text":"<p>Linux automatically places users in their home-directory when they login to a node. By default, Linux will create your home-directory under the <code>/home/</code> directory, named <code>flight</code> (<code>/home/flight</code>). However, on some systems the home directory location may differ.</p> <p>The Linux command line will accept the <code>~</code> (tilde) symbol as a substitute for the currently logged-in user's home-directory. The environment variable <code>$HOME</code> is also set to this value by default. Hence, the following three commands are all equivalent when logged in as the user flight:</p> <ul> <li><code>ls /home/flight</code></li> <li><code>ls ~</code></li> <li><code>ls $HOME</code></li> </ul> <p>The root user in Linux has special meaning as a privileged user, and usually does not have a shared home-directory across a research environment. The root account on all nodes is likely to have a home-directory in <code>/root</code>, which is separate for every node. For security reasons, users are not permitted to login to a node as the root user directly - please login as a standard user and use the <code>sudo</code> command to get privileged access.</p>"},{"location":"docs/hpc-environment-basics/linux-usage/working-with-data/copying-data/","title":"Copying Data to a HPC Environment","text":"<p>Many compute workloads involve processing data on the research environment - users often need to copy data files to the research environment for processing, and retrieve processed data and results afterwards. This documentation describes a number of methods of working with data on your research environment, depending on how users prefer to transfer it.</p> <p>Later in the documentation a method to move data using the Flight Web Suite will be described.</p>"},{"location":"docs/hpc-environment-basics/linux-usage/working-with-data/copying-data/object-storage-archiving-data/","title":"Object Storage for Archiving Data","text":"<p>As an alternative to copying data back to your client machine, users may prefer to upload their data to a cloud-based object storage service instead. Cloud storage solutions such as AWS S3, Dropbox and SWIFT have command-line tools which can be used to connect existing cloud storage to your research environment. Benefits of using an object-based storage service include:</p> <ul> <li>Data is kept safe and does not have to be independently backed-up</li> <li>Storage is easily scalable, with the ability for data to grow to practically any size</li> <li>You only pay for what you use; you do not need to buy expansion room in advance</li> <li>Storage service providers often have multiple tiers available, helping to reduce the cost of storing data</li> <li>Data storage and retrieval times may be improved, as storage service providers typically have more bandwidth than individual sites</li> <li>Your company, institution or facility may receive some storage capacity for free which you could use</li> </ul> <p>Object storage is particularly useful for archiving data, as it typically provides a convenient, accessible method of storing data which may need to be shared with a wide group of individuals.</p>"},{"location":"docs/hpc-environment-basics/linux-usage/working-with-data/copying-data/using-cli/","title":"Using CLI","text":"<p>Usually a HPC environment will have a \"login\" or \"gateway\" node which is the entrypoint to the rest of the nodes. From outside of the HPC environment the data will need to be copied to this system (likely to shared storage) to make it available.</p>  Linux /  Mac Windows <p>Linux and Mac users can use in-built SSH support to copy files. </p> <pre><code>scp mydata.zip flight@52.48.62.34:/home/flight/ # (1)!\n</code></pre> <ol> <li>Copy local file <code>mydata.zip</code> to <code>/home/flight/mydata.zip</code> on host <code>52.48.62.34</code> authenticating as the user <code>flight</code>.</li> </ol> <p>Windows users can download and install the pscp command to perform the same operation (you may need your .pem key in .ppk format, see connecting from Windows with Putty):</p> <pre><code>pscp -i mykeyfile.ppk mydata.zip flight@52.48.62.34:/home/flight/ # (1)!\n</code></pre> <ol> <li>Copy local file <code>mydata.zip</code> to <code>/home/flight/mydata.zip</code> on host <code>52.48.62.34</code> authenticating as the user <code>flight</code>.</li> </ol> <p>Tip</p> <p>Both the <code>scp</code> and the <code>pscp</code> commands take the parameter <code>-r</code> to recursively copy entire directories of files to the research environment.</p> <p>To retrieve files from the research environment, simply specify the location of the remote file first in the scp command, followed by the location on the local system to put the file.</p>"},{"location":"docs/hpc-environment-basics/linux-usage/working-with-data/copying-data/using-graphical-client/","title":"Using Graphical Client","text":"<p>There are also a number of graphical file-management interfaces available that support the SSH/SCP/SFTP protocols. A graphical interface can make it easier for new users to manage their data, as they provide a simple drag-and-drop interface that helps to visualise where data is being stored. The example below shows how to configure the WinSCP utility on a Windows client to allow data to be moved to and from a research environment.</p> <ul> <li>On a Windows client, download and install WinSCP</li> <li>Start WinSCP; in the login configuration box, enter the IP address of your login node in the <code>Host name</code> box</li> <li>Enter the username you configured for your research environment in the <code>User name</code> box (e.g. <code>flight</code>)</li> <li>Click on the <code>Advanced</code> box and navigate to the <code>SSH</code> sub-menu, and the <code>Authentication</code> item</li> <li>In the <code>Private key file</code> box, select your research environment access private key, and click the <code>OK</code> box.</li> </ul> <p></p> <ul> <li>Optionally click the <code>Save</code> button and give this session a name</li> <li>Click the <code>Login</code> button to connect to your research environment</li> <li>Accept the warning about adding a new server key to your cache; this message is displayed only once when you first connect to the research environment</li> <li>WinSCP will login to your research environment; the window shows your local client machine on the left, and the research environment on the right</li> <li>To copy files to the research environment from your client, click and drag them from the left-hand window and drop them on the right-hand window</li> <li>To copy files from the research environment to your client, click and drag them from the right-hand window and drop them on the left-hand window</li> </ul> <p></p> <p>The amount of time taken to copy data to and from your research environment will depend on a number of factors, including:</p> <ul> <li>The size of the data being copied</li> <li>The speed of your Internet link to the research environment; if you are copying large amounts of data, try to connect using a wired connection rather than wireless</li> <li>The type and location of your research environment login node in relation to your client location</li> </ul>"},{"location":"docs/hpc-workflows/","title":"HPC Workflows - Overview","text":"<p>This section details some common workflows and use-cases of a HPC environment.</p>"},{"location":"docs/hpc-workflows/bowtie/","title":"Bowtie Workflow Example","text":"<p>Bowtie is a sequencing tool-set for aligning sets of DNA into large genomes.</p>"},{"location":"docs/hpc-workflows/bowtie/#workflow","title":"Workflow","text":""},{"location":"docs/hpc-workflows/bowtie/#installing-bowtie-with-spack","title":"Installing Bowtie with Spack","text":"<p>Note</p> <p>The flight environment will need to be activated before the environments can be created so be sure to run <code>flight start</code> or setup your environment to automatically activate the flight environment.</p> <ol> <li>Create a spack software environment:     <pre><code>[flight@chead1 (mycluster1) ~]$ flight env create spack\n</code></pre></li> <li>Activate the environment:     <pre><code>[flight@chead1 (mycluster1) ~]$ flight env activate spack\n</code></pre></li> <li>Install bowtie:     <pre><code>&lt;spack&gt; [flight@chead1 (mycluster1) ~]$ spack install bowtie\n</code></pre></li> </ol>"},{"location":"docs/hpc-workflows/bowtie/#running-a-bowtie-job","title":"Running a Bowtie Job","text":"<ol> <li>Download example ecoli data:     <pre><code>&lt;spack&gt; [flight@chead1 (mycluster1) ~]$ flight silo file pull openflight:bowtie/ecoli.fq\n</code></pre></li> <li>Download an example job script to run in the current working directory:     <pre><code>&lt;spack&gt; [flight@chead1 (mycluster1) ~]$ flight silo file pull openflight:bowtie/ecoli-job.sh\n</code></pre></li> <li>Submit the job to the queue:     <pre><code>&lt;spack&gt; [flight@chead1 (mycluster1) ~]$ sbatch ecoli-job.sh\n</code></pre></li> </ol> <p>The results can then be reviewed from the slurm output file for the job in the current working directory.</p>"},{"location":"docs/hpc-workflows/hadoop/","title":"Hadoop Workflow Example","text":"<p>Hadoop is a scalable, distributed computing solution provided by Apache. Similar to queuing systems, Hadoop allows for distributed processing of large data sets.</p> <p>This example sets up hadoop on a single node and processes an example data set. </p>"},{"location":"docs/hpc-workflows/hadoop/#installing-running-hadoop","title":"Installing &amp; Running Hadoop","text":"<p>Note</p> <p>The flight environment will need to be activated before the environments can be created so be sure to run <code>flight start</code> or setup your environment to automatically activate the flight environment.</p> <ul> <li>Install dependencies for Hadoop:     <pre><code>[flight@chead1 (mycluster1) ~]$ sudo yum install -y java-1.8.0-openjdk.x86_64 java-1.8.0-openjdk-devel.x86_64\n</code></pre></li> <li>Set default Java version:     <pre><code>[flight@chead1 (mycluster1) ~]$ sudo alternatives --set java java-1.8.0-openjdk.x86_64\n[flight@chead1 (mycluster1) ~]$ sudo alternatives --set javac java-1.8.0-openjdk.x86_64\n</code></pre></li> <li> <p>Download Hadoop v3.2.1:     <pre><code>[flight@chead1 (mycluster1) ~]$ flight silo software pull --repo openflight hadoop 3.2.1\n</code></pre></p> </li> <li> <p>Add the hadoop installation to the user's path along with the Java home (replacing <code>SILO_SOFTWARE_DIR</code> with the software directory used by silo in the download above, this can be done temporarily in the CLI or by adding to the user's <code>~/.bashrc</code>):     <pre><code>export HADOOP_HOME=SILO_SOFTWARE_DIR/hadoop/3.2.1\nexport PATH=\"$PATH:$HADOOP_HOME/bin/:$HADOOP_HOME/sbin/\"\nexport CLASSPATH=\"$HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.2.1.jar:$HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.2.1.jar:$HADOOP_HOME/share/hadoop/common/hadoop-common-3.2.1.jar:~/MapReduceTutorial/SalesCountry/*:$HADOOP_HOME/lib/*\"\n</code></pre></p> <p>Note</p> <p>If the above has been set in the <code>~/.bashrc</code> then a new session for the user will need to be started for the environment changes to take effect, otherwise the below commands will not be located</p> </li> <li> <p>Start the Hadoop distributed file system service:     <pre><code>[flight@chead1 (mycluster1) ~]$ start-dfs.sh\n</code></pre></p> </li> <li>Start the resource manager, node manager and app manager service:     <pre><code>[flight@chead1 (mycluster1) ~]$ start-yarn.sh\n</code></pre></li> </ul>"},{"location":"docs/hpc-workflows/hadoop/#downloading-the-hadoop-job","title":"Downloading the Hadoop Job","text":"<p>These steps help setup the Hadoop environment and download a spreadsheet of data which will Hadoop will sort into sales units per region.</p> <ul> <li>Create job directory:     <pre><code>[flight@chead1 (mycluster1) ~]$ mkdir MapReduceTutorial\n</code></pre></li> <li>Download job data:     <pre><code>[flight@chead1 (mycluster1) ~]$ cd MapReduceTutorial\n[flight@chead1 (mycluster1) MapReduceTutorial]$ flight silo file pull openflight:hadoop/hdfiles.tar.gz\n[flight@chead1 (mycluster1) MapReduceTutorial]$ tar xf hdfiles.tar.gz\n</code></pre></li> <li>Check that job data files are present:     <pre><code>[flight@chead1 (mycluster1) MapReduceTutorial]$ ls\nManifest.txt  SalesCountryDriver.java  SalesCountryReducer.java  SalesJan2009.csv  SalesMapper.java  desktop.ini  hdfiles.tar.gz\n</code></pre></li> </ul>"},{"location":"docs/hpc-workflows/hadoop/#preparing-the-hadoop-job","title":"Preparing the Hadoop Job","text":"<ul> <li>Compile java for job:     <pre><code>[flight@chead1 (mycluster1) MapReduceTutorial]$ javac -d . SalesMapper.java SalesCountryReducer.java SalesCountryDriver.java\n</code></pre></li> <li>Compile the final java file for job:     <pre><code>[flight@chead1 (mycluster1) MapReduceTutorial]$ jar cfm ProductSalePerCountry.jar Manifest.txt SalesCountry/*.class\n</code></pre></li> </ul>"},{"location":"docs/hpc-workflows/hadoop/#loading-data-into-hadoop","title":"Loading Data into Hadoop","text":"<ul> <li>Create directory for processing data and copy sales results in:     <pre><code>[flight@chead1 (mycluster1) MapReduceTutorial]$ mkdir ~/inputMapReduce\n[flight@chead1 (mycluster1) MapReduceTutorial]$ cp SalesJan2009.csv ~/inputMapReduce/\n</code></pre></li> <li>Check data can be seen in distributed file system:      <pre><code>[flight@chead1 (mycluster1) MapReduceTutorial]$ hdfs dfs -ls ~/inputMapReduce\n</code></pre></li> </ul>"},{"location":"docs/hpc-workflows/hadoop/#running-the-hadoop-job","title":"Running the Hadoop Job","text":"<ul> <li>Execute the MapReduce job:     <pre><code>[flight@chead1 (mycluster1) MapReduceTutorial]$ hadoop jar ProductSalePerCountry.jar ~/inputMapReduce ~/mapreduce_output_sales\n</code></pre></li> <li>View the job results:     <pre><code>[flight@chead1 (mycluster1) MapReduceTutorial]$ hdfs dfs -cat ~/mapreduce_output_sales/part-00000 | less\n</code></pre></li> </ul>"},{"location":"docs/hpc-workflows/hpl/","title":"HPL Benchmark Example","text":"<p>High Performance Linpack (HPL) is a synthetic benchmark, designed to solve a large mathematical problem on computers - including those with Distributed Memory, such as those within a cluster. It can be run on a single node to check performance and stress only that node. It can also be run over multiple nodes via an interconnect such as Infiniband or Ethernet - some of the steps below are only applicable to certain Interconnects.</p> <p>It solves the problem in Double-Precision (64bit) arithmetic, which quickly stresses the Floating Point unit of a CPU along with the Memory subsystem and will normally show any hardware problems with a node quite quickly. When run across multiple nodes via an interconnect, it is useful for showing up errors on the fabric as poor performance will be exhibited and often accompanied with reported errors.</p>"},{"location":"docs/hpc-workflows/hpl/#notes-for-running-hpl","title":"Notes for running HPL","text":"<ul> <li>Ensure that all System Event Logs are clear before commencing running of Memtester - you can use <code>service ipmi start &amp;&amp; ipmitool -c 'sel clear'</code> to clear these logs.</li> <li>Infiniband Only - Ensure that the <code>ibcheckerrors</code> and <code>ibclearerrors</code> commands are used to check for any current errors and clear any previous errors reported on the Fabric.</li> <li>Calculate the correct settings for the HPL.dat file with the HPL Calculator - Recommended settings are an NB size of 192 and a Memory usage of 88%.</li> <li>Use a Grid Shape (PxQ) with as close to \"square\" as possible, making Q slightly larger than P.</li> <li>For systems using an MPI version of 1.8.x or higher, ensure that the <code>mpirun</code> line includes the \"--bind-to-core\" parameter.</li> <li>Infiniband Only - Use the <code>ibcheckerrors</code> command to ensure that there are no issues with the fabric reported after running the HPL benchmark.</li> </ul>"},{"location":"docs/hpc-workflows/hpl/#example-script","title":"Example Script","text":"<p>The below example script is for 16 core, 64GB node with no IB Interconnect. The script is designed to be run with SLURM.</p> hpl.sh<pre><code>#!/bin/bash\n# jobscript to run HPL benchmark\n\n# Export environment and merge+set output file\n#$ -j y -N HPL-1node -o /PATH/TO/JOB_DIR/HPL.$JOB_ID\n\nmodule load mpi/openmpi # (1)!\necho $LD_LIBRARY_PATH\n\nldd xhpl\n\n# change to directory\ncd /PATH/TO/JOB_DIR/\nmpirun -np 16 /PATH/TO/XHPL_INSTALL_DIR/xhpl # (2)!\n</code></pre> <ol> <li> <p>This line loads <code>openmpi</code> using <code>modules</code>. It requires both <code>modules</code> and <code>openmpi</code> to be installed on the system. Doing so is outside of the scope of this documentation</p> </li> <li> <p>This line executes <code>xhpl</code>, installation of which is currently outside of scope of this document</p> </li> </ol> HPL.dat<pre><code>HPLinpack benchmark input file\nInnovative Computing Laboratory, University of Tennessee\nHPL.out      output file name (if any)\n6            device out (6=stdout,7=stderr,file)\n2            # of problems sizes (N)\n2560 81408   Ns\n1            # of NBs\n192          NBs\n0            PMAP process mapping (0=Row-,1=Column-major)\n1            # of process grids (P x Q)\n4            Ps\n4            Qs\n16.0         threshold\n1            # of panel fact\n1 0 2        PFACTs (0=left, 1=Crout, 2=Right)\n1            # of recursive stopping criterium\n4 2          NBMINs (&gt;= 1)\n1            # of panels in recursion\n2            NDIVs\n1            # of recursive panel fact.\n1 0 2        RFACTs (0=left, 1=Crout, 2=Right)\n1            # of broadcast\n0            BCASTs (0=1rg,1=1rM,2=2rg,3=2rM,4=Lng,5=LnM)\n1            # of lookahead depth\n0            DEPTHs (&gt;=0)\n2            SWAP (0=bin-exch,1=long,2=mix)\n256          swapping threshold\n1            L1 in (0=transposed,1=no-transposed) form\n1            U  in (0=transposed,1=no-transposed) form\n0            Equilibration (0=no,1=yes)\n8            memory alignment in double (&gt; 0)\n</code></pre>"},{"location":"docs/hpc-workflows/imb/","title":"IMB Benchmark Example","text":"<p>The Intel MPI Benchmark provides a tool for testing the parallel execution speed of one to many systems. </p>"},{"location":"docs/hpc-workflows/imb/#install-imb","title":"Install IMB","text":"<p>Installing IMB is outside of the scope of this documentation. Further information can be found by following installation as described in the Intel documentation. It requires Intel MPI to be installed which can be obtained by registering a system with Intel.</p>"},{"location":"docs/hpc-workflows/imb/#run-benchmark","title":"Run Benchmark","text":"<p>To use 64 CPU cores on a SLURM HPC environment for a single application, the instruction <code>--ntasks=64</code> can be used.</p> <p>The following example shows launching the Intel Message-passing MPI benchmark across 64 cores on your research environment. This application is launched via the OpenMPI <code>mpirun</code> command - the number of threads and list of hosts are automatically assembled by the scheduler and passed to the MPI at runtime. This job script loads the <code>apps/imb</code> module before launching the application, which automatically loads the module for OpenMPI.</p> <pre><code>#!/bin/bash -l\n#SBATCH -n 64\n#SBATCH --job-name=imb\n#SBATCH -D $HOME/\n#SBATCH --output=imb.out.%j\nmodule load apps/imb\nmpirun --prefix $MPI_HOME \\\n       IMB-MPI1\n</code></pre> <p>We can then submit the IMB job script to the scheduler, which will automatically determine which nodes to use:</p> <pre><code>[flight@chead1 (mycluster1) ~]$ sbatch imb.sh\nSubmitted batch job 1162\n[flight@chead1 (mycluster1) ~]$ squeue\n             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n                           1162       all      imb    centos  R       0:01      8 ip-10-75-1-[42,45,62,67,105,178,233,250]\n</code></pre>"},{"location":"docs/hpc-workflows/imb/#view-results","title":"View Results","text":"<p>The results for the benchmark are available in the SLURM output file.</p> <pre><code>[flight@chead1 (mycluster1) ~]$ cat imb.out.1162\n#------------------------------------------------------------\n#    Intel (R) MPI Benchmarks 4.0, MPI-1 part\n#------------------------------------------------------------\n# Date                  : Tue Aug 30 10:34:08 2016\n# Machine               : x86_64\n# System                : Linux\n# Release               : 3.10.0-327.28.3.el7.x86_64\n# Version               : #1 SMP Thu Aug 18 19:05:49 UTC 2016\n# MPI Version           : 3.0\n# MPI Thread Environment:\n\n#---------------------------------------------------\n# Benchmarking PingPong\n# #processes = 2\n# ( 62 additional processes waiting in MPI_Barrier)\n#---------------------------------------------------\n       #bytes #repetitions      t[usec]   Mbytes/sec\n            0         1000         3.17         0.00\n            1         1000         3.20         0.30\n            2         1000         3.18         0.60\n            4         1000         3.19         1.19\n            8         1000         3.26         2.34\n           16         1000         3.22         4.74\n           32         1000         3.22         9.47\n           64         1000         3.21        19.04\n          128         1000         3.22        37.92\n          256         1000         3.30        73.90\n          512         1000         3.41       143.15\n         1024         1000         3.55       275.36\n         2048         1000         3.75       521.04\n         4096         1000        10.09       387.14\n         8192         1000        11.12       702.51\n        16384         1000        12.06      1296.04\n        32768         1000        14.65      2133.32\n        65536          640        19.30      3238.72\n       131072          320        29.50      4236.83\n       262144          160        48.17      5189.77\n       524288           80        84.36      5926.88\n      1048576           40       157.40      6353.32\n      2097152           20       305.00      6557.31\n      4194304           10       675.20      5924.16\n</code></pre>"},{"location":"docs/hpc-workflows/tensorflow/","title":"Tensorflow Workflow Example","text":"<p>Tensorflow is an open-source machine learning platform. It provides an ecosystem of tools and libraries that allow researchers to build and deploy machine learning applications.</p> <p>Note</p> <p>The flight environment will need to be activated before the environments can be created so be sure to run <code>flight start</code> or setup your environment to automatically activate the flight environment.</p>"},{"location":"docs/hpc-workflows/tensorflow/#installing-tensorflow","title":"Installing Tensorflow","text":"SingularityTensorflow <ul> <li>Create a singularity software environment:     <pre><code>[flight@chead1 (mycluster1) ~]$ flight env create singularity\n</code></pre></li> <li>Activate the environment:     <pre><code>[flight@chead1 (mycluster1) ~]$ flight env activate singularity\n</code></pre></li> </ul> <ul> <li>Create a conda software environment:</li> </ul> <pre><code>[flight@chead1 (mycluster1) ~]$ flight env create conda\n</code></pre> <ul> <li>Activate the environment:</li> </ul> <pre><code>[flight@chead1 (mycluster1) ~]$ flight env activate conda\n</code></pre> <ul> <li>Create a Python environment for tensorflow:</li> </ul> <pre><code>(base) &lt;conda&gt; [flight@chead1 (mycluster1) ~]$ conda create -n tensorflow python=3.6\n</code></pre> <ul> <li>Activate the Python environment:</li> </ul> <pre><code>(base) &lt;conda&gt; [flight@chead1 (mycluster1) ~]$ source activate tensorflow\n</code></pre> <ul> <li>Install the tensorflow package:</li> </ul> <pre><code>(tensorflow) &lt;conda&gt; [flight@chead1 (mycluster1) ~]$ pip install tensorflow==1.15\n</code></pre>"},{"location":"docs/hpc-workflows/tensorflow/#running-the-job","title":"Running the Job","text":"SingularityTensorflow <ul> <li>Download the example job models:     <pre><code>&lt;singularity&gt; [flight@chead1 (mycluster1) ~]$ git clone -b v1.13.0 https://github.com/tensorflow/models.git\n</code></pre></li> <li>Launch the tensorflow docker container with singularity to run the job:     <pre><code>&lt;singularity&gt; [flight@chead1 (mycluster1) ~]$ singularity exec docker://tensorflow/tensorflow:1.15.0 python ./models/tutorials/image/mnist/convolutional.py\n</code></pre></li> </ul> <ul> <li>Download the example job models:     <pre><code>(tensorflow) &lt;conda&gt; [flight@chead1 (mycluster1) ~]$ git clone -b v1.13.0 https://github.com/tensorflow/models.git\n</code></pre></li> <li>Execute the job with python:     <pre><code>(tensorflow) &lt;conda&gt; [flight@chead1 (mycluster1) ~]$ python ./models/tutorials/image/mnist/convolutional.py\n</code></pre></li> </ul>"},{"location":"docs/hpc-workflows/openfoam/","title":"OpenFOAM","text":"<p>OpenFOAM is a popular engineering application toolbox. It's open source and is used for simulating fluid flow around objects. This section describes the different installation process available and how to run some basic jobs. Some parts in this section use the Flight Silo tool to download necessary files.</p> <p></p>"},{"location":"docs/hpc-workflows/openfoam/install/","title":"Install OpenFOAM","text":"<p>There are 3 different ways you can install OpenFOAM, each with varying levels of time to setup and complexity. The Basic setup is the fastest and integrates best with the Flight User Suite. The Advanced setup is for users that want more control, and can take over an hour on a 16 core machine.</p> Basic Advanced Legacy  <ol> <li> <p>Download and install OpenFlight OpenFOAM software using Flight Silo <pre><code>flight silo software pull OpenFOAM 22.12 --repo openflight\n</code></pre></p> </li> <li> <p>Install dependencies for visualisation.     <pre><code>sudo dnf install -y paraview\n</code></pre></p> </li> <li> <p>Install dependencies on all nodes.     <pre><code>pdsh -g all 'sudo dnf install -y openmpi'\n</code></pre></p> </li> <li> <p>Source the OpenFOAM environment and run a basic test. Make sure to change the source file path to the location of your installation. Note that the <code>module</code> command will not be available to a shell session existing before the installation, logout and back in for it to be present <pre><code>module load mpi\nsource apps/OpenFOAM/22.12/etc/bashrc\nfoamTestTutorial -full incompressible/simpleFoam/pitzDaily\n</code></pre></p> </li> </ol> <p>Firstly, become the root user, the added permissions are necessary for setup.     <pre><code>sudo su -\n</code></pre></p> <ol> <li>Install prerequisites     <pre><code>dnf install gcc cmake boost fftw-devel paraview-devel paraview-openmpi-devel openmpi-devel flex m4 qt5-devel\n</code></pre></li> <li>Make a directory for OpenFOAM in shared storage, so that the OpenFOAM build is available to all nodes in the cluster.     <pre><code>cd /opt/apps\nmkdir OpenFOAM\n</code></pre></li> <li>Obtain OpenFOAM Source.     <pre><code>flight silo file pull openflight:openfoam/OpenFOAM-v2212.tar.gz\ntar xf OpenFOAM-v2212.tgz\n</code></pre></li> <li>Compile it.     <pre><code>cd OpenFOAM-v2212\nmodule load mpi\nsource etc/bashrc\n\nfoamSystemCheck # Check whether expected to work\n\n./Allwmake -j -s -q -l # Compile on all cores\n</code></pre></li> <li>Install dependencies for visualisation.     <pre><code>dnf install -y paraview\n</code></pre></li> <li>Install dependencies on all nodes.     <pre><code>pdsh -g all 'dnf install -y openmpi'\n</code></pre></li> <li>Test that the build works.     <pre><code>foamTestTutorial -full incompressible/simpleFoam/pitzDaily\n</code></pre> !!!     From this point being the root user is no longer necessary.</li> </ol> <ol> <li> <p>Enable copr for OpenFOAM.     <pre><code>sudo dnf -y copr enable openfoam/openfoam\n</code></pre></p> </li> <li> <p>Install OpenFOAM, the OpenFOAM version selector, the additional sub-packages and paraview.     <pre><code>sudo dnf install -y openfoam-selector\nsudo dnf install -y openfoam\nsudo dnf install -y openfoam2212-default\nsudo dnf install -y paraview\n</code></pre></p> </li> <li> <p>Ensure that the correct version will be used with:     <pre><code>openfoam-selector --set openfoam2212\n</code></pre></p> </li> <li> <p>Refresh the shell by logging out and back in to make openfoam commands accessible.</p> <p>Note</p> <p>Make sure to do the above installation steps on all nodes in the cluster.</p> </li> <li> <p>Check an OpenFOAM command can be seen by viewing the help page:     <pre><code>icoFoam -help\n</code></pre></p> </li> </ol>"},{"location":"docs/hpc-workflows/openfoam/mpi/","title":"OpenFOAM MPI Workflow Example","text":"<p>Tip</p> <p>Your environment will need to have a job scheduler such as Slurm, which can be set up by following the Flight Solo cluster build methods.</p>"},{"location":"docs/hpc-workflows/openfoam/mpi/#prepare-job","title":"Prepare Job","text":"<ol> <li> <p>Start by downloading this job directory in a location of your choice.     <pre><code>flight silo file pull openflight:openfoam/motorBike.tar.gz\n</code></pre></p> </li> <li> <p>Decompress it, which will make a directory called <code>motorBike</code>.     <pre><code>tar xf motorBike.tar.gz\n</code></pre></p> </li> <li> <p>Update the job configuration.</p> <ol> <li>Open <code>motorBike/system/decomposeParDict</code> and set:<ol> <li><code>X</code> to the number of processes to run across.</li> <li><code>A B C</code> to be numbers such that <code>A</code>*<code>B</code>*<code>C</code>=<code>X</code></li> </ol> </li> </ol> </li> <li> <p>Update the job script.</p> <ol> <li>Open <code>motorBike/motorbike-example.sh</code>.<ol> <li>Set <code>-n X</code> to the number of processes to run across.</li> <li>Set <code>source</code> to point to the location of OpenFOAM.</li> <li>Set <code>$HOME/motorBike</code> to the location of the motorBike job.</li> </ol> </li> </ol> </li> </ol>"},{"location":"docs/hpc-workflows/openfoam/mpi/#run-job","title":"Run Job","text":"<ol> <li>Submit the job script to the scheduler.     <pre><code>sbatch motorBike/motorbike-example.sh\n</code></pre></li> </ol>"},{"location":"docs/hpc-workflows/openfoam/mpi/#view-results","title":"View Results","text":"<p>Once the job has finished running, the results can be visualised through a desktop session.</p> <ol> <li> <p>Connect to VNC or web-suite desktop (For more information on launching desktops, see the Flight Desktop section).</p> </li> <li> <p>Open a terminal and navigate to the job directory.</p> </li> <li> <p>Launch Paraview     <pre><code>source /opt/apps/OpenFOAM/OpenFOAM-v2212/etc/bashrc\nparaFoam\n</code></pre></p> </li> <li> <p>In the window that opens, go to the bottom left and scroll down to \"Mesh parts\". Select all the boxes  except <code>frontAndBack</code>, <code>inlet</code>, <code>internalMesh</code>, <code>outlet</code> and <code>upperWall</code>, and then click apply.</p> <p></p> </li> <li> <p>You should now see a motorbike along with a rider and the forces visualised on them!</p> <p></p> </li> </ol>"},{"location":"docs/hpc-workflows/openfoam/simple/","title":"Simple OpenFOAM Workflow Example","text":"<p>Tip</p> <p>Your environment will need to have a job scheduler such as Slurm, which can be set up by following the Flight Solo cluster build methods.</p>"},{"location":"docs/hpc-workflows/openfoam/simple/#run-job","title":"Run Job","text":"<ol> <li> <p>Download a job script to the working directory. This script assumes that it is being launched from the home directory, change it if that is not the case. It also cannot be run as the root user.</p> <pre><code>flight silo file pull openflight:openfoam/cavity-example.sh\n</code></pre> </li> <li> <p>Submit the job to the queue system:     <pre><code>sbatch cavity-example.sh\n</code></pre></p> </li> <li> <p>Check that the job is running (it may take some time to complete):     <pre><code>squeue\n</code></pre></p> </li> </ol>"},{"location":"docs/hpc-workflows/openfoam/simple/#view-results","title":"View Results","text":"<p>Once the cavity job has finished running, the results can be visualised through a desktop session.</p> <ol> <li> <p>Connect to VNC or web-suite desktop (For more information on launching desktops, see the Flight Desktop section).</p> </li> <li> <p>Navigate to the cavity directory and launch the paraFoam viewer:     <pre><code>cd cavity\nparaFoam\n</code></pre></p> </li> <li> <p>In the window that opens, scroll down to \"Mesh parts\" and select all the boxes, then click apply     </p> </li> <li> <p>Next, change the display to 'U' in the drop-down menu, click play to change the timestamp and then click and drag to rotate the subject     </p> </li> </ol>"},{"location":"docs/overview/get-help/","title":"Where can I get help?","text":"<p>Among many other areas, this documentation is designed to walk users through the first stages of creating their research environments, and getting started in the environment. Capable users with some experience can be up and running in a handful of minutes - don't panic if it takes you a little more time, especially if you've not used Linux or HPC research environments before. Firstly - don't worry that you might break something complicated and expensive; one of the joys of having your own personal environment is that no one will tell you that you're doing it wrong, and nothing is at risk of being broken, aside from the data and work you've done yourself in the environment.</p> <p>We encourage new users to run through a few tutorials in this documentation - even if you have plenty of HPC experience. Technology moves forward all the time and new features are constantly popping up that could save you effort in future. If you do run into problems, try replicating the steps you went through to get where you are - sometimes a typo in a command early-on in your workflow might not cause any errors until right at the end of your work. It can help to work collaboratively with other researchers running similar jobs - not only are two sets of eyes better than one, you'll both get something out of working together to achieve a shared goal.</p> <p>For software application support you're going to need to contact the developers of the packages themselves. Remember that many of these software products are open-source and you've paid no fee to use them - try to make your bug-reports and enhancement requests as helpful and friendly as possible to the application developers. They've done you a great service by making their software available for you to use - please be respectful of their time and effort if you need to contact them, and remember to credit their software in your research publications.</p>"},{"location":"docs/overview/prerequisites/","title":"Prerequisites","text":"<p>You're going to need access to some computers. If you already have access to cloud resources, then great - you're ready to go. There are some specific requirements depending on your platform type, which are discussed in the relevant chapters of this guide.</p> <p>You'll need a client device - something to log into your research environment from. The requirements on client devices are fairly minimal, but you'll need:  - A computer with a screen and a keyboard; you can probably access on a tablet or smartphone too, if your typing is good enough  - A relatively modern web-browser; Apple Safari, Google Chrome, Microsoft Edge, Mozilla Firefox, and pretty much anything else that was written/updated in the last couple of years should all be fine.  - An SSH client; this comes built-in for Linux and Mac based machines, but you'll need to install one for Windows clients and some tablets.  - Internet access; it seems dumb to list this as a requirement for running HPC on cloud resources, but a surprising number of sites actually limit outbound connectivity to the Internet. You'll need to be able to access the Internet from your client device.  - Optionally: A graphical data transfer tool; you don't actually need one of these, but they can really help new users get started more quickly.</p> <p>It's worth checking for centrally-managed client systems that you can install the software that you need - some research sites don't allow users to install new software. Here are some recommendations of software that you can use on client machines; this is far from a complete list, but should help you get started:</p> <ul> <li>SSH client:<ul> <li>Use the built-in <code>ssh</code> client for Mac and Linux</li> <li>For Windows, try Putty or SmaTTY</li> </ul> </li> <li>Web-browser:<ul> <li>Use the built-in Safari browser on Macs</li> <li>For Linux and Windows, install Firefox or Chrome</li> </ul> </li> <li>VNC (Graphical desktop client):<ul> <li>Use the built-in VNC client on Macs</li> <li>For Linux, install <code>vncviewer</code> package, or install RealVNC viewer</li> <li>For Windows, install TurboVNC</li> </ul> </li> <li>Graphical file-transfer tools:<ul> <li>For Macs and Linux, install Cyberduck or Filezilla</li> <li>For Windows, try WinSCP, Cyberduck or Filezilla</li> </ul> </li> </ul> <p>We've tried to make recommendations for open-source and/or free software client software here - as ever, please read and obey the licensing terms, and try to contribute to the supporting projects either financially, or by referencing them in your research publications.</p>"}]}